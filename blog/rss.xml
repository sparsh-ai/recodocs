<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Recohut Blog</title>
        <link>https://docs.recohut.com/blog</link>
        <description>Recohut Blog</description>
        <lastBuildDate>Fri, 01 Oct 2021 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[ByteDance's secret sauce of recommendation]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/bytedance's-secret-sauce-of-recommendation</link>
            <guid>/2021/10/01/bytedance's-secret-sauce-of-recommendation</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[For those who have been envious of¬†ByteDance‚Äôs¬†immense success across its various consumer products, and more specifically, the recommendation algorithms powering those products, there is finally good news.]]></description>
            <content:encoded><![CDATA[<p>For those who have been envious of¬†<a href="https://en.pingwest.com/search/bytedance">ByteDance‚Äôs</a>¬†immense success across its various consumer products, and more specifically, the recommendation algorithms powering those products, there is finally good news.</p><p><strong>With a new brand called¬†<a href="https://www.volcengine.com/">Volcano Engine,</a>¬†ByteDance is now making the technologies that drove the success of TikTok, Douyin, Toutiao, etc., available as a commercialized service to enterprise clients for the first time.</strong></p><p>Volcano Engine is a suite of services that purportedly assembles the ‚Äútools, technologies, and methodology‚Äù of ByteDance‚Äôs growth legend, covering its recommendation algorithms, audio-visual technologies, data insight services, cloud computing solutions, and more. It is quite literally a copy of the ByteDance&#x27;s growth hacking ‚Äúsecret sauce‚Äù.</p><p>The intelligent recommendation service relies on the advanced large-scale machine learning and personalized recommendation technology of Volcano Engine. It has the ability to accumulate in information and information, live video, social networking, e-commerce and other fields, and provides customized recommendation algorithm services for partners. Improve core business indicators and create value through algorithmic capabilities</p><p><img src="/img/content-blog-raw-blog-bytedance&#x27;s-secret-sauce-of-recommendation-untitled.png" alt="/img/content-blog-raw-blog-bytedance&#x27;s-secret-sauce-of-recommendation-untitled.png"/></p><p>Volcano Engine-Intelligent Recommendation Service has been applied in many industries. Through in-depth mining and intelligent analysis of user behavior data, personalized content distribution and recommended product. Intelligent recommendation service has helped partners achieve a 150% increase in CTR in specific scenarios, 180% increase in advertising revenue in cooperation scenarios, helping partners understand users better</p><p><img src="/img/content-blog-raw-blog-bytedance&#x27;s-secret-sauce-of-recommendation-untitled-1.png" alt="/img/content-blog-raw-blog-bytedance&#x27;s-secret-sauce-of-recommendation-untitled-1.png"/></p><h3><strong>Data description</strong></h3><p>For customers in the e-commerce industry, 3 data sheets are required to access the smart recommendation service. For specific field requirements, please refer to the following &quot;Data Field Description&quot;:</p><ol><li>Product table (product): synchronize all product information of the access scene;</li><li>User table (user): To synchronize all user information, the uniqueness of the user needs to be guaranteed, and the user id information is carried when requesting the service;</li><li>User behavior table (behavior): Synchronize historical and incremental user behavior data. It is recommended to provide 6-12 months of historical behavior data. The longer the synchronization time, the better the effect;</li></ol><h3><strong>Data synchronization method</strong></h3><p>Data synchronization includes offline data and incremental data. It supports synchronization through API interface, SDK or public cloud object storage. Among them:</p><ul><li>Offline data: need to provide product data, user data and historical user behavior data;</li></ul><p><strong>Remarks:</strong> Commodity data and user data are recommended to send day-level data snapshots. If there is no snapshot, then the latest data will be sent; Behavior data is synchronized at the day level; Commodity data, user data and behavior data are synchronized in json format. Each row of the data is json, corresponding to a record;</p><ul><li>Incremental data: Incremental product data, user data, and user behavior data need to be provided;</li></ul><p><strong>Remarks:</strong> ‚ë† Real-time incremental synchronization of behavioral data and product data is strongly recommended; ‚ë° User attribute data can be synchronized in increments of days;</p><h3><strong>Data field description</strong></h3><p>The following are the specific field requirements of the user, product, and user behavior table. Whether it is mandatory to include three types: &quot;Yes&quot;, &quot;Suggestion&quot; and &quot;No&quot;. It is recommended to upload as much as possible. The more fields available for the algorithm model, the better the effect of the model.</p><h3><strong>Data synchronization</strong></h3><p>Data synchronization supports synchronization through¬†<a href="https://www.volcengine.com/docs/4462/38312/">API interface</a>¬†,¬†<a href="https://www.volcengine.com/docs/4462/39599/">SDK</a>¬†or public cloud object storage.</p><ul><li>If the amount of offline data is large, support customers to dump the data to public cloud object storage, and we will pull it through the public network</li><li>The offline data synchronization solution of the privatization deployment plan is more flexible and supports the direct pull of HDFS directories</li><li>The data to be synchronized includes offline data and incremental data. The synchronization method is described as follows:</li><li>Offline data: need to provide product data, user data and historical user behavior data</li><li>Commodity data and user data are recommended to send day-level data snapshots. If there is no snapshot, then the latest data will be sent</li><li>Behavior data is synchronized at the day level</li><li>Commodity data, user data, and behavioral data are synchronized in JSON format. Each row of the data is JSON, corresponding to a record</li><li>Incremental data: Incremental product data, user data, and user behavior data need to be provided;</li><li>Real-time incremental synchronization of behavioral data and product data is strongly recommended</li><li>User attribute data can be synchronized in increments of days</li></ul><h3>References</h3><ol><li><a href="https://www.volcengine.com/">https://www.volcengine.com/</a></li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Clinical Decision Making]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/clinical-decision-making</link>
            <guid>/2021/10/01/clinical-decision-making</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Health insurance can be complicated‚Äîespecially when it comes to prior authorization (also referred to as pre-approval, pre-authorization, and pre-certification). The manual labor involved in obtaining prior authorizations (PAs) is a well-recognized burden among providers. Up to 46% of PA requests are still submitted by fax, and 60% require a telephone call, according to America‚Äôs Health Insurance Plans (AHIP). A 2018 survey by the American Medical Association (AMA) found that doctors and their staff spend an average of 2 days a week completing PAs. In addition to eating up time that physicians could spend with patients, PAs also contribute to burnout.]]></description>
            <content:encoded><![CDATA[<p>Health insurance can be complicated‚Äîespecially when it comes to prior authorization (also referred to as pre-approval, pre-authorization, and pre-certification). The manual labor involved in obtaining prior authorizations (PAs) is a well-recognized burden among providers. Up to 46% of PA requests are still submitted by fax, and 60% require a telephone call, according to America‚Äôs Health Insurance Plans (AHIP). A 2018 survey by the American Medical Association (AMA) found that doctors and their staff spend an average of 2 days a week completing PAs. In addition to eating up time that physicians could spend with patients, PAs also contribute to burnout.</p><p>The objective was to identify the patterns from data to create clinical decision making in Pre-Auth and improve the accuracy in a clinical decision based on historical data analysis. </p><p>Two use cases were identified. Use Case 1 - <em>Supervised Learning Model - to aid clinicians in UM decision making. Tasks -</em> Ingest Pre-authorization data from Mongo DB into the analytical environment, Exploratory Data Analysis and Feature Engineering, Train supervised analytical models, model validation and model selection, Create a web service to be plugged into the case processing flow to call the model, and Display the recommendation from the model on UI on the authorization review screen. Use Case 2 - <em>Unsupervised Learning Model - to generate insights from the pre-authorization data. Tasks -</em> Ingest Pre-authorization data from Mongo DB into the analytical environment, Cluster analysis, univariate and multivariate analysis, and Generate insights and display insights on the dashboard.</p><p>Final Deliverables - Model re-training (batch mode), validation and deployment code (python scripts) with Unix command line support, Documentation - PPT, Recorded video, Technical document, Flask API backend system, HTML/PHP Web App frontend UI integration, and Plotly Dash Supervised/Unsupervised learning and insights generation dashboard.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Detectron 2]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/detectron-2</link>
            <guid>/2021/10/01/detectron-2</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-detectron-2-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-detectron-2-untitled.png" alt="/img/content-blog-raw-blog-detectron-2-untitled.png"/></p><h1>Introduction</h1><p>Detectron 2 is a next-generation open-source object detection system from Facebook AI Research. With the repo you can use and train the various state-of-the-art models for detection tasks such as bounding-box detection, instance and semantic segmentation, and person keypoint detection.</p><p>The following is the directory tree of detectron 2:</p><pre><code>detectron2
‚îú‚îÄcheckpoint  &lt;- checkpointer and model catalog handlers
‚îú‚îÄconfig      &lt;- default configs and handlers
‚îú‚îÄdata        &lt;- dataset handlers and data loaders
‚îú‚îÄengine      &lt;- predictor and trainer engines
‚îú‚îÄevaluation  &lt;- evaluator for each dataset
‚îú‚îÄexport      &lt;- converter of detectron2 models to caffe2 (ONNX)
‚îú‚îÄlayers      &lt;- custom layers e.g. deformable conv.
‚îú‚îÄmodel_zoo   &lt;- pre-trained model links and handler
‚îú‚îÄmodeling   
‚îÇ  ‚îú‚îÄmeta_arch &lt;- meta architecture e.g. R-CNN, RetinaNet
‚îÇ  ‚îú‚îÄbackbone  &lt;- backbone network e.g. ResNet, FPN
‚îÇ  ‚îú‚îÄproposal_generator &lt;- region proposal network
‚îÇ  ‚îî‚îÄroi_heads &lt;- head networks for pooled ROIs e.g. box, mask heads
‚îú‚îÄsolver       &lt;- optimizer and scheduler builders
‚îú‚îÄstructures   &lt;- structure classes e.g. Boxes, Instances, etc
‚îî‚îÄutils        &lt;- utility modules e.g. visualizer, logger, etc
</code></pre><h1>Installation</h1><pre><code class="language-python">%%time
!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html;
!pip install cython pyyaml==5.1;
!pip install -U &#x27;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#x27;;
!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html;

from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog
</code></pre><h1>Inference on pre-trained models</h1><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-1.png" alt="Original image"/></p><p>Original image</p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-2.png" alt="Object detection with Faster-RCNN-101"/></p><p>Object detection with Faster-RCNN-101</p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-3.png" alt="Instance segmentation with Mask-RCNN-50"/></p><p>Instance segmentation with Mask-RCNN-50</p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-4.png" alt="Keypoint estimation with Keypoint-RCNN-50"/></p><p>Keypoint estimation with Keypoint-RCNN-50</p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-5.png" alt="Panoptic segmentation with Panoptic-FPN-101"/></p><p>Panoptic segmentation with Panoptic-FPN-101</p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-6.png" alt="Default Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison"/></p><p>Default Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison</p><h1>Fine-tuning Balloons Dataset</h1><h3>Load the data</h3><pre><code># download, decompress the data
!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip
!unzip balloon_dataset.zip &gt; /dev/null
</code></pre><h3>Convert dataset into Detectron2&#x27;s standard format</h3><pre><code>from detectron2.structures import BoxMode
# write a function that loads the dataset into detectron2&#x27;s standard format
def get_balloon_dicts(img_dir):
    json_file = os.path.join(img_dir, &quot;via_region_data.json&quot;)
    with open(json_file) as f:
        imgs_anns = json.load(f)

    dataset_dicts = []
    for _, v in imgs_anns.items():
        record = {}
        
        filename = os.path.join(img_dir, v[&quot;filename&quot;])
        height, width = cv2.imread(filename).shape[:2]
        
        record[&quot;file_name&quot;] = filename
        record[&quot;height&quot;] = height
        record[&quot;width&quot;] = width
      
        annos = v[&quot;regions&quot;]
        objs = []
        for _, anno in annos.items():
            assert not anno[&quot;region_attributes&quot;]
            anno = anno[&quot;shape_attributes&quot;]
            px = anno[&quot;all_points_x&quot;]
            py = anno[&quot;all_points_y&quot;]
            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]
            poly = list(itertools.chain.from_iterable(poly))

            obj = {
                &quot;bbox&quot;: [np.min(px), np.min(py), np.max(px), np.max(py)],
                &quot;bbox_mode&quot;: BoxMode.XYXY_ABS,
                &quot;segmentation&quot;: [poly],
                &quot;category_id&quot;: 0,
                &quot;iscrowd&quot;: 0
            }
            objs.append(obj)
        record[&quot;annotations&quot;] = objs
        dataset_dicts.append(record)
    return dataset_dicts

from detectron2.data import DatasetCatalog, MetadataCatalog
for d in [&quot;train&quot;, &quot;val&quot;]:
    DatasetCatalog.register(&quot;balloon/&quot; + d, lambda d=d: get_balloon_dicts(&quot;balloon/&quot; + d))
    MetadataCatalog.get(&quot;balloon/&quot; + d).set(thing_classes=[&quot;balloon&quot;])
balloon_metadata = MetadataCatalog.get(&quot;balloon/train&quot;)
</code></pre><h3>Model configuration and training</h3><pre><code>from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;))
cfg.DATASETS.TRAIN = (&quot;balloon/train&quot;,)
cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025
cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but you can certainly train longer
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()
</code></pre><h3>Inference and Visualization</h3><pre><code>from detectron2.utils.visualizer import ColorMode

# load weights
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, &quot;model_final.pth&quot;)
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model
# Set training data-set path
cfg.DATASETS.TEST = (&quot;balloon/val&quot;, )
# Create predictor (model for inference)
predictor = DefaultPredictor(cfg)

dataset_dicts = get_balloon_dicts(&quot;balloon/val&quot;)
for d in random.sample(dataset_dicts, 3):    
    im = cv2.imread(d[&quot;file_name&quot;])
    outputs = predictor(im)
    v = Visualizer(im[:, :, ::-1],
                   metadata=balloon_metadata, 
                   scale=0.8, 
                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels
    )
    v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))
    cv2_imshow(v.get_image()[:, :, ::-1])
</code></pre><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-7.png" alt="/img/content-blog-raw-blog-detectron-2-untitled-7.png"/></p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-8.png" alt="/img/content-blog-raw-blog-detectron-2-untitled-8.png"/></p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-9.png" alt="/img/content-blog-raw-blog-detectron-2-untitled-9.png"/></p><h1>Fine-tuning Chip Dataset</h1><h3>Load the data</h3><pre><code>#get the dataset
!pip install -q kaggle
!pip install -q kaggle-cli
os.environ[&#x27;KAGGLE_USERNAME&#x27;] = &quot;sparshag&quot; 
os.environ[&#x27;KAGGLE_KEY&#x27;] = &quot;1b1f894d1fa6febe9676681b44ad807b&quot;
!kaggle datasets download -d tannergi/microcontroller-detection
!unzip microcontroller-detection.zip
</code></pre><h3>Convert dataset into Detectron2&#x27;s standard format</h3><pre><code># Registering the dataset
from detectron2.structures import BoxMode
def get_microcontroller_dicts(csv_file, img_dir):
    df = pd.read_csv(csv_file)
    df[&#x27;filename&#x27;] = df[&#x27;filename&#x27;].map(lambda x: img_dir+x)

    classes = [&#x27;Raspberry_Pi_3&#x27;, &#x27;Arduino_Nano&#x27;, &#x27;ESP8266&#x27;, &#x27;Heltec_ESP32_Lora&#x27;]

    df[&#x27;class_int&#x27;] = df[&#x27;class&#x27;].map(lambda x: classes.index(x))

    dataset_dicts = []
    for filename in df[&#x27;filename&#x27;].unique().tolist():
        record = {}
        
        height, width = cv2.imread(filename).shape[:2]
        
        record[&quot;file_name&quot;] = filename
        record[&quot;height&quot;] = height
        record[&quot;width&quot;] = width

        objs = []
        for index, row in df[(df[&#x27;filename&#x27;]==filename)].iterrows():
          obj= {
              &#x27;bbox&#x27;: [row[&#x27;xmin&#x27;], row[&#x27;ymin&#x27;], row[&#x27;xmax&#x27;], row[&#x27;ymax&#x27;]],
              &#x27;bbox_mode&#x27;: BoxMode.XYXY_ABS,
              &#x27;category_id&#x27;: row[&#x27;class_int&#x27;],
              &quot;iscrowd&quot;: 0
          }
          objs.append(obj)
        record[&quot;annotations&quot;] = objs
        dataset_dicts.append(record)
    return dataset_dicts

classes = [&#x27;Raspberry_Pi_3&#x27;, &#x27;Arduino_Nano&#x27;, &#x27;ESP8266&#x27;, &#x27;Heltec_ESP32_Lora&#x27;]
for d in [&quot;train&quot;, &quot;test&quot;]:
  DatasetCatalog.register(&#x27;microcontroller/&#x27; + d, lambda d=d: get_microcontroller_dicts(&#x27;Microcontroller Detection/&#x27; + d + &#x27;_labels.csv&#x27;, &#x27;Microcontroller Detection/&#x27; + d+&#x27;/&#x27;))
  MetadataCatalog.get(&#x27;microcontroller/&#x27; + d).set(thing_classes=classes)
microcontroller_metadata = MetadataCatalog.get(&#x27;microcontroller/train&#x27;)
</code></pre><h3>Model configuration and training</h3><pre><code># Train the model
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml&quot;))
cfg.DATASETS.TRAIN = (&#x27;microcontroller/train&#x27;,)
cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml&quot;)
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.MAX_ITER = 1000
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()
</code></pre><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-10.png" alt="/img/content-blog-raw-blog-detectron-2-untitled-10.png"/></p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-11.png" alt="/img/content-blog-raw-blog-detectron-2-untitled-11.png"/></p><h3>Inference and Visualization</h3><pre><code>cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, &quot;model_final.pth&quot;)
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8   # set the testing threshold for this model
cfg.DATASETS.TEST = (&#x27;microcontroller/test&#x27;, )
predictor = DefaultPredictor(cfg)

df_test = pd.read_csv(&#x27;Microcontroller Detection/test_labels.csv&#x27;)

dataset_dicts = DatasetCatalog.get(&#x27;microcontroller/test&#x27;)
for d in random.sample(dataset_dicts, 3):    
    im = cv2.imread(d[&quot;file_name&quot;])
    outputs = predictor(im)
    v = Visualizer(im[:, :, ::-1], 
                   metadata=microcontroller_metadata, 
                   scale=0.8
                   )
    v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))
    cv2_imshow(v.get_image()[:, :, ::-1])
</code></pre><h3>Real-time Webcam inference</h3><pre><code>from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename=&#x27;photo.jpg&#x27;, quality=0.8):
  js = Javascript(&#x27;&#x27;&#x27;
    async function takePhoto(quality) {
      const div = document.createElement(&#x27;div&#x27;);
      const capture = document.createElement(&#x27;button&#x27;);
      capture.textContent = &#x27;Capture&#x27;;
      div.appendChild(capture);

      const video = document.createElement(&#x27;video&#x27;);
      video.style.display = &#x27;block&#x27;;
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) =&gt; capture.onclick = resolve);

      const canvas = document.createElement(&#x27;canvas&#x27;);
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext(&#x27;2d&#x27;).drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL(&#x27;image/jpeg&#x27;, quality);
    }
    &#x27;&#x27;&#x27;)
  display(js)
  data = eval_js(&#x27;takePhoto({})&#x27;.format(quality))
  binary = b64decode(data.split(&#x27;,&#x27;)[1])
  with open(filename, &#x27;wb&#x27;) as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print(&#x27;Saved to {}&#x27;.format(filename))
  
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))
</code></pre><pre><code>model_path = &#x27;/content/output/model_final.pth&#x27;
config_path= model_zoo.get_config_file(&quot;COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml&quot;)

# Create config
cfg = get_cfg()
cfg.merge_from_file(config_path)
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1
cfg.MODEL.WEIGHTS = model_path

predictor = DefaultPredictor(cfg)

im = cv2.imread(&#x27;photo.jpg&#x27;)
outputs = predictor(im)

v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))
cv2_imshow(v.get_image()[:, :, ::-1])
</code></pre><h1>Fine-tuning on Face dataset</h1><p>The process is same. Here is the output.</p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-12.png" alt="/img/content-blog-raw-blog-detectron-2-untitled-12.png"/></p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-13.png" alt="/img/content-blog-raw-blog-detectron-2-untitled-13.png"/></p><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-14.png" alt="/img/content-blog-raw-blog-detectron-2-untitled-14.png"/></p><h3>Behind the scenes</h3><p><img src="/img/content-blog-raw-blog-detectron-2-untitled-15.png" alt="/img/content-blog-raw-blog-detectron-2-untitled-15.png"/></p><h3>References</h3><ul><li><a href="https://medium.com/deepvisionguru/how-to-embed-detectron2-in-your-computer-vision-project-817f29149461">How to embed Detectron2 in your computer vision project - blogpost</a></li><li><a href="https://gilberttanner.com/blog/detectron2-train-a-instance-segmentation-model">Detectron2 Train a Instance Segmentation Model by Gilbert Tanner</a></li><li><a href="https://www.dlology.com/blog/how-to-train-detectron2-with-custom-coco-datasets/">How to train Detectron2 with Custom COCO Datasets - DLology</a></li><li><a href="https://towardsdatascience.com/character-recognition-and-segmentation-for-custom-data-using-detectron2-599de82b393c">Character Recognition and Segmentation For Custom Data Using Detectron2 - blogpost</a></li><li><a href="https://www.celantur.com/blog/panoptic-segmentation-in-detectron2/">Training models with Panoptic Segmentation in Detectron2</a></li><li><a href="https://www.kaggle.com/lewisgmorris/image-segmentation-using-detectron2">Image segmentation using Detectron2 - Kaggle</a></li><li><a href="https://towardsdatascience.com/a-beginners-guide-to-object-detection-and-computer-vision-with-facebook-s-detectron2-700b6273390e">A Beginner‚Äôs Guide To Object Detection And Computer Vision With Facebook‚Äôs Detectron2</a></li><li><a href="https://www.curiousily.com/posts/face-detection-on-custom-dataset-with-detectron2-in-python/">Face Detection on Custom Dataset with Detectron2 and PyTorch using Python</a></li><li><a href="https://www.notion.so/Detectron-2-d31ac9c14a8d4d9888882df14a4e0eee">My Experiment Notion</a></li><li><a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5">Official Colab</a></li><li><a href="https://research.fb.com/wp-content/uploads/2019/12/4.-detectron2.pdf">Official Slide</a></li><li><a href="https://github.com/facebookresearch/detectron2">Official Git</a></li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Distributed Training of Recommender Systems]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/distributed-training-of-recommender-systems</link>
            <guid>/2021/10/01/distributed-training-of-recommender-systems</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[The usage and importance of recommender systems are increasing at a fast pace. And deep learning is gaining traction as the preferred choice for model architecture. Giants like Google and Facebook are already using recommenders to earn billions of dollars.]]></description>
            <content:encoded><![CDATA[<p>The usage and importance of recommender systems are increasing at a fast pace. And deep learning is gaining traction as the preferred choice for model architecture. Giants like Google and Facebook are already using recommenders to earn billions of dollars.</p><p>Recently, Facebook shared its approach to maintain its 12 trillion parameter recommender. Building these large systems is challenging because it requires huge computation and memory resources. And we will soon enter into 100 trillion range. And SMEs will not be left behind due to open-source environment of software architectures and the decreasing cost of hardware, especially on the cloud infrastructure.</p><p>As per one estimate, a model with 100 trillion parameters would require at least 200TB just to store the model, even at 16-bit floating-point accuracy. So we need architectures that can support efficient and distributed training of recommendation models.</p><p><strong><em>Memory-intensive vs Computation-intensive</em></strong>: The increasing parameter comes mostly from the embedding layer which maps each entrance of an ID type feature (such as an user ID and a session ID) into a fixed length low-dimensional embedding vector. Consider the billion scale of entrances for the ID type features in a production recommender system and the wide utilization of feature crosses, the embedding layer usually domains the parameter space, which makes this component extremely <strong>memory-intensive</strong>. On the other hand, these low-dimensional embedding vectors are concatenated with diversified Non-ID type features (e.g., image, audio, video, social network, etc.) to feed a group of increasingly sophisticated neural networks (e.g., convolution, LSTM, multi-head attention) for prediction(s). Furthermore, in practice, multiple objectives can also be combined and optimized simultaneously for multiple tasks. These mechanisms make the rest neural network increasingly <strong>computation-intensive</strong>.</p><p><img src="/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled.png" alt="An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network."/></p><p>An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network.</p><p><a href="https://github.com/alibaba/x-deeplearning">Alibaba&#x27;s XDL</a>, <a href="https://github.com/PaddlePaddle/PaddleRec">Baidu&#x27;s PaddleRec</a>, and <a href="https://github.com/persiaml/persia">Kwai&#x27;s Persia</a> are some open-source frameworks for this large-scale distributed training of recommender systems.</p><aside> üìå ***Synchronous vs Asynchronous Algorithms***: Synchronous algorithms always use the up-to-date gradient to update the model to ensure the model accuracy. However, the overhead of communications for synchronous algorithms starts to become too expensive to scale out the training procedure, causing inefficiency in running time. While asynchronous algorithm have better hardware efficiency, it often leads to a ‚Äúsignificant‚Äù loss in model accuracy at this scale‚Äîfor production recommender systems (e.g., Baidu‚Äôs search engine). Recall that even 0.1% drop of accuracy would lead to a noticeable loss in revenue.</aside><h3>Parameter Server Framework</h3><p>Existing distributed systems for deep learning based recommender models are usually built on top of the parameter server (PS) framework, where one can add elastic distributed storage to hold the increasingly large amount of parameters of the embedding layer. On the other hand, the computation workload does not scale linearly with the increasing parameter scale of the embedding layer‚Äîin fact, with an efficient implementation, a lookup operation over a larger embedding table would introduce almost no additional computations.</p><p><img src="/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-1.png" alt="Left: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. [Source](https://arxiv.org/pdf/2111.05897v1.pdf)."/></p><p>Left: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. <a href="https://arxiv.org/pdf/2111.05897v1.pdf">Source</a>.</p><h3>PERSIA</h3><p><strong>PERSIA</strong>¬†(<strong>P</strong>arallel r<strong>E</strong>commendation t<strong>R</strong>aining¬†<strong>S</strong>ystem with hybr<strong>I</strong>d¬†<strong>A</strong>cceleration) is a PyTorch-based system for training deep learning recommendation models on commodity hardware. It supports models containing more than 100 trillion parameters.</p><p>It uses a hybrid training algorithm to tackle the embedding layer and dense neural network modules differently‚Äîthe embedding layer is trained in an asynchronous fashion to improve the throughput of training samples, while the rest neural network is trained in a synchronous fashion to preserve the statistical efficiency.</p><p>It also uses a distributed system to manage the hybrid computation resources (CPUs and GPUs) to optimize the co-existence of asynchronicity and synchronicity in the training algorithm.</p><p><img src="/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-2.png" alt="Untitled"/></p><p><img src="/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-3.png" alt="Untitled"/></p><p>Persia includes a data loader module, a embedding PS (Parameter Server) module, a group of embedding workers over CPU nodes, and a group of NN workers over GPU instances. Each module can be dynamically scaled for different model scales and desired training throughput:</p><ul><li>A data loader that fetches training data from distributed storages such as Hadoop, Kafka, etc;</li><li>A embedding parameter server (embedding PS for short) manages the storage and update of the parameters in the embedding layer $\mathrm{w}^{emb}$;</li><li>A group of embedding workers that runs Algorithm 1 for getting the embedding parameters from the embedding PS; aggregating embedding vectors (potentially) and putting embedding gradients back to embedding PS;</li><li>A group of NN workers that runs the forward-/backward- propagation of the neural network $\mathrm{NN_{w^{nn}}(¬∑)}$.</li></ul><p><img src="/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-4.png" alt="The architecture of Persia."/></p><p>The architecture of Persia.</p><p>Logically, the training procedure is conducted by Persia in a data dispatching based paradigm as below:</p><ol><li>The data loader will dispatch the ID type feature $\mathrm{x^{ID}}$ to an embedding worker‚Äîthe embedding worker will generate an unique sample ID ùúâ for this sample, buffer this sample ID with the ID type feature $\mathrm{x_\xi^{ID}}$ locally, and returns this ID ùúâ back the data loader; the data loader will associate this sample‚Äôs Non-ID type features and labels with this unique ID.</li><li>Next, the data loader will dispatch the Non-ID type feature and label(s) $\mathrm{(x<em>\xi^{NID},y</em>\xi)}$ to a NN worker.</li><li>Once a NN worker receives this incomplete training sample, it will issue a request to pull the ID type features‚Äô $\mathrm{(x<em>\xi^{ID})}$ embedding $\mathrm{w</em>\xi^{emb}}$ from some embedding worker according to the sample ID ùúâ‚Äîthis would trigger the forward propagation in Algorithm 1, where the embedding worker will use the buffered ID type feature $\mathrm{x<em>\xi^{ID}}$ to get the corresponding $\mathrm{w</em>\xi^{emb}}$ from the embedding PS.</li><li>Then the embedding worker performs some potential aggregation of original embedding vectors. When this computation finishes, the aggregated embedding vector $\mathrm{w_\xi^{emb}}$ will be transmitted to the NN worker that issues the pull request.</li><li>Once the NN worker gets a group of complete inputs for the dense module, it will create a mini-batch and conduct the training computation of the NN according to Algorithm 2. Note that the parameter of the NN always locates in the device RAM of the NN worker, where the NN workers synchronize the gradients by the AllReduce Paradigm.</li><li>When the iteration of Algorithm 2 is finished, the NN worker will send the gradients of the embedding ($\mathrm{F_\xi^{emb&#x27;}}$) back to the embedding worker (also along with the sample ID ùúâ).</li><li>The embedding worker will query the buffered ID type feature $\mathrm{x<em>\xi^{ID}}$ according to the sample ID ùúâ; compute gradients $\mathrm{F</em>\xi^{emb&#x27;}}$ of the embedding parameters and send the gradients to the embedding PS, so that the embedding PS can finally compute the updates according the embedding parameter‚Äôs gradients by its SGD optimizer and update the embedding parameters.</li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Document Recommendation]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/document-recommendation</link>
            <guid>/2021/10/01/document-recommendation</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-document-recommendation-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-document-recommendation-untitled.png" alt="/img/content-blog-raw-blog-document-recommendation-untitled.png"/></p><h2><strong>Introduction</strong></h2><h3>Business objective</h3><p>For the given user query, recommend relevant documents (BRM_ifam)</p><h3>Technical objective</h3><p>1-to-N mapping of given input text</p><h2><strong>Proposed Framework 1 ‚Äî Hybrid Recommender System</strong></h2><ul><li>Text ‚Üí Vector (Universal Sentence Embedding with TF Hub)</li><li>Vector ‚Üí Content-based Filtering Recommendation</li><li>Index ‚Üí Interaction Matrix</li><li>Interaction Matrix ‚Üí Collaborative Filtering Recommendation</li><li>Collaborative + Content-based ‚Üí Hybrid Recommendation</li><li>Evaluation: Area-under-curve</li></ul><h2><strong>Proposed Framework 2 ‚Äî Content-based Recommender System</strong></h2><ol><li>Find A most similar user ‚Üí Cosine similarity</li><li>For each user in A, find TopK Most Similar Items ‚Üí Map Argsort</li><li>For each item Find TopL Most Similar Items ‚Üí Cosine similarity</li><li>Display</li><li>Implement an evaluation metric</li><li>Evaluate</li></ol><h2><strong>Results and Discussion</strong></h2><ul><li>build.py ‚Üí this script will take the training data as input and save all the required files in the same working directory</li><li>recommend.py ‚Üí this script will take the user query as input and predict top-K BRM recommendations</li></ul><p>Variables (during recommendation, you will be asked 2‚Äì3 choices, the meaning of those choices are as following)</p><ul><li>top-K ‚Äî how many top items you want to get in recommendation</li><li>secondary items: this will determine how many similar items you would like to add in consideration, for each primary matching item</li><li>sorted by frequency: since multiple input queries might point to same output, therefore this option allows to take that frequence count of outputs in consideration and will move the more frequent items at the top.</li></ul><h3><strong>Code</strong></h3><p><a href="https://gist.github.com/sparsh-ai/4e5f06ba3c55192b33a276ee67dbd42c#file-text-recommendations-ipynb">https://gist.github.com/sparsh-ai/4e5f06ba3c55192b33a276ee67dbd42c#file-text-recommendations-ipynb</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fake Voice Detection]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/fake-voice-detection</link>
            <guid>/2021/10/01/fake-voice-detection</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-fake-voice-detection-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-fake-voice-detection-untitled.png" alt="/img/content-blog-raw-blog-fake-voice-detection-untitled.png"/></p><h1>Introduction</h1><p>Fake audio can be used for malicious purposes which affect directly or indirectly human life. The objective is to differentiate between fake and real voice. Python and deep learning has been used and implemented to achieve the objective. Audio files or video file are being used as an input of this work then model has been trained for uniquely identify features for voice creation and voice detection. Deep learning technique is used to find accuracy between real and fake.</p><p>Speaker recognition usually refers to both speaker identification and speaker verification. A speaker identification system identifies who the speaker is, while an automatic speaker verification (ASV) system decides if an identity claim is true or false.
A general ASV system is robust to zero-effort impostors, they are¬†vulnerable to more sophisticated attacks. Such vulnerability represents one of the security concerns of ASV systems.¬†Spoofing involves an adversary (attacker) who masquerades as the target speaker to gain the access to a system.¬†Such spoofing attacks can happen to various biometric traits, such as fingerprints, iris, face, and voice patterns. We are focusing only on the voice-based spoofing and anti-spoofing techniques for ASV system.¬†The spoofed speech samples can be obtained through speech synthesis, voice conversion, or replay of recorded speech.¬†<strong>Imagine the following scenario‚Ä¶</strong>
Your phone rings, you pick up. It‚Äôs your spouse asking you for details about your savings account ‚Äî they don‚Äôt have the account information on hand, but want to deposit money there this afternoon. Later, you realize a bunch of money has went missing! After investigating, you find out that the person masquerading as them on the other line was a voice 100% generated with AI. You‚Äôve just been scammed, and on top of that, can‚Äôt believe the voice you thought belonged to your spouse was actually a fake.</p><p>To discern between real and fake audio, the detector uses visual representations of audio clips called spectrograms, which are also used to train speech synthesis models.
Google‚Äôs 2019¬†<a href="https://www.blog.google/outreach-initiatives/google-news-initiative/advancing-research-fake-audio-detection/">AVSSpoof dataset</a>¬†contains over 25,000 clips of audio, featuring both real and fake clips of a variety of male and female speakers.<strong>Temporal Convolution Model</strong></p><h1>Modeling Approach</h1><p>First, raw audio is preprocessed and converted into a mel-frequency spectrogram ‚Äî this is the input for the model. The model performs convolutions over the time dimension of the spectrogram, then uses masked pooling to prevent overfitting. Finally, the output is passed into a dense layer and a sigmoid activation function, which ultimately outputs a predicted probability between 0 (fake) and 1 (real).
The baseline model achieved 99%, 95%, and 85% accuracy on the train, validation, and test sets respectively. The differing performance is caused by differences between the three datasets. While all three datasets feature distinct and different speakers, the test set uses a different set of fake audio generating algorithms that were not present in the train or validation set.</p><h1>Proposed Framework</h1><h1>Process Flow</h1><ul><li>Voice detection<ul><li>Temporal Convolution model<ul><li>Install packages</li><li>Download pretrained models</li><li>Initialize the model</li><li>Load data</li><li>Detect DeepFakes</li></ul></li><li>GMM-UBG model<ul><li>Install packages</li><li>Train the model</li><li>Load data</li><li>Detect DeepFakes</li></ul></li><li>Convolutional VAE model<ul><li>Install packages</li><li>Train the model</li><li>Load data</li><li>Detect DeepFakes</li></ul></li><li>Voice Similarity<ul><li>Install packages</li><li>Load data</li><li>Voice similarity match</li><li>Embedding visualization</li></ul></li></ul></li></ul><h1>Models Algorithms</h1><ol><li>Temporal Convolution</li><li>ResNet</li><li>GMM</li><li>Light CNN</li><li>Fusion</li><li>SincNet</li><li>ASSERT</li><li>HOSA</li><li>CVAE</li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Finding Hardware Parts in Warehouse]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/finding-hardware-parts-in-warehouse</link>
            <guid>/2021/10/01/finding-hardware-parts-in-warehouse</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Imagine a customer visits a store to buy hardware parts (e.g. a PVC  pipe or a concrete block) and describe the requirements in a natural language like 'I need 2" x 5" concrete block' but the exact description of this part might be different in the seller's database e.g. 'concrete block solid of width 2 inch and height 5 inches'. So the objective is to build a system that will help the store owner to find and offer the right item from the database for the description given by the customer.]]></description>
            <content:encoded><![CDATA[<p>Imagine a customer visits a store to buy hardware parts (e.g. a PVC  pipe or a concrete block) and describe the requirements in a natural language like <code>&#x27;I need 2&quot; x 5&quot; concrete block&#x27;</code> but the exact description of this part might be different in the seller&#x27;s database e.g. <code>&#x27;concrete block solid of width 2 inch and height 5 inches&#x27;</code>. So the objective is to build a system that will help the store owner to find and offer the right item from the database for the description given by the customer. </p><h1>Proposed Solution</h1><p>Semantic text similarity to map the queries. Bag-of-words based Count vectorizer model with different types of tokenization process and n-gram range. </p><h1>Modeling Approach</h1><h3>Data Description</h3><p>Data 1 - Core training data with 2 columns - Part ID and Description, to find top similar queries</p><p>Data 2 - Lookup data to find the top most similar query</p><p>Data 3 - Lookup data to fetch the exact description if description is exactly matching in the database</p><p>Data 4 - List of keywords for a tokenizer</p><p>Data 5 - List of substitutions for more accurate similarity</p><p>Data 6 - Test data with 2 columns - Description and part ID (optional)</p><h3>Modeling</h3><p>Count vectorizer models</p><ul><li>Unigram, Bigram and Trigram tokens</li><li>Non-alphabet tokens</li><li>Keyword tokens</li><li>M1 - Text similarity model trained on small file</li><li>M2 - Text similarity model trained on large file</li><li>M3 - Text similarity model trained on mixed file</li></ul><h3>Algorithm</h3><ul><li>1-gram and 2-gram tokenization</li><li>Count vectorization</li><li>Cosine or Euclidean distance</li></ul><h1>Final Repository Structure</h1><p><img src="/img/content-blog-raw-blog-finding-hardware-parts-in-warehouse-untitled.png" alt="/img/content-blog-raw-blog-finding-hardware-parts-in-warehouse-untitled.png"/></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Image Similarity System]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/image-similarity-system</link>
            <guid>/2021/10/01/image-similarity-system</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-image-similarity-system-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-image-similarity-system-untitled.png" alt="/img/content-blog-raw-blog-image-similarity-system-untitled.png"/></p><h1>Choice of variables</h1><h3>Image Encoder</h3><p>We can select any pre-trained image classification model. These models are commonly known as encoders because their job is to encode an image into a feature vector. I analyzed four encoders named 1) MobileNet, 2) EfficientNet, 3) ResNet and 4) <a href="https://tfhub.dev/google/bit/m-r152x4/1">BiT</a>. After basic research, I decided to select BiT model because of its performance and state-of-the-art nature. I selected the BiT-M-50x3 variant of model which is of size 748 MB. More details about this architecture can be found on the official page <a href="https://tfhub.dev/google/bit/m-r50x3/1">here</a>. </p><h3>Vector Similarity System</h3><p>Images are represented in a fixed-length feature vector format. For the given input vector, we need to find the TopK most similar vectors, keeping the memory efficiency and real-time retrival objective in mind. I explored the most popular techniques and listed down five of them: Annoy, Cosine distance, L1 distance, Locally Sensitive Hashing (LSH) and Image Deep Ranking. I selected Annoy because of its fast and efficient nature. More details about Annoy can be found on the official page <a href="https://github.com/spotify/annoy">here</a>.</p><h3>Dataset</h3><p>I listed down 3 datasets from Kaggle that were best fitting the criteria of this use case: 1) <a href="https://www.kaggle.com/bhaskar2443053/fashion-small?">Fashion Product Images (Small)</a>, 2) <a href="https://www.kaggle.com/trolukovich/food11-image-dataset?">Food-11 image dataset</a> and 3) <a href="https://www.kaggle.com/jessicali9530/caltech256?">Caltech 256 Image Dataset</a>. I selected Fashion dataset and Foods dataset.</p><h1>Literature review</h1><ul><li>Determining Image similarity with Quasi-Euclidean Metric <a href="https://arxiv.org/abs/2006.14644v1">arxiv</a></li><li>CatSIM: A Categorical Image Similarity Metric <a href="https://arxiv.org/abs/2004.09073v1">arxiv</a></li><li>Central Similarity Quantization for Efficient Image and Video Retrieval <a href="https://arxiv.org/abs/1908.00347v5">arxiv</a></li><li>Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval <a href="https://arxiv.org/abs/1803.02987v3">arxiv</a></li><li>Model-based Behavioral Cloning with Future Image Similarity Learning <a href="https://arxiv.org/abs/1910.03157v1">arxiv</a></li><li>Why do These Match? Explaining the Behavior of Image Similarity Models <a href="https://arxiv.org/abs/1905.10797v1">arxiv</a></li><li>Learning Non-Metric Visual Similarity for Image Retrieval <a href="https://arxiv.org/abs/1709.01353v2">arxiv</a></li></ul><h1>Process Flow</h1><h3>Step 1: Data Acquisition</h3><p>Download the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended. </p><h3>Step 2: Encoder Fine-tuning</h3><p>Download the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use.</p><p><img src="/img/content-blog-raw-blog-image-similarity-system-untitled-1.png" alt="Fig: a screenshot of encoder fine-tuning process"/></p><p>Fig: a screenshot of encoder fine-tuning process</p><h3>Step 3: Image Vectorization</h3><p>Now, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use.</p><h3>Step 4: Metadata and Indexing</h3><p>We will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index object for later use.</p><h3>Step 5: API Call</h3><p>We will receive an image from user, encode it with our image encoder, find TopK similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user.</p><h1>Deployment</h1><p>The API was deployed on AWS cloud infrastructure using AWS Elastic Beanstalk service.</p><p><img src="/img/content-blog-raw-blog-image-similarity-system-untitled-2.png" alt="/img/content-blog-raw-blog-image-similarity-system-untitled-2.png"/></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Insurance Personalization]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/insurance-personalization</link>
            <guid>/2021/10/01/insurance-personalization</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Author: Alexsoft]]></description>
            <content:encoded><![CDATA[<p>Author: <a href="https://www.altexsoft.com/blog/personalized-insurance/">Alexsoft</a></p><p>In a hyper-connected world, where advanced analytics and smart devices constantly re-assess and monitor risks, the traditional once-a-year insurance policy looks increasingly irrelevant and static. Insurance will become a breathing and living thing that shrinks and scales with time to accommodate the changing risks in the clients‚Äô daily lives. As technology continues to expand, real-time data from connected devices and predictive analysis from AIs and machine learning will enhance personalized insurance to benefit the client and insurer.</p><p>To satisfy the expectations of clients, insurers may need to go beyond the personalization of marketing communication and start personalizing product bundles for individuals.</p><h2>What is personalized insurance?</h2><p><strong>Personalized insurance</strong>¬†is the process of reaching insurance customers with targeted pricing, offers, and messages at the right time. Personalization spans across various types of insurance services, from health to property insurance.</p><p>Some insurers are already defining themselves as trusted advisors aiding people in navigating, anticipating, and eliminating risks rather than just paying the compensation when things go wrong.</p><p>For example, these companies use customer data from wearable and smart devices to monitor the user‚Äôs lifestyle. If the user‚Äôs data indicate the emergence of a serious medical condition, they can send the customer content designed to change their detrimental lifestyle or recommend immediate treatment. When the customer stays fit, healthy and does not carry out risky activities, their insurance cost will be decreased.</p><p><img src="/img/content-blog-raw-blog-insurance-personalization-untitled.png" alt="/img/content-blog-raw-blog-insurance-personalization-untitled.png"/></p><p>Insurers can provide personalization to customers at different levels:</p><ul><li><strong>Personalized product bundles.</strong>¬†The insurer offers a wide range of products such as health, car, life, and property insurance. So, clients can choose the specific products they want and group them in a bundle.</li><li><strong>Personalized communications.</strong>¬†Insurers use data collected from smart devices to notify customers about harmful activities and lifestyles. They also send recommendations on lifestyle changes. Some insurers take a step further to provide clients with incentives for a healthy lifestyle.</li><li><strong>Personalized insurance quote.</strong>¬†Customers are able to adjust the price of their insurance premiums by turning off the ones they don‚Äôt need at any time. Some insurers enable automatic quote adjustments depending on customer‚Äôs behavior (e.g., driving habits) or lifestyle choices (e.g., exercising).</li></ul><h3>Why is it important?</h3><p>Collecting and analyzing user data is vital in personalizing products based on individual behavior and preferences. In addition, insurers should use this data to enhance external relationships with their customers and guide their internal processes. This will eventually lead to delightful customer experiences and efficient operations.</p><p>Personalized insurance is important for many reasons:</p><p><strong>Customers expect personalized treatment.</strong>¬†Every customer wants to feel special, and the personalization of your services and products will do just that. It will make them stay loyal to you. Moreover, customers are open for personalization. According to the¬†<a href="https://www.accenture.com/_acnmedia/PDF-95/Accenture-2019-Global-Financial-Services-Consumer-Study.pdf#zoom=50">Accenture study</a>, 95 percent of new customers are ready to share their data in exchange for personalized insurance services. And about 58 percent of conservative users would be willing to do so.</p><p><strong>Driving more effective sales and increasing revenue.</strong>¬†Personalization benefits your sales and income in two ways. First, lots of people are ready to share their data with you in exchange for incentives and reduced premiums. Secondly, having access to clients‚Äô data gives you the ability to target people who are already interested in your product, thereby increasing sales and revenue at a lower cost. You will be able to reach your customers at the right time and with the product they need.</p><p><strong>Streamlining operations and working with customers more accurately.</strong>¬†Having an insight into customer preferences and behavior is crucial if you want to provide personalized services. Data obtained from social media activity, fitness trackers, GPS, and other tech can help you serve customers better.</p><h2>Success stories</h2><h3>Lemonade</h3><p>Use of AI and chatbots to personalize communications. </p><p>Lemonade is a US insurance company that uses Maya ‚Äì an AI-powered bot, to collect and analyze customer data. Maya acts as a virtual assistant that gets information, provides quotes, and handles payments. It also has the ability to provide customized answers to user‚Äôs questions and even help them make changes to existing policies. Lemonade uses Natural Action Synthesis and Natural Language Processing to ensure that Maya gets smarter the more it chats. This is possible because their machine learning model is retrained almost daily.</p><p><img src="/img/content-blog-raw-blog-insurance-personalization-untitled-1.png" alt="/img/content-blog-raw-blog-insurance-personalization-untitled-1.png"/></p><p>On top of that, the company uses big data analytics to quantify losses and predict risks by placing the client into a risk group and quoting a relevant premium. Customers are grouped according to their risk behaviors. The groups are created using algorithms that collect extensive customer data, such as health conditions.</p><h3>Cover</h3><p><strong><a href="https://cover.com/">Cover</a></strong>¬†is a US-based insurance metasearch company that notifies its clients of price drops for their premiums. Their technology works by scanning the market, looking for discounted and lowered prices of insurance premiums for their clients. Cover blends automation, mobile technology, and expert advice to provide customers with high-quality insurance protection at the best prices.</p><p>Cover compares with policy data and prices from over 30 different insurers. From the start, the customers need to provide answers to some questions, which will be used to match the client with a policy that suits their needs.</p><h3>Oscar</h3><p><strong><a href="https://www.hioscar.com/">Oscar</a></strong>¬†is a health insurer that provides its clients with a concierge team of medical professionals who give health advice and help them know if they see the best specialist for their specific health condition. They also help with finding the best doctors that accept Oscar insurance and manage and treat chronic conditions. Also, they set aside a separate concierge team in cases of emergencies that helps with the patient‚Äôs discharge and follow-up care.</p><p>Oscar‚Äôs mobile app acts as an intermediary between the user and the health system. The platform facilitates the customer‚Äôs interaction with their healthcare professionals. Clients can receive their lab reports, medical records, physician recommendations, and virtual care from the app. Oscar has also improved its high-touch services, including telemedicine and an ‚ÄúAsk your concierge‚Äù feature that connects users with a health insurance advice team.</p><p><img src="/img/content-blog-raw-blog-insurance-personalization-untitled-2.png" alt="/img/content-blog-raw-blog-insurance-personalization-untitled-2.png"/></p><h3>Alllstate</h3><p>Allstate is an auto insurance company that offers personalized car insurance to its customers using telematics programs called Drivewise and Milewise. Drivewise is offered through a mobile app that monitors the customers driving behavior and provides feedback after each drive. Customers also receive incentives for safe driving. From the app interface, clients can check their rewards and driving behavior for the last 100 trips. The customer‚Äôs premium is then calculated based on factors like speeding, abrupt braking, and time of the trip. One of the nice things about Drivewise is that even those who do not have an Allstate care insurance policy can participate in this program. Their Milewise program, as the name suggests, lets customers pay insurance based on the miles covered. So, the app monitors the distance covered by the car, and low-mileage drivers can save on insurance.</p><p><img src="/img/content-blog-raw-blog-insurance-personalization-untitled-3.png" alt="/img/content-blog-raw-blog-insurance-personalization-untitled-3.png"/></p><h2>How to approach personalization?</h2><p><img src="/img/content-blog-raw-blog-insurance-personalization-untitled-4.png" alt="/img/content-blog-raw-blog-insurance-personalization-untitled-4.png"/></p><p>Before fully investing in personalization, you need to carefully plan your approach. This will ensure you have all the pieces for success, and it will help you follow through with your plan.</p><h3>Explore existing data</h3><p>Having customer data is the minimum requirement to provide personalized services. First, you need to envision the type of personalization you want to offer. Then, make sure you have data collection channels that provide you with relevant data needed for your tasks. For instance, some of your documents may contain the required information, and you have to digitize, structure those, or extract specific details for that. So, you should audit your current information and data collection mechanisms to estimate whether you‚Äôll need any additional effort to gather this data. For instance, you may want to use¬†<a href="https://www.altexsoft.com/blog/intelligent-document-processing/">intelligent document processing</a>.</p><h3>Engage data scientists to make the proof of concept and carry out A/B tests</h3><p>Your vision on personalization may not work for every business model. Or your data quality may be low to reach project feasibility. We‚Äôve talked about that while explaining how to approach¬†<a href="https://www.altexsoft.com/blog/business/how-to-estimate-roi-and-costs-for-machine-learning-and-data-science-projects/">ROI¬†calculations¬†with machine learning projects</a>. So, you need to present the data you have to a data science team to run several experiments and build prototypes. Once they are ready, you can roll out your new algorithms for a subset of customers to run A/B tests. Their results may show that the conventional approaches work better for you or help iterate on your assumptions.</p><h3>Invest in data infrastructure</h3><p>If the A/B tests show that personalization will work for your business model, that is where automation comes into play. You can start investing in data infrastructure and¬†<a href="https://www.altexsoft.com/blog/data-pipeline-components-and-types/">analytical pipelines</a>¬†to automate data collection and analysis mechanisms.</p><p>You‚Äôll need a¬†<a href="https://www.altexsoft.com/blog/datascience/what-is-data-engineering-explaining-data-pipeline-data-warehouse-and-data-engineer-role/">data engineering team</a>¬†for that. These specialists set up connections with data sources, such as mobile, IoT, and telematics devices, enable automatic data preparation, configure storages, and integrate your infrastructure with business intelligence software that helps explore and visualize data.</p><h3>Continuously learn your customers‚Äô preferences and needs</h3><p>The data you collect is only as good as the insights gained from it. That is why it is vital to have a¬†<a href="https://www.altexsoft.com/blog/business/complete-guide-to-business-intelligence-and-analytics-strategy-steps-processes-and-tools/">comprehensive analytic solution</a>. A high-quality analytic software will transform the data into your most valuable asset. This data will be used to improve product development, make more accurate decisions, and provide personalized services to your customers.</p><h3>Iterate on your infrastructure and algorithms</h3><p>Personalization isn‚Äôt a one-time project. Whether you apply machine learning or build personalization based on rule-based systems, you still have to revisit your technology, continuously gather new data, and adapt your workflows.</p><h3>Ensure a personalized cross-channel experience</h3><p>Since the data collected from IoT devices and other tech is vital for personalization, it is important to make the customer experience seamless across different communication channels. Therefore, the customer should always be provided with the same level of personalization regardless of the touchpoint.</p><h2>Challenges</h2><p><strong>Personalization is financially intensive.</strong>¬†The ability of insurers to personalize insurance differs only marginally between marketing communications and products. Most of them, especially startups, do not have the funds to implement advanced technologies like machine learning needed for personalized insurance. However, insurers do not need to start with all the levels of personalization. They can often start by customizing their customer service, gathering data and insights, and then gradually developing towards more complex systems.</p><p><strong>Complex process involving multiple parties.</strong>¬†Also, it is difficult to balance personalization with financial targets, especially when establishing a price for risk. In-depth personalization of insurance must use data analytics from different sources to ensure that personalized offers reflect the client‚Äôs needs as well as the profitability and risks implications for the company.</p><p><strong>Customer data is heavily regulated.</strong>¬†Customer data from different sources are subject to industry regulations and privacy concerns. It is often a difficult task to obtain approval from regulators to use this data. Also, customers are becoming more aware of how companies are using their data and approve strict regulations. That is why laws such as General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) have been passed, which gives customers more control over their data. Insurers can address this barrier by explaining to people how their systems work and how personal data is used. Read more on¬†<a href="https://www.altexsoft.com/blog/interpretability-machine-learning/">explainable machine learning</a>¬†in our dedicated article. Besides being open, insurers can provide clients with incentives and other services for free in exchange for access to personal data.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Name & Address Parsing]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/name-&amp;-address-parsing</link>
            <guid>/2021/10/01/name-&amp;-address-parsing</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-name-&-address-parsing-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled.png" alt="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled.png"/></p><h1>Introduction</h1><p>Create an API that can parse and classify names and addresses given a string. We tried <a href="https://github.com/datamade/probablepeople">probablepeople</a> and <a href="https://github.com/datamade/usaddress">usaddress</a>. These work well separately but need the functionality of these packages combined, and better accuracy than what probablepeople provides.
For the API, I&#x27;d like to mimic <a href="https://parserator.datamade.us/api-docs/">this</a> with some minor modifications.
A few examples: </p><ul><li>&quot;KING JOHN A 5643 ROUTH CREEK PKWY #1314 RICHARDSON TEXAS 750820146 UNITED STATES OF AMERICA&quot; would return type: person; first_name: JOHN; last_name: KING; middle: A; street_address: 5643 ROUTH CREEK PKWY #1314; city: RICHARDSON; state: TEXAS; zip: 75082-0146; country: UNITED STATES OF AMERICA.</li><li>&quot;THRM NGUYEN LIVING TRUST 2720 SUMMERTREE CARROLLTON HOUSTON TEXAS 750062646 UNITED STATES OF AMERICA&quot; would return type: entity; name: THRM NGUYEN LIVING TRUST; street_address: 2720 SUMMERTREE CARROLLTON; state: TEXAS; city: HOUSTON; zip: 75006-2646; country: UNITED STATES OF AMERICA.</li></ul><h1>Modeling Approach</h1><h3>List of Entities</h3><p>List of Entities A - Person, Household, Corporation</p><p>List of Entities B - Person First name, Person Middle name, Person Last name, Street address, City, State, Pincode, Country, Company name</p><h3>Endpoint Configuration</h3><p><strong>OOR Endpoint</strong></p><p>Input Instance: ANDERSON, EARLINE 1423 NEW YORK AVE FORT WORTH, TX 76104 7522</p><pre><code>Output Tags:-
&lt;Type&gt; - Person/Household/Corporation
&lt;GivenName&gt;, &lt;MiddleName&gt;, &lt;Surname&gt; - if Type Person/Household
&lt;Name&gt; - Full Name - if Type Person 
&lt;Name&gt; - Household - if Type Household
&lt;Name&gt; - Corporation - If Type Corporation
&lt;Address&gt; - Full Address
&lt;StreetAddress&gt;, &lt;City&gt;, &lt;State&gt;, &lt;Zipcode&gt;, &lt;Country&gt;
~~NameConfidence, AddrConfidence~~
</code></pre><p><strong>Name Endpoint</strong></p><p>Input Instance: ANDERSON, EARLINE</p><pre><code>Output Tags:-

- &lt;Type&gt; - Person/Household/Corporation
- &lt;GivenName&gt;, &lt;MiddleName&gt;, &lt;Surname&gt; - if Type Person/Household
- &lt;Name&gt; - Full Name - if Type Person
- &lt;Name&gt; - Household - if Type Household
- &lt;Name&gt; - Corporation - If Type Corporation
- ~~NameConfidence~~
</code></pre><p><strong>Address Endpoint</strong></p><p>Input Instance: 1423 NEW YORK AVE FORT WORTH, TX 76104 7522</p><pre><code>Output Tags:-

- &lt;Address&gt; - Full Address
- &lt;StreetAddress&gt;, &lt;City&gt;, &lt;State&gt;, &lt;Zipcode&gt;, &lt;Country&gt;
- ~~AddrConfidence~~
</code></pre><h3>Process Flow</h3><ul><li>Pytorch Flair NER model</li><li>Pre trained word embeddings</li><li>Additional parsing models on top of name tags</li><li>Tagging of 1000+ records to create training data</li><li>Deployment as REST api with 3 endpoints - name parse, address parse and whole string parse</li></ul><h1>Framework</h1><p><img src="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled-1.png" alt="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled-1.png"/></p><p><img src="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled-2.png" alt="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled-2.png"/></p><h1>Tagging process</h1><p>I used Doccano (<a href="https://github.com/doccano/doccano">https://github.com/doccano/doccano</a>) for labeling the dataset. This tool is open-source and free to use. I deployed it with a one-click Heroku service (fig 1). After launching the app, log in with the provided credentials, and create a project (fig 2). Create the labels and upload the dataset (fig 3). Start the annotation process (fig 4). Now after enough annotations (you do not need complete all annotations in one go), go back to projects &gt; edit section and export the data (fig 5). Bring the exported JSON file in python and run the model training code. The whole model will automatically get trained on the new annotations. To make the training faster, you can use Nvidia GPU support.</p><p><img src="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled-3.png" alt="fig 1: screenshot taken from Doccano&#x27;s github page"/></p><p>fig 1: screenshot taken from Doccano&#x27;s github page</p><p><img src="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled-4.png" alt="fig 2: Doccano&#x27;s deployed app homepage"/></p><p>fig 2: Doccano&#x27;s deployed app homepage</p><p><img src="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled-5.png" alt="fig 3: create the labels. I defined these labels for my project"/></p><p>fig 3: create the labels. I defined these labels for my project</p><p><img src="/img/content-blog-raw-blog-name-&amp;-address-parsing-untitled-6.png" alt="fig 5: export the annotations"/></p><p>fig 5: export the annotations</p><h1>Model</h1><p>I first tried the Spacy NER blank model but it was not giving high-quality results. So I moved to the PyTorch Flair NER model. This model was a way faster (5 min training because of GPU compatibility comparing to 1-hour Spacy training time) and also much more accurate. F1 results for all tags were near perfect (score of 1).  This score will increase further with more labeled data. This model is production-ready.</p><h1>Inference</h1><p>For OOR, I directly used the model&#x27;s output for core tagging and created the aggregated tags like recipient (aggregation of name tags) and address (aggregation of address tags like city and state) using simple conditional concatenation. For only Name and only Address inference, I added the dummy address in name text and dummy name in address text. This way, I passed the text in same model and later on filtered the required tags as output. </p><h3>API</h3><p>I used Flask REST framework in Python to build the API with 3 endpoints. This API is production-ready.</p><h1>Results and Discussion</h1><ul><li>0.99 F1 score on 6 out of 8 tags &amp; 0.95+ F1 score on other 2 tags</li><li>API inference time of less than 1 second on single CPU</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Object Detection Hands-on Exercises]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/object-detection-hands-on-exercises</link>
            <guid>/2021/10/01/object-detection-hands-on-exercises</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[We are going to discuss the following 4 use cases:]]></description>
            <content:encoded><![CDATA[<p>We are going to discuss the following 4 use cases:</p><ol><li>Detect faces, eyes, pedestrians, cars, and number plates using OpenCV haar cascade classifiers</li><li>Streamlit app for MobileNet SSD Caffe Pre-trained model</li><li>Streamlit app for various object detection models and use cases</li><li>Detect COCO-80 class objects in videos using TFHub MobileNet SSD model</li></ol><h3>Use Case 1 -  <strong>Object detection with OpenCV</strong></h3><p><strong>Face detection</strong> - We will use the frontal face Haar cascade classifier model to detect faces in the given image. The following function first passes the given image into the classifier model to detect a list of face bounding boxes and then runs a loop to draw a red rectangle box around each detected face in the image:</p><pre><code class="language-python">def detect_faces(fix_img):
    face_rects = face_classifier.detectMultiScale(fix_img)
    for (x, y, w, h) in face_rects:
        cv2.rectangle(fix_img,
                     (x,y),
                     (x+w, y+h),
                     (255,0,0),
                     10)
    return fix_img
</code></pre><p><strong>Eyes detection</strong> - The process is almost similar to the face detection process. Instead of frontal face Haar cascade, we will use the eye detection Haar cascade model.</p><p><img src="/img/content-blog-raw-blog-object-detection-with-opencv-untitled.png" alt="Input image"/></p><p>Input image</p><p><img src="/img/content-blog-raw-blog-object-detection-with-opencv-untitled-1.png" alt="detected faces and eyes in the image"/></p><p>detected faces and eyes in the image</p><p><strong>Pedestrian detection</strong> - We will use the full-body Haar cascade classifier model for pedestrian detection. We will apply this model to a video this time. The following function will run the model on each frame of the video to detect the pedestrians:</p><pre><code class="language-python"># While Loop
while cap.isOpened():
    # Read the capture
        ret, frame = cap.read()
    # Pass the Frame to the Classifier
        bodies = body_classifier.detectMultiScale(frame, 1.2, 3)
    # if Statement
        if ret == True:
        # Bound Boxes to Identified Bodies
                for (x,y,w,h) in bodies:
            cv2.rectangle(frame,
                         (x,y),
                         (x+w, y+h),
                         (25,125,225),
                         5)
            cv2.imshow(&#x27;Pedestrians&#x27;, frame) 
        # Exit with Esc button
                if cv2.waitKey(1) == 27:
            break  
    # else Statement
        else:
        break
    
# Release the Capture &amp; Destroy All Windows
cap.release()
cv2.destroyAllWindows()
</code></pre><p><strong>Car detection</strong> - The process is almost similar to the pedestrian detection process. Again, we will use this model on a video. Instead of people Haar cascade, we will use the car cascade model.</p><p><strong>Car number plate detection</strong> - The process is almost similar to the face and eye detection process. We will use the car number plate cascade model.</p><p><em>You can find the code <a href="https://github.com/sparsh-ai/0D7ACA15">here</a> on Github.</em></p><h3>Use Case 2 - MobileNet SSD Caffe Pre-trained model</h3><p><em>You can play with the live app <a href="https://share.streamlit.io/sparsh-ai/streamlit-5a407279/app.py">here</a>. Souce code is available</em> <a href="https://github.com/sparsh-ai/streamlit-489fbbb7">here</a> <em>on Github.</em></p><h3>Use Case 3 - YOLO Object Detection App</h3><p><em>You can play with the live app</em> <a href="https://share.streamlit.io/sparsh-ai/streamlit-489fbbb7/app.py">*here</a>. Source code is available <a href="https://github.com/sparsh-ai/streamlit-5a407279/tree/master">here</a> on Github.*</p><p>This app can detect COCO 80-classes using three different models - Caffe MobileNet SSD, Yolo3-tiny, and Yolo3. It can also detect faces using two different models - SSD Res10 and OpenCV face detector.  Yolo3-tiny can also detect fires.</p><p><img src="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png" alt="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png"/></p><p><img src="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png" alt="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png"/></p><h3>Use Case 4 - TFHub MobileNet SSD on Videos</h3><p>In this section, we will use the MobileNet SSD object detection model from TFHub. We will apply it to videos. We can load the model using the following command:</p><pre><code class="language-python">module_handle = &quot;https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1&quot;
detector = hub.load(module_handle).signatures[&#x27;default&#x27;]
</code></pre><p>After loading the model, we will capture frames using OpenCV video capture method, and pass each frame through the detection model:</p><pre><code class="language-python">cap = cv2.VideoCapture(&#x27;/content/Spectre_opening_highest_for_a_James_Bond_film_in_India.mp4&#x27;)
for i in range(1,total_frames,200):
    cap.set(cv2.CAP_PROP_POS_FRAMES,i)
    ret,frame = cap.read()
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    run_detector(detector,frame)
</code></pre><p>Here are some detected objects in frames: </p><p><img src="/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled.png" alt="/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled.png"/></p><p><img src="/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-1.png" alt="/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-1.png"/></p><p><img src="/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-2.png" alt="/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-2.png"/></p><p><em>You can find the code <a href="https://gist.github.com/sparsh-ai/32ff6fe8c073f6be5d893029e4dc2960">here</a> on Github.</em></p><hr/><p><em>Congrats! In the next post of this series, we will cover 5 exciting use cases - 1) detectron 2 object detection fine-tuning on custom class, 2) Tensorflow Object detection API inference, fine-tuning, and few-shot learning, 3) Inference with 6 pre-trained models, 4) Mask R-CNN object detection app, and 5) Logo detection app deployment as a Rest API using AWS elastic Beanstalk.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Object detection with OpenCV]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/object-detection-with-opencv</link>
            <guid>/2021/10/01/object-detection-with-opencv</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Face detection]]></description>
            <content:encoded><![CDATA[<h2><strong>Face detection</strong></h2><p>We will use the frontal face Haar cascade classifier model to detect faces in the given image. The following function first passes the given image into the classifier model to detect a list of face bounding boxes and then runs a loop to draw a red rectangle box around each detected face in the image:</p><pre><code class="language-python">def detect_faces(fix_img):
    face_rects = face_classifier.detectMultiScale(fix_img)
    for (x, y, w, h) in face_rects:
        cv2.rectangle(fix_img,
                     (x,y),
                     (x+w, y+h),
                     (255,0,0),
                     10)
    return fix_img
</code></pre><h2><strong>Eyes detection</strong></h2><p>The process is almost similar to the face detection process. Instead of frontal face Haar cascade, we will use the eye detection Haar cascade model.</p><p><img src="/img/content-blog-raw-blog-object-detection-with-opencv-untitled.png" alt="Input image"/></p><p>Input image</p><p><img src="/img/content-blog-raw-blog-object-detection-with-opencv-untitled-1.png" alt="detected faces and eyes in the image"/></p><p>detected faces and eyes in the image</p><h2><strong>Pedestrian detection</strong></h2><p>We will use the full-body Haar cascade classifier model for pedestrian detection. We will apply this model to a video this time. The following function will run the model on each frame of the video to detect the pedestrians:</p><pre><code class="language-python"># While Loop
while cap.isOpened():
    # Read the capture
        ret, frame = cap.read()
    # Pass the Frame to the Classifier
        bodies = body_classifier.detectMultiScale(frame, 1.2, 3)
    # if Statement
        if ret == True:
        # Bound Boxes to Identified Bodies
                for (x,y,w,h) in bodies:
            cv2.rectangle(frame,
                         (x,y),
                         (x+w, y+h),
                         (25,125,225),
                         5)
            cv2.imshow(&#x27;Pedestrians&#x27;, frame) 
        # Exit with Esc button
                if cv2.waitKey(1) == 27:
            break  
    # else Statement
        else:
        break
    
# Release the Capture &amp; Destroy All Windows
cap.release()
cv2.destroyAllWindows()
</code></pre><h2><strong>Car detection</strong></h2><p>The process is almost similar to the pedestrian detection process. Again, we will use this model on a video. Instead of people Haar cascade, we will use the car cascade model.</p><h2><strong>Car number plate detection</strong></h2><p>The process is almost similar to the face and eye detection process. We will use the car number plate cascade model.</p><p><em>You can find the code <a href="https://github.com/sparsh-ai/0D7ACA15">here</a> on Github.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Object detection with YOLO3]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/object-detection-with-yolo3</link>
            <guid>/2021/10/01/object-detection-with-yolo3</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Live app]]></description>
            <content:encoded><![CDATA[<h2>Live app</h2><p>This app can detect COCO 80-classes using three different models - Caffe MobileNet SSD, Yolo3-tiny, and Yolo3. It can also detect faces using two different models - SSD Res10 and OpenCV face detector.  Yolo3-tiny can also detect fires.</p><p><img src="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png" alt="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png"/></p><p><img src="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png" alt="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png"/></p><h2>Code</h2><pre><code class="language-python">import streamlit as st
import cv2
from PIL import Image
import numpy as np
import os

from tempfile import NamedTemporaryFile
from tensorflow.keras.preprocessing.image import img_to_array, load_img

temp_file = NamedTemporaryFile(delete=False)

DEFAULT_CONFIDENCE_THRESHOLD = 0.5
DEMO_IMAGE = &quot;test_images/demo.jpg&quot;
MODEL = &quot;model/MobileNetSSD_deploy.caffemodel&quot;
PROTOTXT = &quot;model/MobileNetSSD_deploy.prototxt.txt&quot;

CLASSES = [
    &quot;background&quot;,
    &quot;aeroplane&quot;,
    &quot;bicycle&quot;,
    &quot;bird&quot;,
    &quot;boat&quot;,
    &quot;bottle&quot;,
    &quot;bus&quot;,
    &quot;car&quot;,
    &quot;cat&quot;,
    &quot;chair&quot;,
    &quot;cow&quot;,
    &quot;diningtable&quot;,
    &quot;dog&quot;,
    &quot;horse&quot;,
    &quot;motorbike&quot;,
    &quot;person&quot;,
    &quot;pottedplant&quot;,
    &quot;sheep&quot;,
    &quot;sofa&quot;,
    &quot;train&quot;,
    &quot;tvmonitor&quot;,
]
COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))

@st.cache
def process_image(image):
    blob = cv2.dnn.blobFromImage(
        cv2.resize(image, (300, 300)), 0.007843, (300, 300), 127.5
    )
    net = cv2.dnn.readNetFromCaffe(PROTOTXT, MODEL)
    net.setInput(blob)
    detections = net.forward()
    return detections

@st.cache
def annotate_image(
    image, detections, confidence_threshold=DEFAULT_CONFIDENCE_THRESHOLD
):
    # loop over the detections
    (h, w) = image.shape[:2]
    labels = []
    for i in np.arange(0, detections.shape[2]):
        confidence = detections[0, 0, i, 2]

        if confidence &gt; confidence_threshold:
            # extract the index of the class label from the `detections`,
            # then compute the (x, y)-coordinates of the bounding box for
            # the object
            idx = int(detections[0, 0, i, 1])
            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            (startX, startY, endX, endY) = box.astype(&quot;int&quot;)

            # display the prediction
            label = f&quot;{CLASSES[idx]}: {round(confidence * 100, 2)}%&quot;
            labels.append(label)
            cv2.rectangle(image, (startX, startY), (endX, endY), COLORS[idx], 2)
            y = startY - 15 if startY - 15 &gt; 15 else startY + 15
            cv2.putText(
                image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2
            )
    return image, labels

def main():
  selected_box = st.sidebar.selectbox(
    &#x27;Choose one of the following&#x27;,
    (&#x27;Welcome&#x27;, &#x27;Object Detection&#x27;)
    )
    
  if selected_box == &#x27;Welcome&#x27;:
      welcome()
  if selected_box == &#x27;Object Detection&#x27;:
      object_detection() 

def welcome():
  st.title(&#x27;Object Detection using Streamlit&#x27;)
  st.subheader(&#x27;A simple app for object detection&#x27;)
  st.image(&#x27;test_images/demo.jpg&#x27;,use_column_width=True)

def object_detection():
  
  st.title(&quot;Object detection with MobileNet SSD&quot;)

  confidence_threshold = st.sidebar.slider(
    &quot;Confidence threshold&quot;, 0.0, 1.0, DEFAULT_CONFIDENCE_THRESHOLD, 0.05)

  st.sidebar.multiselect(&quot;Select object classes to include&quot;,
  options=CLASSES,
  default=CLASSES
  )

  img_file_buffer = st.file_uploader(&quot;Upload an image&quot;, type=[&quot;png&quot;, &quot;jpg&quot;, &quot;jpeg&quot;])

  if img_file_buffer is not None:
      temp_file.write(img_file_buffer.getvalue())
      image = load_img(temp_file.name)
      image = img_to_array(image)
      image = image/255.0

  else:
      demo_image = DEMO_IMAGE
      image = np.array(Image.open(demo_image))

  detections = process_image(image)
  image, labels = annotate_image(image, detections, confidence_threshold)

  st.image(
      image, caption=f&quot;Processed image&quot;, use_column_width=True,
  )

  st.write(labels)

main()
</code></pre><p><em>You can play with the live app</em> <a href="https://share.streamlit.io/sparsh-ai/streamlit-489fbbb7/app.py">*here</a>. Source code is available <a href="https://github.com/sparsh-ai/streamlit-5a407279/tree/master">here</a> on Github.*</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OCR experiments]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/ocr-experiments</link>
            <guid>/2021/10/01/ocr-experiments</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-ocr-experiments-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-ocr-experiments-untitled.png" alt="/img/content-blog-raw-blog-ocr-experiments-untitled.png"/></p><h2>1. Tesseract</h2><p>Tesseract is an open-source text recognition engine that is available under the Apache 2.0 license and its development has been sponsored by Google since 2006.</p><p><a href="https://nbviewer.jupyter.org/gist/sparsh-ai/2d1f533048a3655de625298c3dd32d47">Notebook on nbviewer</a></p><h2>2. EasyOCR</h2><p>Ready-to-use OCR with 70+ languages supported including Chinese, Japanese, Korean and Thai. EasyOCR is built with Python and Pytorch deep learning library, having a GPU could speed up the whole process of detection. The detection part is using the CRAFT algorithm and the Recognition model is CRNN. It is composed of 3 main components, feature extraction (we are currently using Resnet), sequence labelling (LSTM) and decoding (CTC). EasyOCR doesn‚Äôt have much software dependencies, it can directly be used with its API.</p><p><a href="https://nbviewer.jupyter.org/gist/sparsh-ai/12359606ee4127513c66fc3b4ff18e5b">Notebook on nbviewer</a></p><h2>3. KerasOCR</h2><p>This is a slightly polished and packaged version of the Keras CRNN implementation and the published CRAFT text detection model. It provides a high-level API for training a text detection and OCR pipeline and out-of-the-box OCR models, and an end-to-end training pipeline to build new OCR models.</p><p><a href="https://nbviewer.jupyter.org/gist/sparsh-ai/2fcb764619baf5f56cf7122b1b2c527c">Notebook on nbviewer</a></p><h2>4. ArabicOCR</h2><p>It is an OCR system for the Arabic language that converts images of typed text to machine-encoded text. It currently supports only letters (29 letters).  ArabicOCR aims to solve a simpler problem of OCR with images that contain only Arabic characters (check the dataset link below to see a sample of the images).</p><p><a href="https://nbviewer.jupyter.org/gist/sparsh-ai/26df76b78f8cd2018a068b284b7cfe56">Notebook on nbviewer</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PDF to Wordcloud via Mail]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/pdf-to-wordcloud-via-mail</link>
            <guid>/2021/10/01/pdf-to-wordcloud-via-mail</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-pdf-to-wordcloud-via-mail-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-pdf-to-wordcloud-via-mail-untitled.png" alt="/img/content-blog-raw-blog-pdf-to-wordcloud-via-mail-untitled.png"/></p><h2>Objective</h2><p>Integrating PDF, Text, Wordcloud and Email functionalities in Python</p><h2>Process Flow</h2><ul><li>Step 1 - I use PyPDF2 library to read PDF text in Python</li><li>Step 2 - Import the supporting libraries</li><li>Step 3 - Count No. of Pages for this pdf and extract text for each page using loop</li><li>Step 4 - Build Text corpus by simply attaching text of next page to all the previous ones</li><li>Step 5 - Creating word frequency dataframe by first splitting text into words and counting the frequency of each word</li><li>Step 6.1 - Pre-process text i.e. removing stopwords (using nltk library), grouping common words.</li><li>Step 6.2 - used regex to extract alphabets only, lower all chracters, and sorting as per decreasing order of frequency.</li><li>Step 7 - Creating Wordcloud using matplotlib and wordcloud libraries</li><li>Step 8 - Importing required libraries like smtplib, MIME, win32 for sending the mail</li><li>Step 9 - Create outlook mail object with supporting data like filepath attachment, recepient address, mail body etc.</li><li>Step 10 - Sending the mail with required wordcloud image file attached and checking if mail is received or not!</li></ul><h2>Code</h2><p><a href="https://nbviewer.jupyter.org/gist/sparsh-ai/f1de48fd4fac199bcc95e1d136fbdfd0">Notebook on nbviewer</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Personalized Unexpectedness in  Recommender Systems]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/personalized-unexpectedness-in-recommender-systems</link>
            <guid>/2021/10/01/personalized-unexpectedness-in-recommender-systems</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Classical recommender systems typically provides familier items, which not only bores customer after some time, but create a critical bias problem also, generally known as filter bubble or echo chamber problem.]]></description>
            <content:encoded><![CDATA[<p>Classical recommender systems typically provides familier items, which not only bores customer after some time, but create a critical bias problem also, generally known as <em>filter bubble</em> or <em>echo chamber problem</em>. </p><p>To address this issue, instead of recommending best matching product all the time, we intentionally recommend a random product. For example, if a user subscribed to Netflix one month ago and watching action movies all the time. If we recommend another action movie, there is a high probability that user will click but keeping in mind the long-term user satisfaction and to address the filter bubble bias, we would recommend a comedy movie. Surprisingly, this strategy works!!</p><p>The most common metric is <strong><em>diversity</em></strong> factor but diversity only measures dispersion among recommended items. The better alternative is <strong><em>unexpectedness</em></strong> factor. It measures deviations of recommended items from user expectations and thus captures the concept of user surprise and allows recommender systems to break from the filter bubble. The goal is to provide novel, surprising and satisfying recommendations. </p><p>Including session-based information into the design of an unexpected recommender system is beneficial. For example, it is more reasonable to recommend the next episode of a TV series to the user who has just finished the first episode, instead of recommending new types of videos to that person. On the other hand, if the user has been binge-watching the same TV series in one night, it is better to recommend something different to him or her.</p><h3>Model</h3><p><img src="/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled.png" alt="/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled.png"/></p><p><em>Overview of the proposed PURS model. The base model estimates the click-through rate of certain user-item pairs, while the unexpected model captures the unexpectedness of the new recommendation as well as user perception towards unexpectedness.</em></p><h3>Offline Experiment Results</h3><p><img src="/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled-1.png" alt="/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled-1.png"/></p><h3>Online A/B Test Results</h3><p>Authors conducted the online A/B test at Alibaba-Youku, a major video recommendation platform from 2019-11 to 2019-12. During the testing period, they compared the proposed PURS model with the latest production model in the company. They measured the performance using standard business metrics: <strong>VV</strong> (Video View, average video viewed by each user), <strong>TS</strong> (Time Spent, average time that each user spends on the platform), <strong>ID</strong> (Impression Depth, average impression through one session) and <strong>CTR</strong> (Click-Through-Rate, the percentage of user clicking on the recommended video). They also measure the novelty of the recommended videos using the unexpectedness and coverage measures.</p><p><img src="/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled-2.png" alt="Represents statistical significance at the 0.95 level."/></p><p>Represents statistical significance at the 0.95 level.</p><h3>Code Walkthrough</h3><blockquote><p>Note: PURS is <em>implemented in Tensorflow 1.x</em></p></blockquote><p><strong>Unexpected attention (<a href="https://github.com/lpworld/PURS/blob/master/model.py">model.py</a>)</strong></p><pre><code class="language-python">def unexp_attention(self, querys, keys, keys_id):
        &quot;&quot;&quot;
        Same Attention as in the DIN model
        queries:     [Batchsize, 1, embedding_size]
        keys:        [Batchsize, max_seq_len, embedding_size]  max_seq_len is the number of keys(e.g. number of clicked creativeid for each sample)
        keys_id:     [Batchsize, max_seq_len]
        &quot;&quot;&quot;
        querys = tf.expand_dims(querys, 1)
        keys_length = tf.shape(keys)[1] # padded_dim
        embedding_size = querys.get_shape().as_list()[-1]
        keys = tf.reshape(keys, shape=[-1, keys_length, embedding_size])
        querys = tf.reshape(tf.tile(querys, [1, keys_length, 1]), shape=[-1, keys_length, embedding_size])

        net = tf.concat([keys, keys - querys, querys, keys*querys], axis=-1)
        for units in [32,16]:
            net = tf.layers.dense(net, units=units, activation=tf.nn.relu)
        att_wgt = tf.layers.dense(net, units=1, activation=tf.sigmoid)        # shape(batch_size, max_seq_len, 1)
        outputs = tf.reshape(att_wgt, shape=[-1, 1, keys_length], name=&quot;weight&quot;)  #shape(batch_size, 1, max_seq_len)
        scores = outputs
        scores = scores / (embedding_size ** 0.5)       # scale
        scores = tf.nn.softmax(scores)
        outputs = tf.matmul(scores, keys)    #(batch_size, 1, embedding_size)
        outputs = tf.reduce_sum(outputs, 1, name=&quot;unexp_embedding&quot;)   #(batch_size, embedding_size)
        return outputs
</code></pre><p><strong>Unexpected metric calculation (<a href="https://github.com/lpworld/PURS/blob/master/train.py">train.py</a>)</strong></p><pre><code class="language-python">def unexpectedness(sess, model, test_set):
    unexp_list = []
    for _, uij in DataInput(test_set, batch_size):
        score, label, user, item, unexp = model.test(sess, uij)
        for index in range(len(score)):
            unexp_list.append(unexp[index])
    return np.mean(unexp_list)
</code></pre><h3>References</h3><ol><li><a href="https://arxiv.org/pdf/2106.02771v1.pdf">https://arxiv.org/pdf/2106.02771v1.pdf</a></li><li><a href="https://github.com/lpworld/PURS">https://github.com/lpworld/PURS</a></li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Predicting Electronics Resale Price]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/predicting-electronics-resale-price</link>
            <guid>/2021/10/01/predicting-electronics-resale-price</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-predicting-electronics-resale-price-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-predicting-electronics-resale-price-untitled.png" alt="/img/content-blog-raw-blog-predicting-electronics-resale-price-untitled.png"/></p><h1>Objective</h1><p>Predict the resale price based on brand, part id and purchase quantity</p><h1>Milestones</h1><ul><li>Data analysis and discovery - What is the acceptable variance the model needs to meet in terms of similar part number and quantity?</li><li>Model research and validation - Does the model meet the variance requirement? (Variance of the model should meet or be below the variance of the sales history)</li><li>Model deployment - Traffic will increase 10 fold. So, model needs to be containerized or dockerized</li><li>Training - Model needs to be trainable on new sales data. Methodology to accept or reject the variance of the newly trained model documented.</li></ul><h1>Deliverables</h1><ol><li><p>Data Analysis and Discovery (identify target variance for pricing model in terms of similar part numbers and quantities). Analysis should be done on the 12 following quantity ranges: 1-4, 5-9, 10-24, 25-49, 50-99, 100-249, 250-499, 500-999, 1000-2499, 2500-4999, 5000-9999, 10000+.</p></li><li><p>ModelA Training (Resale Value Estimation <!-- -->[$]<!-- --> (Brand+PartNo.+Quantity)</p></li><li><p>ModelA Validation (variance analysis and comparison with sales history variance in terms of similar part numbers and quantities)</p></li><li><p>ModelA Containerization</p></li><li><p>ModelA re-training based on new sales data</p></li><li><p>ScriptA to calculate variance for new sales data (feedback for training results)</p></li><li><p>Documentation for re-training</p></li><li><p>ModelA deployment and API</p></li></ol><h1>Modeling Approach</h1><h3>Framework</h3><ul><li>Fully connected regression neural network</li><li>NLP feature extraction from part id</li><li>Batch generator to feed large data in batches</li><li>Hyperparameter tuning to find the best model fit</li></ul><h3>List of Variables</h3><ul><li>2 years of sales history</li><li>PRC</li><li>PARTNO</li><li>ORDER_NUMBER</li><li>ORIG_ORDER_QTY</li><li>UNIT_COST</li><li>UNIT_REASLE</li><li>UOM (UNIT OF MEASUREMENT)</li></ul><h1>Bucket of Ideas</h1><ol><li>Increase n-gram range; e.g. in part_id ABC-123-23, these are 4-grams: ABC-, BC-1, C-12, -123, 123-, 23-2, 3-23; Idea is to see if increasing this range further will increase the model&#x27;s performance</li><li>Employ Char-level LSTM to capture sequence information; e.g. in same part_id ABC-123-23, currently we are not maintaining sequence of grams, we don&#x27;t know if 3-23 is coming at first or last; here, the idea is to see if lstm model can be employed to capture this sequence information to improve model&#x27;s performance</li><li>New Loss function - including cost based loss</li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Real-time news personalization with Flink]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/real-time-news-personalization-with-flink</link>
            <guid>/2021/10/01/real-time-news-personalization-with-flink</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Overview]]></description>
            <content:encoded><![CDATA[<h2>Overview</h2><p>News recommendation system has a high degree of real-time because there will be a large number of news and hot spots at any time. Incremental updating, online learning, local updating and even reinforcement learning can make the recommender system quickly respond to the user‚Äòs new behavior, and the premise of these updating strategies is that the sample itself has enough real-time information. In news recommendation system, the typical training sample is the user‚Äôs click behavior data.</p><h3>Why is the real-time nature of the recommendation system important?</h3><p>Intuitively, when users use personalized news applications, users expect to find articles that match their interests faster; when using short video services, they expect to &quot;flash&quot; content that they are interested in faster; when doing online shopping, I also hope to find the products that I like, faster. All recommendations highlight the word &quot;fast&quot;, which is an intuitive manifestation of the &quot;real-time&quot; role of the recommendation system.</p><p>From a professional point of view, the real-time performance of the recommendation system is also crucial, which is mainly reflected in the following two aspects:</p><ol><li><strong>The faster the update speed of the recommendation system is, the more it can reflect the user&#x27;s recent user habits, and the more time-sensitive it can make recommendations to the user.</strong></li><li><strong>The faster the recommendation system is updated, the easier it is for the model to find the latest popular data patterns, and the more it can make the model react to find the latest fashion trends.</strong></li></ol><h3>The real-time nature of the &quot;feature&quot; of the recommendation system</h3><p>Suppose a user has watched a 10-minute &quot;badminton teaching&quot; video in its entirety. Then there is no doubt that the user is interested in the subject of &quot;badminton&quot;. The system hopes to continue to recommend &quot;badminton&quot; related videos when the user turns the page next time. However, due to the lack of real-time features of the system, the user‚Äôs viewing history cannot be fed back to the recommendation system in real time. As a result, the recommendation system learned that the user had watched the video &quot;Badminton Teaching&quot;. It was already half an hour later. Has left the app. This is an example of recommendation failure caused by poor real-time performance of the recommendation system.</p><p>It is true that the next time the user opens the application, the recommendation system can use the last user behavior history to recommend &quot;badminton&quot; related videos, but the recommendation system undoubtedly loses what is most likely to increase user viscosity and increase user retention. opportunity.</p><h3>The real-time nature of the &quot;model&quot; of the recommender system</h3><p>No matter how strong the real-time feature is, the scope of influence is limited to the current user. Compared with the real-time nature of &quot;features&quot;, the real-time nature of the recommendation system model is often considered from a more global perspective . The real-time nature of the feature attempts to describe a person with more accurate features, so that the recommendation system can give a recommendation result that is more in line with the person. The real-time nature of the model hopes to capture new data patterns at the global level faster and discover new trends and relevance.</p><p>Take, for example, a large number of promotional activities on Double Eleven on an e-commerce website. The real-time nature of the feature will quickly discover the products that the user may be interested in based on the user&#x27;s recent behavior, but will never find the latest preferences of similar users, the latest correlation information between the products, and the trend information of new activities.</p><p>To discover such global data changes, the model needs to be updated faster. The most important factor affecting the real-time performance of the model is the training method of the model.</p><ol><li><strong>Full update -</strong> The most common way of model training is full update.¬†The model will use all training samples in a certain period of time for retraining, and then replace the &quot;outdated&quot; model with the new trained model. However, the full update requires a large amount of training samples, so the training time required is longer; and the full update is often performed on offline big data platforms, such as spark+tensorflow, so the data delay is also longer, which leads to the full update It is the worst &quot;real-time&quot; model update method. In fact, for a model that has been trained, it is enough to learn only the newly added incremental samples, which is called incremental update.</li><li><strong>Incremental update (Incremental Learning)</strong> - Incremental update only feeds newly added samples to the model for incremental learning . Technically, deep learning models often use stochastic gradient descent (SGD) and its variants for learning. The model&#x27;s learning of incremental samples is equivalent to continuing to input incremental samples for gradient descent on the basis of the original samples. Therefore, based on the deep learning model, it is not difficult to change from full update to incremental update. But everything in engineering is a tradeoff, there is never a perfect solution, and incremental updates are no exception. Since only incremental samples are used for learning, the model also converges to the best point of the new sample after multiple epochs, and it is difficult to converge to the global best point of all the original samples + incremental samples. Therefore, in the actual recommendation system, the incremental update and the global update are often combined . After several rounds of incremental update, the global update is performed in a time window with a small business volume, and the model is corrected after the incremental update process. Accumulated errors in. Make trade-offs and trade-offs between &quot;real-time performance&quot; and &quot;global optimization&quot;.</li><li><strong>Online learning</strong> - &quot;Online learning&quot; is a further improvement of &quot;incremental update&quot;, &quot;incremental update&quot; is to perform incremental update when a batch of new samples is obtained, and online learning is to update the model in real time every time a new sample is obtained. Online learning can also be implemented technically through SGD. But if you use the general SGD method, online learning will cause a very serious problem, that is, the sparsity of the model is very poor, opening too many &quot;fragmented&quot; unimportant features. We pay attention to the &quot;sparseness&quot; of the model in a sense that is also an engineering consideration. For example, in a model with an input vector of several million dimensions, if the sparsity of the model is good, the effect of the model can be maintained without affecting the model. , Only make the corresponding weight of the input vector of a very small part of the dimension non-zero, that is to say, when the model is online, the volume of the model is very small, which is undoubtedly beneficial to the entire model serving process. Both the memory space required to store the model and the speed of online inference will benefit from the sparsity of the model. If the SGD method is used to update the model, it is easier to generate a large number of features with small weights than the batch method, which increases the difficulty of model deployment and update. So in order to take into account the training effect and model sparsity in the online learning process, there are a lot of related researches. The most famous ones include Microsoft&#x27;s RDA, Google&#x27;s FOBOS and the most famous FTRL, etc.</li><li><strong>Partial model update</strong> - Another improvement direction to improve the real-time performance of the model is to perform a partial update of the model. The general idea is to reduce the update frequency of the part with low training efficiency and increase the update frequency of the part with high training efficiency . This approach is representative of the GBDT+LR model of Facebook.</li></ol><p><img src="/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled.png" alt="/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled.png"/></p><h2>Data pipeline of a typical news recommendation system</h2><p>When a user is exposed with a list of news articles, a page view events are sent to the backend server and when that user clicks on the news of interest, the action events are also sent to the backend server. After receiving these 2 event streams (page view and clicks), the backend server will send these user behaviour events to the message queue. And message queue finally stores these messages into the distributed file system, such as HDFS.</p><p>For model training, we need a training sample. The most common sampling technique is negative sampling. In this, we generate &#x27;n&#x27; negative samples for each positive event that we receive. Users will only generate behavior for some exposed news samples, which are positive samples, and the remaining exposure samples without behavior are negative samples. After generating positive and negative samples, the model can be trained.</p><p>The recommendation system with low real-time requirements can use batch processing technology (APACHE spark is a typical tool) to generate samples, as shown in the left figure. Set a timing task, and read the user behavior log and exposure log in the time window from HDFS every other period of time, such as one hour, to perform join operation, generate training samples, and then write the training samples back to HDFS, Then start the training update of the model.</p><p><img src="/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled-1.png" alt="/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled-1.png"/></p><h3>Problems</h3><p>One obvious problem with batch processing is <strong>latency</strong>. The typical cycle of running batch tasks regularly is one hour, which means that there is a delay of at least one hour from sample generation to model training. Sometimes, if the batch platform is overloaded and the tasks need to be queued, the delay will be greater.</p><p>Another problem is the <strong>boundary</strong> problem. If page view (PV) data is generated at the end of the log time window selected by the batch task, the corresponding action data may fall into the next time window of the batch task, resulting in join failure and false negative samples.</p><p>A related problem to this is the time synchronization problem. When a news item is exposed to the user, the user may click immediately after the PV data stream is generated, or the user may act after a few minutes, more than ten minutes, or even several hours. This means that after the PV data stream arrives, it needs to wait for a period of time to join with the action data stream. If the waiting time is too long, some samples (positive samples) that should have user behavior will be wrongly marked as negative samples because the user behavior has no time to return. Too long waiting time will damage and increase the system delay. Offline analysis of the delay distribution between the actual action data stream and PV data stream is a very typical exponential distribution.</p><p><img src="/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled-2.png" alt="/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled-2.png"/></p><h2>Apache Flink to the rescue</h2><h3>How Apache Flink solves the latency problem?</h3><p>In order to enhance the real-time performance, we use Apache Flink framework to rewrite the sample generation logic with stream processing technology. As shown in the right figure above, after the user exposure and behavior logs generated by online services are written into the message queue, instead of waiting for them to drop to HDFS, we directly consume these message flows with Flink. At the same time, Flink reads the necessary feature information from the redis cache and generates the sample message stream directly. The sample message flow is written back to the Kafka queue, and downstream tensorflow can directly consume the message flow for model training.</p><h3>How Apache Flink solved the boundary and synchronization problem?</h3><p>As per the exponential distribution (analyzed on a private dataset of a news recommender app), most of the user behavior has reflow within a few minutes. And if few minutes is an acceptable delay, a simple solution is to set a time window with a compromise size. Flink provides window join to implement this logic.</p><h2>References</h2><ol><li><a href="https://developpaper.com/flink-streaming-processing-and-real-time-sample-generation-in-recommender-system/">https://developpaper.com/flink-streaming-processing-and-real-time-sample-generation-in-recommender-system/</a></li><li><a href="https://zhuanlan.zhihu.com/p/74813776">https://zhuanlan.zhihu.com/p/74813776</a></li><li><a href="https://zhuanlan.zhihu.com/p/75597761">https://zhuanlan.zhihu.com/p/75597761</a></li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Semantic Similarity]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/semantic-similarity</link>
            <guid>/2021/10/01/semantic-similarity</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-semantic-similarity-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-semantic-similarity-untitled.png" alt="/img/content-blog-raw-blog-semantic-similarity-untitled.png"/></p><h1>Introduction</h1><p>Deliverable - Two paragraph-level distance outputs for L and Q, each has 35 columns. </p><p>For each paragraph, we need to calculate the L1 distance of consecutive sentences in this paragraph, and then generate the mean and standard deviation of all these distances for this paragraph. For example, say the paragraph 1 starts from sentence1 and ends with sentence 5. First, calculate the L1 distances for L1(1,2), L1(2,3), L1(3,4) and L1(4,5) and then calculate the mean and standard deviation of the 4 distances. In the end we got two measures for this paragraph: L1_m and L1_std. Similarly, we need to calculate the mean and standard deviation using L2 distance, plus a simple mean and deviation of the distances. We use 6 different embeddings: all dimensions of BERT embeddings, 100,200 and 300 dimensions of PCA Bert embeddings (PCA is a dimension reduction technique </p><p>In the end, we will have 35 columns for each paragraph : Paragraph ID +#sentences in the paragraph +(cosine_m, cosine_std,cossimillarity_m, cosimmilarity_std, L1_m, L1_std, L2_m, L2_std ) ‚Äì by- ( all, 100, 200, 300)= 3+8*4. </p><p>Note: for paragraph that only has 1 sentence, the std measures are empty.</p><h1>Modeling Approach</h1><h3>Process Flow for Use Case 1</h3><ol><li>Splitting paragraphs into sentences using 1) NLTK Sentence Tokenizer, 2) Spacy Sentence Tokenizer and, on two additional symbols <code>:</code> and <code>...</code></li><li>Text Preprocessing: Lowercasing, Removing Non-alphanumeric characters, Removing Null records, Removing sentence records (rows) having less than 3 words.</li><li>TF-IDF vectorization</li><li>LSA over document-term matrix</li><li>Cosine distance calculation of adjacent sentences (rows)</li></ol><h3>Process Flow for Use Case 2</h3><ul><li>Split paragraphs into sentences</li><li>Text cleaning</li><li>BERT Sentence Encoding</li><li>BERT PCA 100</li><li>BERT PCA 200</li><li>BERT PCA 300</li><li>Calculate distance between consecutive sentences in the paragraph</li><li>Distances: L1, L2 and Cosine and Cosine similarity</li><li>Statistics: Mean, SD</li></ul><h1>Experimental Setup</h1><ol><li>#IncrementalPCA</li><li>GPU to speed up</li><li>Data chunking</li><li>Calculate BERT for a chunk and store in disk</li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Short-video Background Music Recommender]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/short-video-background-music-recommender</link>
            <guid>/2021/10/01/short-video-background-music-recommender</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Matching micro-videos with suitable background music can help uploaders better convey their contents and emotions, and increase the click-through rate of their uploaded videos. However, manually selecting the background music becomes a painstaking task due to the voluminous and ever-growing pool of candidate music. Therefore, automatically recommending background music to videos becomes an important task.]]></description>
            <content:encoded><![CDATA[<p>Matching micro-videos with suitable background music can help uploaders better convey their contents and emotions, and increase the click-through rate of their uploaded videos. However, manually selecting the background music becomes a painstaking task due to the voluminous and ever-growing pool of candidate music. Therefore, automatically recommending background music to videos becomes an important task.</p><p>In <a href="https://arxiv.org/pdf/2107.07268.pdf">this</a> paper, Zhu et. al. shared their approach to solve this task. They first collected ~3,000 background music from popular TikTok videos and also ~150,000 video clips that used some kind of background music. They named this dataset <code>TT-150K</code>.</p><p><img src="/img/content-blog-raw-blog-short-video-background-music-recommender-untitled.png" alt="An exemplar subset of videos and their matched background music in the established TT-150k dataset"/></p><p>An exemplar subset of videos and their matched background music in the established TT-150k dataset</p><p>After building the dataset, they worked on modeling and proposed the following architecture:</p><p><img src="/img/content-blog-raw-blog-short-video-background-music-recommender-untitled-1.png" alt="Proposed CMVAE (Cross-modal Variational Auto-encoder) framework"/></p><p>Proposed CMVAE (Cross-modal Variational Auto-encoder) framework</p><p>The goal is to represent videos (<code>users</code> in recsys terminology) and music (<code>items</code>) in a shared latent space. To achieve this, CMVAE use pre-trained models to extract features from unstructured data - <code>vggish</code> model for audio2vec, <code>resnet</code> for video2vec and <code>bert-multilingual</code> for text2vec.  Text and video vectors are then fused using product-of-expert approach. </p><p>It uses the reconstruction power of variational autoencoders to 1) reconstruct video from music latent vector and, 2) reconstruct music from video latent vector. In layman terms, we are training a neural network that will try to guess the video activity just by listening background music, and also try to guess the background music just by seeing the video activities. </p><p>The joint training objective is $\mathcal{L}<em>{(z_m,z_v)} = \beta \cdot\mathcal{L}</em>{cross<!-- -->_<!-- -->recon} - \mathcal{L}<em>{KL} + \gamma \cdot \mathcal{L}</em>{matching}$, where $\beta$ and $\gamma$ control the weight of the cross reconstruction loss and the matching loss, respectively.</p><p>After training the model, they compared the model&#x27;s performance with existing baselines and the results are as follows:</p><p><img src="/img/content-blog-raw-blog-short-video-background-music-recommender-untitled-2.png" alt="/img/content-blog-raw-blog-short-video-background-music-recommender-untitled-2.png"/></p><p><strong>Conclusion</strong>: I don&#x27;t make short videos myself but can easily imagine the difficulty in finding the right background music. If I have to do this task manually, I will try out 5-6 videos and select one that I like. But here, I will be assuming that my audience would also like this music. Moreover, feedback is not actionable because it will create kind of an implicit sub-conscious effect (because when I see a video, I mostly judge it at overall level and rarely notice that background music is the problem). So, this kind of recommender system will definitely help me in selecting a better background music. Excited to see this feature soon in TikTok, Youtube Shorts and other similar services.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The progression of analytics in enterprises]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/the-progression-of-analytics-in-enterprises</link>
            <guid>/2021/10/01/the-progression-of-analytics-in-enterprises</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[An organization‚Äôs analytics strategy is how its¬†people, processes, tools, and data¬†work together to collect, store, and analyze data. Processes¬†refers to¬†how¬†analytics are produced, consumed, and maintained. A more modern approach to analytics is intended to support greater business agility at scale. This requires faster data preparation from a wider variety of sources, rapid prototyping and analytics model building, and cross-team collaboration processes. Tools, or technologies, are the raw programs and applications used to prepare for and perform analyses, such as the provisioning, flow, and automation of tasks and resources. As an analytics strategy matures, the technologies used to implement it tend to move from monolithic structures to composable microservices. The last element is¬†data. A modern analytics architecture supports a growing volume and variety of data sources, which may include data from data warehouses and data lakes‚Äîstreaming data, relational databases, graph databases, unstructured or semi-structured data, text data, and images.]]></description>
            <content:encoded><![CDATA[<p>An organization‚Äôs analytics strategy is how its¬†<em>people, processes, tools, and data</em>¬†work together to collect, store, and analyze data. <em>Processes</em>¬†refers to¬†<em>how</em>¬†analytics are produced, consumed, and maintained. A more modern approach to analytics is intended to support greater business agility at scale. This requires faster data preparation from a wider variety of sources, rapid prototyping and analytics model building, and cross-team collaboration processes. <em>Tools</em>, or technologies, are the raw programs and applications used to prepare for and perform analyses, such as the provisioning, flow, and automation of tasks and resources. As an analytics strategy matures, the technologies used to implement it tend to move from monolithic structures to composable microservices. The last element is¬†<em>data.</em> A modern analytics architecture supports a growing volume and variety of data sources, which may include data from data warehouses and data lakes‚Äîstreaming data, relational databases, graph databases, unstructured or semi-structured data, text data, and images.</p><h3>Analytics Past, Present, and Future</h3><table><thead><tr><th></th><th>Past</th><th>Present</th><th>Future</th></tr></thead><tbody><tr><td></td><td>This refers to an era of analytics starting in the 1990s and running through the mid-2000s. During this phase, organizations were able to consolidate mostly transactional data into a unified system, often a¬†data warehouse, which limited end users‚Äô ability to interact directly with the data due to technical and governance requirements.</td><td>Starting in the late 2000s, organizations were forced to rethink how they used analytics, in no small part due to the explosion of data during this time. This was the era of ‚ÄúBig Data‚Äù and its infamous¬†‚Äú V‚Äôs‚Äù:¬†volume, velocity, and variety.4¬†As organizations shifted their approach during this period, they unlocked¬†diagnostic analytics, or the capability to answer ‚ÄúWhy did it happen?‚Äù</td><td>The Future of Analytics Is Converged. Converged analytics unifies advances in AI, streaming data, and related technologies into a seamless analytics experience for all users. This arrangement unlocks¬†prescriptive¬†analytics across an organization, allowing anyone to make data-driven decisions that answer important questions.</td></tr><tr><td>People</td><td>IT professionals were needed to kick off any data-based work by extracting data from a centralized, difficult-to-use source. This process could take multiple days, and the number of query requests could easy exceed the IT team‚Äôs ability to fulfill those requests‚Äîand the opportune time for new insights.If some change was needed to data collection or storage methods, it could easily take IT months to perform. The data analysis and modeling work could take nearly as long. Rank-and-file domain experts did have¬†some¬†access to data, through so-called¬†self-service business intelligence (BI)¬†features. However, due to the same speed and accessibility issues that technical professionals faced, it was often difficult for domain experts like line of business leaders to truly lead with data for decision making.</td><td>It‚Äôs no coincidence that around the same time as Big Data emerged, so did the role of the¬†data scientist. Compared with earlier roles like researcher or statistician, the data scientist blends quantitative and domain expertise with a greater degree of computational thinking. These skills became necessary both to handle the greater variety and volume of data sources and to update and deploy data and analytics models without the assistance of IT professionals.Whereas IT in the past sought to meticulously catalog and structure data to enter into a data warehouse, they no longer needed to always clean the data before collecting it; these analytics teams could focus on ease of use and speed to governed access.With these new workflows and organization structures in place, domain leaders are better able to lead with data: both via self-service BI tools and from frequent collaboration with data analysts, data scientists, and other data specialists.</td><td>Statisticians and IT served information to business users at the inception of a wider analytics adoption; further into maturity, data analysts and scientists built systems where business users could self-serve insights. In a converged architecture, not only is the business user at the center, but their decision making is augmented by automation. Given this arrangement, there is more collaboration, more automation, and greater scale for data-driven insights as a result of the convergence of teams and workstreams. Teams can work cross-functionally and in parallel across different domains iterating the system to their needs with the raw time and human resources needed to create and maintain analytics products such as dashboards and models.</td></tr><tr><td>Processes</td><td>IT professionals spent long periods of time gathering requirements for analytics projects before they could build or deploy solutions. The team meticulously catalogued sources of data used across the organization, from financial or point-of-sale systems to frequently used external datasets. As part of the warehousing process, it was decided¬†which¬†of these data sources to store and¬†how¬†to store them.Once deployed, data passed into the warehouse through an¬†extract-transform-load¬†process (ETL), where the data was copied from these various sources, cleaned and reshaped into the defined structure of the data warehouse, then inserted into production. In other words, data went through rigorous cleaning and preprocessing¬†before¬†use.To reach this data, users needed to write time-consuming ad hoc queries. Alternatively, particular data segments or summaries that were frequently requested by business users could be delivered via scheduled automation to reports, dashboards, and scorecards.</td><td>As opposed to earlier analytics strategies, IT professionals now seek to collect data as is from any possible source of value. This data can be in a variety of formats, so few predefined rules or relationships are established for ingestion. Depending on the data size, data is processed in batch over discrete time periods, or in streams and events near real time. Because data cleaning is the last step, this process is sometimes referred to as¬†extract-load-transform¬†(ELT), as opposed to the ETL of earlier architectures. For data scientists and other technical professionals, faster access to more¬†and more dynamic¬†data better enables the rapid development of training sets of data for machine learning models. The ELT process allows for the construction of¬†machine learning¬†models, where computers are able to improve performance as more data is passed to them.As more data is collected and put into production, the importance of a¬†data governance¬†process typically grows, describing who has authority over data and how that data should be used. Similar approaches are necessary to audit how models are put into production and how they work.</td><td>While perhaps using different¬†means, the¬†ends¬†of older analytics approaches were the same: insights, whether historic or in support of future decisions, using governed data and processes. In the methods for doing so, however, infrastructure tended to bloat, either from fragile data storage jobs or increasingly complex data pipelines.Given the volume, velocity, and variety of data needed for prescriptive analytics, such monolithic, centralized approaches are less than optimal. Using the tools discussed in the next section, a converged architecture offers a more nimble approach for providing the right insights at the right time to users of all technical levels.Such democratization relies on quick deployment and adjustment of data products; optimizing production, for example, requires bringing more machine learning models to production faster and at scale. The practice of¬†ModelOps¬†is used to institute and govern such rapid production. These processes have become a necessity in rapidly changing business conditions; for example, as the COVID-19 pandemic made structural changes to the economy, many models lost their predictive edge in the face of fundamentally different data.</td></tr><tr><td>Tools</td><td>Data warehouses implemented some new technologies relative to the traditional relational database model. Importantly, the data warehouse separated data into¬†fact¬†tables, where measurements were stored, and¬†dimension¬†tables, which contained descriptive attributes. Business users interacted with the data via reporting software to view static data summaries. These tended to rely on overnight batch jobs to update.In a more sophisticated architecture, analysts could take advantage of¬†online analytical processing¬†(OLAP) cubes. Usually relying on a star schema, OLAP let users query the data across dimensions during interactive sessions. For example, they could ‚Äúslice and dice‚Äù or ‚Äúroll up and drill down‚Äù on the data.By this point, end users had some autonomy in how they looked at and acted upon the data. Automated processes to inform business activities through data were also put into place, such as alerts when inventory or sales dropped below some threshold. Basic what-if analyses also helped business users evaluate decisions and plan for the future.That said, given the limited sources of data from the data warehouse, there were limited ways to customize and work with the data. While reporting and basic analytics were automated, end users operated largely without the assistance of models developed by¬†statisticians. Although business intelligence and operations research seek to create value from data, too often these complementary tools were siloed.</td><td>In 2011, James Dixon, then chief technology officer of Pentaho, coined the term¬†data lake¬†as the architecture needed to support the next level of analytics maturity.¬†Dixon argued that because of the inherently rigid structures of data warehouses, getting value from the increasing volume and variety of data associated with Big Data was difficult. A data lake, ‚Äúa repository of data stored in its natural/raw format,‚Äù was a better approach. In particular, this arrangement wasn‚Äôt suited to operate or capitalize on the expanding volume and variety of Big Data.The data lake is often powered by cloud computing for the benefits of reliability, redundancy, and scalability. Dominant cloud service providers include Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). Open source technologies like Hadoop and Spark are used to process and store massive datasets using parallel computing and distributed storage. Because this data is often unstructured, it may be stored in graph, document, or other non-relational databases.With the increasing volume and velocity of data, and the use of data lakes along with data warehouses to enable data-driven decisions, businesses needed better ways to scale and share business intelligence. One such path was through interactive, immersive exploration and visualization of the data, as pioneered with Spotfire. Other paths were through visual reports and dashboards, as used by not just Spotfire, but by Jaspersoft, Power BI, WebFOCUS, and many others. As BI tools matured, self-service capabilities and automation for end users also matured.</td><td>If maintaining legacy analytics is like raising a thoroughbred, then developing converged analytics is like cultivating a school of goldfish. That is, the backend provisioning is no longer served by monolithic systems but rather by composable groups of¬†microservices. This arrangement supports elastic and scalable analytics; composability makes it easier to adapt to changes driven in part by a growing volume and variety of data sources. In previous analytics approaches, the distinction between backward-facing BI and prediction-focused data science was clear. Under convergence, analytics at the edge is possible‚Äîautomating analytic computations so they can be performed on non-centralized data generated by sensors, switches, and similar. With converged analytics, individuals no longer need to wait for data science teams to provide ad hoc deeper insights. They have all the data-driven insights at their fingertips, assisted by AI to quickly explore and make decisions. This isn‚Äôt just the case for back-office analysts: frontline workers can, for example, adjust how they interact with a customer given data retrieved about that customer at the time of that interaction.</td></tr><tr><td>Data</td><td>During this period, data tended to be¬†transactional, or related to sales and purchases. Take a point-of-sales (POS) system, for example. Each time a sale is made, information about what was sold,¬†possibly¬†to whom, is recorded in the POS system. Those records can be compiled into tables and ultimately processed into a data¬†warehouse.Under this process, data is gathered from prespecified sources at prespecified times, such as a nightly POS extract. Not all data made its way to the data warehouse, especially in the earlier days of analytics‚Äîeither because it was judged unimportant, or because it was not prioritized.</td><td>Contemporary analytics expands the variety of data available and used: both¬†structured¬†tables and¬†unstructured¬†sources like natural language and images are available. On account of stream processing, refreshes of this data are available in minutes or even less. In particular, the data lake can accommodate real-time events such as IoT sensor readings, GPS signals, and online transactions as they¬†happen.</td><td>A primary feature of converged analytics is the blending of historical and real-time data. According to a study by Seagate and International Data Corporation (IDC), 30% of all data will be real time by 2025. In particular, IoT sensor readings, GPS signals, and online transactions as they happen are available for immediate analysis and modeling.</td></tr><tr><td>Agility</td><td>The relatively rigid nature of the data warehouse made changes to the collection and dissemination of data difficult. Subsequently, business agility was limited. Business users could get historic data about the business through static reports (descriptive analytics). Through OLAP cubes, they could possibly even dig into the data to parse out cause and effect (diagnostic analytics). But without more immediate access to broader data, it was difficult to advance to¬†predictive analytics, or the ability to ask: ‚ÄúWhat is¬†going¬†to happen?‚Äù</td><td>This next phase in the evolution of analytics gets data-driven insights into the hands of end users quickly, with technology allowing them to interact with it on a deeper level. Data scientists are able to build machine learning systems that improve with more data. Using drag-and-drop tools, business users can process and analyze data without technical assistance. With cloud, automation, and streaming technologies, organizations have been better able to adapt to and plan for changing circumstances. That said, machine learning works only so long in production before the algorithm struggles to account for changes to the business and needs intervention. While data scientists undertake these predictive challenges, BI professionals and domain experts tend to operate solely in analyzing current or past data. The next generation of analytics architecture will further reflect organizational needs for greater collaboration among data scientists, BI and analytics teams, and business users and consumers of analytics insights.</td><td>Earlier analytics tended to isolate skills and processes: technical versus highly technical roles, data collection versus deployment versus modeling, and so forth. Converged analytics promotes close collaboration between teams to rapidly model, deploy, and act on data. As data operations become decentralized, teams and individuals can rapidly mine and act on the analytics.In particular, the marriage of real-time data with machine learning and AI-infused BI allows any user to magnify their own domain knowledge with data-driven insights. These features square precisely with the definition of business agility as ‚Äúinnovation via collaboration to be able to anticipate challenges and opportunities before they occur.‚Äù With the support of converged analytics, any professional can detect and act on both challenges and opportunities at the moment of impact, rather than months later.</td></tr></tbody></table><hr/><p>¬©Ô∏è2021, RecoHut.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tools for building recommender systems]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/tools-for-building-recommender-systems</link>
            <guid>/2021/10/01/tools-for-building-recommender-systems</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled.png"/></p><h2>Recombee - Recommendation as a service API</h2><p>Recombee is a Recommender as a Service with easy integration and Admin UI. It can be used in many domains, for example in media (VoD, news ‚Ä¶), e-commerce, job boards, aggregators or classifieds. Basically, it can be used in any domain with a catalog of¬†<strong>items</strong>¬†that can be interacted by¬†<strong>users</strong>. The users can interact with the items in many ways: for example¬†view them,¬†rate them,¬†bookmark them,¬†purchase them, etc. Both items and users can have¬†various properties¬†(metadata) that are also used by the recommendation models.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-1.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-1.png"/></p><p><a href="https://docs.recombee.com/tutorial.html">Here</a> is the official tutorial series to get started. </p><h2>Amazon Personalize - Self-service Platform to build and serve recommenders</h2><p>Amazon Personalize is a fully managed machine learning service that goes beyond rigid static rule based recommendation systems and trains, tunes, and deploys custom ML models to deliver highly customized recommendations to customers across industries such as retail and media and entertainment.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-2.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-2.png"/></p><p>It covers 6 use-cases:</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-3.png" alt="Popular Use-cases"/></p><p>Popular Use-cases</p><p>Following are the hands-on tutorials:</p><ol><li><a href="https://github.com/data-science-on-aws/workshop/tree/937f6e4fed53fcc6c22bfac42c2c18a687317995/oreilly_book/02_usecases/personalize_recommendations">Data Science on AWS Workshop - Personalize Recommendations<strong>p</strong></a></li><li><a href="https://aws.amazon.com/blogs/machine-learning/creating-a-recommendation-engine-using-amazon-personalize/">https://aws.amazon.com/blogs/machine-learning/creating-a-recommendation-engine-using-amazon-personalize/</a></li><li><a href="https://aws.amazon.com/blogs/machine-learning/omnichannel-personalization-with-amazon-personalize/">https://aws.amazon.com/blogs/machine-learning/omnichannel-personalization-with-amazon-personalize/</a></li><li><a href="https://aws.amazon.com/blogs/machine-learning/using-a-b-testing-to-measure-the-efficacy-of-recommendations-generated-by-amazon-personalize/">https://aws.amazon.com/blogs/machine-learning/using-a-b-testing-to-measure-the-efficacy-of-recommendations-generated-by-amazon-personalize/</a></li></ol><p>Also checkout these resources:</p><ol><li><a href="https://www.youtube.com/playlist?list=PLN7ADELDRRhiQB9QkFiZolioeJZb3wqPE">https://www.youtube.com/playlist?list=PLN7ADELDRRhiQB9QkFiZolioeJZb3wqPE</a></li></ol><h2>Azure Personalizer - An API based service with Reinforcement learning capability</h2><p>Azure Personalizer is a cloud-based API service that helps developers create rich, personalized experiences for each user of your app. It learns from customer&#x27;s real-time behavior, and uses reinforcement learning to select the best item (action) based on collective behavior and reward scores across all users. Actions are the content items, such as news articles, specific movies, or products. It takes a list of items (e.g. list of drop-down choices) and their context (e.g. Report Name, User Name, Time Zone) as input and returns the ranked list of items for the given context. While doing that, it also allows feedback submission regarding the relevance and efficiency of the ranking results returned by the service. The feedback (reward score) can be automatically calculated and submitted to the service based on the given personalization use case.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-4.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-4.png"/></p><p>You can use the Personalizer service to determine what product to suggest to shoppers or to figure out the optimal position for an advertisement. After the content is shown to the user, your application monitors the user&#x27;s reaction and reports a reward score back to the Personalizer service. This ensures continuous improvement of the machine learning model, and Personalizer&#x27;s ability to select the best content item based on the contextual information it receives. </p><p>Following are some of the interesting use cases of Azure Personalizer:</p><ol><li>Blog Recommender [<a href="https://youtu.be/fsn7hTOKXsY?list=PLN7ADELDRRhhHRu1tS3gmdeUfeQkG82k_&amp;t=1145">Video tutorial</a>, <a href="https://github.com/georgiakalyva/azure-personalizer-service">GitHub</a>]</li><li>Food Personalizer [<a href="https://youtu.be/A-8OfoWySHQ?list=PLN7ADELDRRhhHRu1tS3gmdeUfeQkG82k_&amp;t=1758">Video tutorial</a>, <a href="https://www.slideshare.net/SetuChokshi/introduction-to-reinforcement-learning-with-azure-personalizer-233272693">Slideshare</a>, <a href="https://pipinstall.me/introduction_to_azure_personalizer/">Code Blog</a>]</li><li>Coffee Personalizer [<a href="https://github.com/Azure-Samples/cognitive-services-personalizer-samples/tree/master/samples/azurenotebook">GitHub</a>, <a href="https://youtu.be/vkbIhX7xhcE?list=PLN7ADELDRRhhHRu1tS3gmdeUfeQkG82k_">Video tutorial</a>]</li><li>News Recommendation</li><li>Movie Recommendation</li><li>Product Recommendation</li><li><strong>Intent clarification &amp; disambiguation</strong>: help your users have a better experience when their intent is not clear by providing an option that is personalized.</li><li><strong>Default suggestions</strong>¬†for menus &amp; options: have the bot suggest the most likely item in a personalized way as a first step, instead of presenting an impersonal menu or list of alternatives.</li><li><strong>Bot traits &amp; tone</strong>: for bots that can vary tone, verbosity, and writing style, consider varying these traits.</li><li><strong>Notification &amp; alert content</strong>: decide what text to use for alerts in order to engage users more.</li><li><strong>Notification &amp; alert timing</strong>: have personalized learning of when to send notifications to users to engage them more.</li><li>Dropdown Options - Different users of an application with manager privileges would see a list of reports that they can run. Before Personalizer was implemented, the list of dozens of reports was displayed in alphabetical order, requiring most of the managers to scroll through the lengthy list to find the report they needed. This created a poor user experience for daily users of the reporting system, making for a good use case for Personalizer. The tooling learned from the user behavior and began to rank frequently run reports on the top of the dropdown list. Frequently run reports would be different for different users, and would change over time for each manager as they get assigned to different projects. This is exactly the situation where Personalizer‚Äôs reward score-based learning models come into play.</li><li>Projects in Timesheet - Every employee in the company logs a daily timesheet listing all of the projects the user is assigned to. It also lists other projects, such as overhead. Depending upon the employee project allocations, his or her timesheet table could have few to a couple of dozen active projects listed. Even though the employee is assigned to several projects, particularly at lead and manager levels, they don‚Äôt log time in more than 2 to 3 projects for a few weeks to months.<ol><li>Reward Score Calculation</li></ol></li></ol><h2>Google Recommendation - Recommender Service from Google</h2><p><img src="https://cloudx-bricks-prod-bucket.storage.googleapis.com/6a0d4afb1778e55d54cb7d66382a4b25f8748a50a93f3c3403d2a835aa166f3d.svg" alt="https://cloudx-bricks-prod-bucket.storage.googleapis.com/6a0d4afb1778e55d54cb7d66382a4b25f8748a50a93f3c3403d2a835aa166f3d.svg"/></p><h2><a href="http://abacus.ai">Abacus.ai</a> - Self-service Platform at cheaper price</h2><p>It uses multi-objective, real-time recommendations models and provides 4 use-cases for fasttrack train-&amp;-deploy process - Personalized recommendations, personalized search, related items and real-time feed recommendations.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-5.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-5.png"/></p><p>Here is the hands-on video tutorial:</p><p><a href="https://youtu.be/7hTKL73f2yA">https://youtu.be/7hTKL73f2yA</a></p><h2>Nvidia Merlin - Toolkit with GPU capabilities</h2><p>Merlin empowers data scientists, machine learning engineers, and researchers to build high-performing recommenders at scale. Merlin includes tools that democratize building deep learning recommenders by addressing common ETL, training, and inference challenges. Each stage of the Merlin pipeline is optimized to support hundreds of terabytes of data, all accessible through easy-to-use APIs. With Merlin, better predictions than traditional methods and increased click-through rates are within reach.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-6.png" alt="End-to-end recommender system architecture. FE: feature engineering; PP: preprocessing; ETL: extract-transform-load."/></p><p>End-to-end recommender system architecture. FE: feature engineering; PP: preprocessing; ETL: extract-transform-load.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-7.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-7.png"/></p><h2>TFRS - Open-source Recommender library built on top of Tensorflow</h2><p>Built with TensorFlow 2.x, TFRS makes it possible to:</p><ul><li>Build and evaluate flexible¬†<strong><a href="https://research.google/pubs/pub48840/">candidate nomination models</a></strong>;</li><li>Freely incorporate item, user, and context¬†<strong><a href="https://tensorflow.org/recommenders/examples/featurization">information</a></strong>¬†into recommendation models;</li><li>Train¬†<strong><a href="https://tensorflow.org/recommenders/examples/multitask">multi-task models</a></strong>¬†that jointly optimize multiple recommendation objectives;</li><li>Efficiently serve the resulting models using¬†<strong><a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a></strong>.</li></ul><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-8.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-8.png"/></p><p>Following is a series of official tutorial notebooks:-</p><p><a href="https://www.tensorflow.org/recommenders/examples/quickstart">TensorFlow Recommenders: Quickstart</a></p><h2>Elliot - An end-to-end framework good for recommender system experiments</h2><p><a href="https://elliot.readthedocs.io/en/latest/">Elliot</a> is a comprehensive recommendation framework that aims to run and reproduce an entire experimental pipeline by processing a simple configuration file. The framework loads, filters, and splits the data considering a vast set of strategies (13 splitting methods and 8 filtering approaches, from temporal training-test splitting to nested K-folds Cross-Validation). Elliot optimizes hyperparameters (51 strategies) for several recommendation algorithms (50), selects the best models, compares them with the baselines providing intra-model statistics, computes metrics (36) spanning from accuracy to beyond-accuracy, bias, and fairness, and conducts statistical analysis (Wilcoxon and Paired t-test). The aim is to provide the researchers with a tool to ease (and make them reproducible) all the experimental evaluation phases, from data reading to results collection.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-9.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-9.png"/></p><h2>RecBole - Another framework good for recommender system model experiments</h2><p>RecBole is developed based on Python and PyTorch for reproducing and developing recommendation algorithms in a unified, comprehensive and efficient framework for research purpose. It can be installed from pip, Conda and source, and easy to use. It includes 65 recommendation algorithms, covering four major categories: General Recommendation, Sequential Recommendation, Context-aware Recommendation, and Knowledge-based Recommendation, which can support the basic research in recommender systems.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-10.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-10.png"/></p><p>Features:</p><ul><li><strong>General and extensible data structure</strong>We deign general and extensible data structures to unify the formatting and usage of various recommendation datasets.</li><li><strong>Comprehensive benchmark models and datasets</strong>We implement 65 commonly used recommendation algorithms, and provide the formatted copies of 28 recommendation datasets.</li><li><strong>Efficient GPU-accelerated execution</strong>We design many tailored strategies in the GPU environment to enhance the efficiency of our library.</li><li><strong>Extensive and standard evaluation protocols</strong>We support a series of commonly used evaluation protocols or settings for testing and comparing recommendation algorithms.</li></ul><h2>Microsoft Recommenders - A powerful set of tools for building high-quality recommender system at low-cost <em>(highly recommended)</em></h2><p>The Microsoft Recommenders repository is an open source collection of python utilities and Jupyter notebooks to help accelerate the process of designing, evaluating, and deploying recommender systems. The repository was initially formed by data scientists at Microsoft to consolidate common tools and best practices developed from working on recommender systems in various industry settings. The goal of the tools and notebooks is to show examples of how to effectively build, compare, and then deploy the best recommender solution for a given scenario. Contributions from the community have brought in new algorithm implementations and code examples covering multiple aspects of working with recommendation algorithms.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-11.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-11.png"/></p><h2>Surprise - An open-source library with easy api and powerful models</h2><p><a href="http://surpriselib.com/">Surprise</a>¬†is a Python¬†<a href="https://www.scipy.org/scikits.html">scikit</a>¬†for building and analyzing recommender systems that deal with explicit rating data.</p><p><a href="http://surpriselib.com/">Surprise</a>¬†<strong>was designed with the following purposes in mind</strong>:</p><ul><li>Give users perfect control over their experiments. To this end, a strong emphasis is laid on¬†<a href="http://surprise.readthedocs.io/en/stable/index.html">documentation</a>, which we have tried to make as clear and precise as possible by pointing out every detail of the algorithms.</li><li>Alleviate the pain of¬†<a href="http://surprise.readthedocs.io/en/stable/getting_started.html#load-a-custom-dataset">Dataset handling</a>. Users can use both¬†<em>built-in</em>¬†datasets (<a href="http://grouplens.org/datasets/movielens/">Movielens</a>,¬†<a href="http://eigentaste.berkeley.edu/dataset/">Jester</a>), and their own¬†<em>custom</em>¬†datasets.</li><li>Provide various ready-to-use¬†<a href="http://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html">prediction algorithms</a>¬†such as¬†<a href="http://surprise.readthedocs.io/en/stable/basic_algorithms.html">baseline algorithms</a>,¬†<a href="http://surprise.readthedocs.io/en/stable/knn_inspired.html">neighborhood methods</a>, matrix factorization-based (¬†<a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD">SVD</a>,¬†<a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#unbiased-note">PMF</a>,¬†<a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp">SVD++</a>,¬†<a href="http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF">NMF</a>), and¬†<a href="http://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html">many others</a>. Also, various¬†<a href="http://surprise.readthedocs.io/en/stable/similarities.html">similarity measures</a>¬†(cosine, MSD, pearson‚Ä¶) are built-in.</li><li>Make it easy to implement¬†<a href="http://surprise.readthedocs.io/en/stable/building_custom_algo.html">new algorithm ideas</a>.</li><li>Provide tools to¬†<a href="http://surprise.readthedocs.io/en/stable/model_selection.html">evaluate</a>,¬†<a href="http://nbviewer.jupyter.org/github/NicolasHug/Surprise/tree/master/examples/notebooks/KNNBasic_analysis.ipynb/">analyse</a>¬†and¬†<a href="http://nbviewer.jupyter.org/github/NicolasHug/Surprise/blob/master/examples/notebooks/Compare.ipynb">compare</a>¬†the algorithms‚Äô performance. Cross-validation procedures can be run very easily using powerful CV iterators (inspired by¬†<a href="http://scikit-learn.org/">scikit-learn</a>¬†excellent tools), as well as¬†<a href="http://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv">exhaustive search over a set of parameters</a>.</li></ul><h2>Spotlight - Another open-source library</h2><p>Spotlight uses PyTorch to build both deep and shallow recommender models. By providing both a slew of building blocks for loss functions (various pointwise and pairwise ranking losses), representations (shallow factorization representations, deep sequence models), and utilities for fetching (or generating) recommendation datasets, it aims to be a tool for rapid exploration and prototyping of new recommender models.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-12.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-12.png"/></p><p><a href="https://github.com/maciejkula/spotlight/tree/master/examples">Here</a> is a series of hands-on tutorials to get started.</p><h2>Vowpal Wabbit - library with reinforcement learning features</h2><p>Vowpal Wabbit is an open source machine learning library, extensively used by industry, and is the first public terascale learning system. It provides fast, scalable machine learning and has unique capabilities such as learning to search, active learning, contextual memory, and extreme multiclass learning. It has a focus on reinforcement learning and provides production ready implementations of Contextual Bandit algorithms. It was developed originally at Yahoo! Research, and currently at Microsoft Research. Vowpal Wabbit sees significant innovation as a research to production vehicle for Microsoft Research.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-13.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-13.png"/></p><p>For most applications, collaborative filtering yields satisfactory results for item recommendations; there are however several issues that arise that might make it difficult to scale up a recommender system.</p><ul><li>The number of features can grow quite large, and given the usual sparsity of consumption datasets, collaborative filtering needs every single feature and datapoint available.</li><li>For new data points, the whole model has to be re-trained</li></ul><p>Vowpal Wabbit‚Äôs matrix factorization capabilities can be used to build a recommender that is similar in spirit to collaborative filtering but that avoids the pitfalls that we mentioned before.</p><p>Following are the three introductory hands-on tutorials on building recommender systems with vowpal wabbit:</p><ol><li><a href="https://github.com/microsoft/recommenders/blob/main/examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb">Vowpal Wabbit Deep Dive - A Content-based Recommender System using Microsoft Recommender Library</a></li><li><a href="https://vowpalwabbit.org/tutorials/cb_simulation.html">Simulating Content Personalization with Contextual Bandits</a></li><li><a href="https://samuel-guedj.medium.com/vowpal-wabbit-the-magic-58b7f1d8e39c">Vowpal Wabbit, The Magic Recommender System!</a></li></ol><h2>DLRM - An open-source scalable model from Facebook&#x27;s AI team, build on top of PyTorch</h2><p>DLRM advances on other models by combining principles from both collaborative filtering and predictive analytics-based approaches, which enables it to work efficiently with production-scale data and provide state-of-art results.</p><p>In the DLRM model, categorical features are processed using embeddings, while continuous features are processed with a bottom multilayer perceptron (MLP). Then, second-order interactions of different features are computed explicitly. Finally, the results are processed with a top MLP and fed into a sigmoid function in order to give a probability of a click.</p><p><img src="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-14.png" alt="/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-14.png"/></p><p>Following are the hands-on tutorials:</p><ol><li><a href="https://nbviewer.jupyter.org/github/gotorehanahmad/Recommendation-Systems/blob/master/dlrm/dlrm_main.ipynb">https://nbviewer.jupyter.org/github/gotorehanahmad/Recommendation-Systems/blob/master/dlrm/dlrm_main.ipynb</a></li><li><a href="https://nbviewer.jupyter.org/github/mabeckers/dlrm/blob/new_dataset/Train_DLRM_Digix.ipynb">Training Facebook&#x27;s DLRM on the digix dataset</a></li></ol><h2>References</h2><ol><li><a href="https://elliot.readthedocs.io/en/latest/">https://elliot.readthedocs.io/en/latest/</a></li><li><a href="https://vowpalwabbit.org/index.html">https://vowpalwabbit.org/index.html</a></li><li><a href="https://abacus.ai/user_eng">https://abacus.ai/user_eng</a></li><li><a href="https://azure.microsoft.com/en-in/services/cognitive-services/personalizer/">https://azure.microsoft.com/en-in/services/cognitive-services/personalizer/</a></li><li><a href="https://aws.amazon.com/personalize/">https://aws.amazon.com/personalize/</a></li><li><a href="https://github.com/facebookresearch/dlrm">https://github.com/facebookresearch/dlrm</a></li><li><a href="https://www.tensorflow.org/recommenders">https://www.tensorflow.org/recommenders</a></li><li><a href="https://magento.com/products/product-recommendations">https://magento.com/products/product-recommendations</a></li><li><a href="https://cloud.google.com/recommendations">https://cloud.google.com/recommendations</a></li><li><a href="https://www.recombee.com/">https://www.recombee.com/</a></li><li><a href="https://recbole.io/">https://recbole.io/</a></li><li><a href="https://github.com/microsoft/recommenders">https://github.com/microsoft/recommenders</a></li><li><a href="http://surpriselib.com/">http://surpriselib.com/</a></li><li><a href="https://github.com/maciejkula/spotlight">https://github.com/maciejkula/spotlight</a></li><li><a href="https://vowpalwabbit.org/tutorials/contextual_bandits.html">https://vowpalwabbit.org/tutorials/contextual_bandits.html</a></li><li><a href="https://github.com/VowpalWabbit/vowpal_wabbit/wiki">https://github.com/VowpalWabbit/vowpal_wabbit/wiki</a></li><li><a href="https://vowpalwabbit.org/tutorials/cb_simulation.html">https://vowpalwabbit.org/tutorials/cb_simulation.html</a></li><li><a href="https://vowpalwabbit.org/rlos/2021/projects.html">https://vowpalwabbit.org/rlos/2021/projects.html</a></li><li><a href="https://vowpalwabbit.org/rlos/2020/projects.html">https://vowpalwabbit.org/rlos/2020/projects.html</a></li><li><a href="https://getstream.io/blog/recommendations-activity-streams-vowpal-wabbit/">https://getstream.io/blog/recommendations-activity-streams-vowpal-wabbit/</a></li><li><a href="https://samuel-guedj.medium.com/vowpal-wabbit-the-magic-58b7f1d8e39c">https://samuel-guedj.medium.com/vowpal-wabbit-the-magic-58b7f1d8e39c</a></li><li><a href="https://vowpalwabbit.org/neurips2019/">https://vowpalwabbit.org/neurips2019/</a></li><li><a href="https://github.com/VowpalWabbit/neurips2019">https://github.com/VowpalWabbit/neurips2019</a></li><li><a href="https://getstream.io/blog/introduction-contextual-bandits/">https://getstream.io/blog/introduction-contextual-bandits/</a></li><li><a href="https://www.youtube.com/watch?v=CeOcNK1xSSA&amp;t=72s">https://www.youtube.com/watch?v=CeOcNK1xSSA&amp;t=72s</a></li><li><a href="https://vowpalwabbit.org/blog/rlos-fest-2021.html">https://vowpalwabbit.org/blog/rlos-fest-2021.html</a></li><li><a href="https://github.com/VowpalWabbit/workshop">https://github.com/VowpalWabbit/workshop</a></li><li><a href="https://github.com/VowpalWabbit/workshop/tree/master/aiNextCon2019">https://github.com/VowpalWabbit/workshop/tree/master/aiNextCon2019</a></li><li><a href="https://www.ais.com/azure-cognitive-services-personalizer-part-one/">Blog post by Nasir Mirza. Azure Cognitive Services Personalizer: Part One. Oct, 2019.</a></li><li><a href="https://www.ais.com/azure-cognitive-services-personalizer-part-two/">Blog post by Nasir Mirza. Azure Cognitive Services Personalizer: Part Two. Oct, 2019.</a></li><li><a href="https://www.ais.com/azure-cognitive-services-personalizer-part-three/">Blog post by Nasir Mirza. Azure Cognitive Services Personalizer: Part Three. Dec, 2019.</a></li><li><a href="https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/what-is-personalizer">Microsoft Azure Personalizer Official Documentation. Oct, 2020.</a></li><li><a href="https://personalizationdemo.azurewebsites.net/">Personalizer demo.</a></li><li><a href="https://azure.microsoft.com/en-in/services/cognitive-services/personalizer/#faqs">Official Page.</a></li><li><a href="https://www.linkedin.com/pulse/get-hands-azure-personalizer-api-jake-wang/">Blog Post by Jake Wong. Get hands on with the Azure Personalizer API. Aug, 2019.</a></li><li><a href="https://enefitit.medium.com/we-tested-azure-personalizer-heres-what-you-can-expect-8c5ec074a28e">Medium Post.</a></li><li><a href="https://www.valoremreply.com/post/azure-personalizer/">Blog Post.</a></li><li><a href="https://github.com/Azure-Samples/cognitive-services-personalizer-samples">Git Repo.</a></li><li><a href="https://youtu.be/7hTKL73f2yA">https://youtu.be/7hTKL73f2yA</a></li><li><a href="https://abacus.ai/blog/2020/03/31/deep-learning-based-recommendation-systems/#:~:text=Deep%2DLearning%20Based%20Recommendation%20Systems%20%E2%80%94%20Learning%20AI,-By%20Abacus.AI&amp;text=Deep%20Learning%20(DL)%20has%20had,of%20Recommender%20Systems%20(RS).">Deep-Learning Based Recommendation Systems‚Ää‚Äî‚ÄäLearning AI</a></li><li><a href="https://abacus.ai/blog/2020/12/11/evaluating-deep-learning-models-recommender-systems/">Evaluating Deep Learning Models with Abacus.AI ‚Äì Recommendation Systems</a></li><li><a href="https://aws.amazon.com/blogs/machine-learning/pioneering-personalized-user-experiences-at-stockx-with-amazon-personalize/">https://aws.amazon.com/blogs/machine-learning/pioneering-personalized-user-experiences-at-stockx-with-amazon-personalize/</a></li><li><a href="https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-personalize/">https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-personalize/</a></li><li><a href="https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Build_a_content-recommendation_engine_with_Amazon_Personalize_AIM304-R1.pdf">https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Build_a_content-recommendation_engine_with_Amazon_Personalize_AIM304-R1.pdf</a></li><li><a href="https://aws.amazon.com/blogs/aws/amazon-personalize-real-time-personalization-and-recommendation-for-everyone/">https://aws.amazon.com/blogs/aws/amazon-personalize-real-time-personalization-and-recommendation-for-everyone/</a></li><li><a href="https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Accelerate_experimentation_with_personalization_models_AIM424-R1.pdf">https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Accelerate_experimentation_with_personalization_models_AIM424-R1.pdf</a></li><li><a href="https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Personalized_user_engagement_with_machine_learning_AIM346-R1.pdf">https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Personalized_user_engagement_with_machine_learning_AIM346-R1.pdf</a></li><li><a href="https://github.com/aws-samples/amazon-personalize-samples">https://github.com/aws-samples/amazon-personalize-samples</a></li><li><a href="https://github.com/aws-samples/amazon-personalize-automated-retraining">https://github.com/aws-samples/amazon-personalize-automated-retraining</a></li><li><a href="https://github.com/aws-samples/amazon-personalize-ingestion-pipeline">https://github.com/aws-samples/amazon-personalize-ingestion-pipeline</a></li><li><a href="https://github.com/aws-samples/amazon-personalize-monitor">https://github.com/aws-samples/amazon-personalize-monitor</a></li><li><a href="https://github.com/aws-samples/amazon-personalize-data-conversion-pipeline">https://github.com/aws-samples/amazon-personalize-data-conversion-pipeline</a></li><li><a href="https://github.com/james-jory/segment-personalize-workshop">https://github.com/james-jory/segment-personalize-workshop</a></li><li><a href="https://github.com/aws-samples/amazon-personalize-samples/tree/master/next_steps/workshops/POC_in_a_box">https://github.com/aws-samples/amazon-personalize-samples/tree/master/next_steps/workshops/POC_in_a_box</a></li><li><a href="https://github.com/Imagination-Media/aws-personalize-magento2">https://github.com/Imagination-Media/aws-personalize-magento2</a></li><li><a href="https://github.com/awslabs/amazon-personalize-optimizer-using-amazon-pinpoint-events">https://github.com/awslabs/amazon-personalize-optimizer-using-amazon-pinpoint-events</a></li><li><a href="https://github.com/aws-samples/amazon-personalize-with-aws-glue-sample-dataset">https://github.com/aws-samples/amazon-personalize-with-aws-glue-sample-dataset</a></li><li><a href="https://github.com/awsdocs/amazon-personalize-developer-guide">https://github.com/awsdocs/amazon-personalize-developer-guide</a></li><li><a href="https://github.com/chrisking/NetflixPersonalize">https://github.com/chrisking/NetflixPersonalize</a></li><li><a href="https://github.com/aws-samples/retail-demo-store">https://github.com/aws-samples/retail-demo-store</a></li><li><a href="https://github.com/aws-samples/personalize-data-science-sdk-workflow">https://github.com/aws-samples/personalize-data-science-sdk-workflow</a></li><li><a href="https://github.com/apac-ml-tfc/personalize-poc">https://github.com/apac-ml-tfc/personalize-poc</a></li><li><a href="https://github.com/dalacan/personalize-batch-recommendations">https://github.com/dalacan/personalize-batch-recommendations</a></li><li><a href="https://github.com/harunobukameda/Amazon-Personalize-Handson">https://github.com/harunobukameda/Amazon-Personalize-Handson</a></li><li><a href="https://www.sagemakerworkshop.com/personalize/">https://www.sagemakerworkshop.com/personalize/</a></li><li><a href="https://github.com/lmorri/vodpocinabox">https://github.com/lmorri/vodpocinabox</a></li><li><a href="https://github.com/awslabs/unicornflix">https://github.com/awslabs/unicornflix</a></li><li><a href="https://www.youtube.com/watch?v=r9J3UZmddC4&amp;t=966s">https://www.youtube.com/watch?v=r9J3UZmddC4&amp;t=966s</a></li><li><a href="https://www.youtube.com/watch?v=kTufCK76Yus&amp;t=1436s">https://www.youtube.com/watch?v=kTufCK76Yus&amp;t=1436s</a></li><li><a href="https://www.youtube.com/watch?v=hY_XzglTkak&amp;t=66s">https://www.youtube.com/watch?v=hY_XzglTkak&amp;t=66s</a></li><li><a href="https://business.adobe.com/lv/summit/2020/adobe-sensei-powers-magento-product-recommendations.html">https://business.adobe.com/lv/summit/2020/adobe-sensei-powers-magento-product-recommendations.html</a></li><li><a href="https://magento.com/products/product-recommendations">https://magento.com/products/product-recommendations</a></li><li><a href="https://docs.magento.com/user-guide/marketing/product-recommendations.html">https://docs.magento.com/user-guide/marketing/product-recommendations.html</a></li><li><a href="https://vod.webqem.com/detail/videos/magento-commerce/video/6195503645001/magento-commerce---product-recommendations?autoStart=true&amp;page=1">https://vod.webqem.com/detail/videos/magento-commerce/video/6195503645001/magento-commerce---product-recommendations?autoStart=true&amp;page=1</a></li><li><a href="https://blog.adobe.com/en/publish/2020/11/23/new-ai-capabilities-for-magento-commerce-improve-retail.html#gs.yw6mtq">https://blog.adobe.com/en/publish/2020/11/23/new-ai-capabilities-for-magento-commerce-improve-retail.html#gs.yw6mtq</a></li><li><a href="https://developers.google.com/recommender/docs/reference/rest">https://developers.google.com/recommender/docs/reference/rest</a></li><li><a href="https://www.youtube.com/watch?v=nY5U0uQZRyU&amp;t=6s">https://www.youtube.com/watch?v=nY5U0uQZRyU&amp;t=6s</a></li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vehicle Suggestions]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/vehicle-suggestions</link>
            <guid>/2021/10/01/vehicle-suggestions</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-vehicle-suggestions-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled.png"/></p><h1>Introduction</h1><p>The customer owns a franchise store for selling Tesla Automobiles. The objective is to predict user preferences using social media data.</p><p>Task 1 - Suggest the best vehicle for the given description</p><p>Task 2 - Suggest the best vehicle for the given social media id of the user</p><h2>Customer queries</h2><pre><code class="language-json">// car or truck or no mention of vehicle type means Cyber Truck
// SUV mention means Model X
const one = &quot;I&#x27;m looking for a fast suv that I can go camping without worrying about recharging&quot;.;
const two = &quot;cheap red car that is able to go long distances&quot;;
const three = &quot;i am looking for a daily driver that i can charge everyday, do not need any extras&quot;;
const four = &quot;i like to go offroading a lot on my jeep and i want to do the same with the truck&quot;;
const five = &quot;i want the most basic suv possible&quot;;
const six = &quot;I want all of the addons&quot;;
// mentions of large family or many people means model x
const seven = &quot;I have a big family and want to be able to take them around town and run errands without worrying about charging&quot;;
</code></pre><ul><li>Expected output<pre><code class="language-json">const oneJson = {
vehicle: &#x27;Model X&#x27;,
trim : &#x27;adventure&#x27;,
exteriorColor: &#x27;whiteExterior&#x27;,
wheels: &quot;22Performance&quot;,
tonneau: &quot;powerTonneau&quot;,
packages: &quot;&quot;,
interiorAddons: &quot;&quot;,
interiorColor: &quot;blackInterior&quot;,
range: &quot;extendedRange&quot;,
software: &quot;&quot;,
}

const twoJSON = {
vehicle: &#x27;Cyber Truck&#x27;,
trim : &#x27;base&#x27;,
exteriorColor: &#x27;whiteExterior&#x27;,
wheels: &quot;21AllSeason&quot;,
tonneau: &quot;powerTonneau&quot;,
packages: &quot;&quot;,
interiorAddons: &quot;&quot;,
interiorColor: &quot;blackInterior&quot;,
range: &quot;extendedRange&quot;,
software: &quot;&quot;,
}

const threeJSON = {
vehicle: &#x27;Cyber Truck&#x27;,
trim : &#x27;base&#x27;,
exteriorColor: &#x27;whiteExterior&#x27;,
wheels: &quot;21AllSeason&quot;,
tonneau: &quot;powerTonneau&quot;,
packages: &quot;&quot;,
interiorAddons: &quot;&quot;,
interiorColor: &quot;blackInterior&quot;,
range: &quot;standardRange&quot;,
software: &quot;&quot;,
}

const fourJSON = {
vehicle: &#x27;Cyber Truck&#x27;,
trim : &#x27;adventure&#x27;,
exteriorColor: &#x27;whiteExterior&#x27;,
wheels: &quot;20AllTerrain&quot;,
tonneau: &quot;powerTonneau&quot;,
packages: &quot;offroadPackage,matchingSpareTire&quot;,
interiorAddons: &quot;&quot;,
interiorColor: &quot;blackInterior&quot;,
range: &quot;extendedRange&quot;,
software: &quot;&quot;,
}

const fiveJSON = {
vehicle: &#x27;Model X&#x27;,
trim : &#x27;base&#x27;,
exteriorColor: &#x27;whiteExterior&#x27;,
wheels: &quot;20AllTerrain&quot;,
tonneau: &quot;manualTonneau&quot;,
packages: &quot;&quot;,
interiorAddons: &quot;&quot;,
interiorColor: &quot;blackInterior&quot;,
range: &quot;standardRange&quot;,
software: &quot;&quot;,
}

const sixJSON = {
vehicle: &#x27;Cyber Truck&#x27;,
trim : &#x27;adventure&#x27;,
exteriorColor: &#x27;whiteExterior&#x27;,
wheels: &quot;20AllTerrain&quot;,
tonneau: &quot;powerTonneau&quot;,
packages: &quot;offroadPackage,matchingSpareTire&quot;,
interiorAddons: &quot;wirelessCharger&quot;,
interiorColor: &quot;blackInterior&quot;,
range: &quot;extendedRange&quot;,
software: &quot;selfDrivingPackage&quot;,
}

const sevenJSON = {
vehicle: &#x27;Model X&#x27;,
trim : &#x27;base&#x27;,
exteriorColor: &#x27;whiteExterior&#x27;,
wheels: &quot;21AllSeason&quot;,
tonneau: &quot;powerTonneau&quot;,
packages: &quot;&quot;,
interiorAddons: &quot;&quot;,
interiorColor: &quot;blackInterior&quot;,
range: &quot;mediumRange&quot;,
software: &quot;&quot;,
}
</code></pre></li><li>Vehicle model configurations<pre><code class="language-json">const configuration = {
meta: {
configurationId: &#x27;???&#x27;,
storeId: &#x27;US_SALES&#x27;,
country: &#x27;US&#x27;,
version: &#x27;1.0&#x27;,
effectiveDate: &#x27;???&#x27;,
currency: &#x27;USD&#x27;,
locale: &#x27;en-US&#x27;,
availableLocales: [&#x27;en-US&#x27;],
},

defaults: {
basePrice: 50000,
deposit: 1000,
initialSelection: [
&#x27;adventure&#x27;,
&#x27;whiteExterior&#x27;,
&#x27;21AllSeason&#x27;,
&#x27;powerTonneau&#x27;,
&#x27;blackInterior&#x27;,
&#x27;mediumRange&#x27;,
],
},

groups: {
trim: {
name: { &#x27;en-US&#x27;: &#x27;Choose trim&#x27; },
multiselect: false,
required: true,
options: [&#x27;base&#x27;, &#x27;adventure&#x27;],
},
exteriorColor: {
name: { &#x27;en-US&#x27;: &#x27;Choose paint&#x27; },
multiselect: false,
required: true,
options: [
&#x27;whiteExterior&#x27;,
&#x27;blueExterior&#x27;,
&#x27;silverExterior&#x27;,
&#x27;greyExterior&#x27;,
&#x27;blackExterior&#x27;,
&#x27;redExterior&#x27;,
&#x27;greenExterior&#x27;,
],
},
wheels: {
name: { &#x27;en-US&#x27;: &#x27;Choose wheels&#x27; },
multiselect: false,
required: true,
options: [&#x27;21AllSeason&#x27;, &#x27;20AllTerrain&#x27;, &#x27;22Performance&#x27;],
},
tonneau: {
name: { &#x27;en-US&#x27;: &#x27;Choose tonneau cover&#x27; },
multiselect: false,
required: true,
options: [&#x27;manualTonneau&#x27;, &#x27;powerTonneau&#x27;],
},
packages: {
name: { &#x27;en-US&#x27;: &#x27;Choose upgrades&#x27; },
multiselect: true,
required: false,
options: [&#x27;offroadPackage&#x27;, &#x27;matchingSpareTire&#x27;],
},
interiorColor: {
name: { &#x27;en-US&#x27;: &#x27;Choose interior&#x27; },
multiselect: false,
required: true,
options: [&#x27;greyInterior&#x27;, &#x27;blackInterior&#x27;, &#x27;greenInterior&#x27;],
},
interiorAddons: {
name: { &#x27;en-US&#x27;: &#x27;Choose upgrade&#x27; },
multiselect: true,
required: false,
options: [&#x27;wirelessCharger&#x27;],
},
range: {
name: { &#x27;en-US&#x27;: &#x27;Choose range&#x27; },
multiselect: false,
required: true,
options: [&#x27;standardRange&#x27;, &#x27;mediumRange&#x27;, &#x27;extendedRange&#x27;],
},
software: {
name: { &#x27;en-US&#x27;: &#x27;Choose upgrade&#x27; },
multiselect: true,
required: false,
options: [&#x27;selfDrivingPackage&#x27;],
},
specs: {
name: { &#x27;en-US&#x27;: &#x27;Specs overview *&#x27; },
attrs: {
description: {
&#x27;en-US&#x27;:
&quot;* Options, specs and pricing may change as we approach production. We&#x27;ll contact you to review any updates to your preferred build.&quot;,
},
},
multiselect: false,
required: false,
options: [&#x27;acceleration&#x27;, &#x27;power&#x27;, &#x27;towing&#x27;, &#x27;range&#x27;],
},
},

options: {
base: {
name: { &#x27;en-US&#x27;: &#x27;Base&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;Production begins 2022&#x27; },
},
visual: true,
price: 0,
},
adventure: {
name: { &#x27;en-US&#x27;: &#x27;Adventure&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;Production begins 2021&#x27; },
},
visual: true,
price: 10000,
},

standardRange: {
name: { &#x27;en-US&#x27;: &#x27;Standard&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;230+ miles&#x27; },
},
price: 0,
},
mediumRange: {
name: { &#x27;en-US&#x27;: &#x27;Medium&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;300+ miles&#x27; },
},
price: 3000,
},
extendedRange: {
name: { &#x27;en-US&#x27;: &#x27;Extended&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;400+ miles&#x27; },
},
price: 8000,
},

greenExterior: {
name: { &#x27;en-US&#x27;: &#x27;Adirondack Green&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/exteriorcolors/green.svg&#x27;,
},
visual: true,
price: 2000,
},
blueExterior: {
name: { &#x27;en-US&#x27;: &#x27;Trestles Blue&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/exteriorcolors/blue.svg&#x27;,
},
visual: true,
price: 1000,
},
whiteExterior: {
name: { &#x27;en-US&#x27;: &#x27;Arctic White&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/exteriorcolors/white.svg&#x27;,
},
visual: true,
price: 0,
},
silverExterior: {
name: { &#x27;en-US&#x27;: &#x27;Silver Gracier&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/exteriorcolors/silver.svg&#x27;,
},
visual: true,
price: 1000,
},
blackExterior: {
name: { &#x27;en-US&#x27;: &#x27;Cosmic Black&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/exteriorcolors/black.svg&#x27;,
},
visual: true,
price: 1000,
},
redExterior: {
name: { &#x27;en-US&#x27;: &#x27;Red Rocks&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/exteriorcolors/red.svg&#x27;,
},
visual: true,
price: 2000,
},
greyExterior: {
name: { &#x27;en-US&#x27;: &#x27;Antracite Grey&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/exteriorcolors/grey.svg&#x27;,
},
visual: true,
price: 1000,
},

&#x27;21AllSeason&#x27;: {
name: { &#x27;en-US&#x27;: &#x27;21&quot; Cast Wheel - All Season&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/wheels/twentyone.svg&#x27;,
},
visual: true,
price: 0,
},
&#x27;20AllTerrain&#x27;: {
name: { &#x27;en-US&#x27;: &#x27;20&quot; Forged Wheel - All Terrain&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/wheels/twenty.svg&#x27;,
},
visual: true,
price: 0,
},
&#x27;22Performance&#x27;: {
name: { &#x27;en-US&#x27;: &#x27;22&quot; Cast Wheel - Performance&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/wheels/twentytwo.svg&#x27;,
},
visual: true,
price: 2000,
},

manualTonneau: {
name: { &#x27;en-US&#x27;: &#x27;Manual&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;Description here&#x27; },
},
price: 0,
},
powerTonneau: {
name: { &#x27;en-US&#x27;: &#x27;Powered&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;Description here&#x27; },
},
price: 0,
},

blackInterior: {
name: { &#x27;en-US&#x27;: &#x27;Black&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/interiorcolors/black.svg&#x27;,
},
visual: true,
price: 0,
},
greyInterior: {
name: { &#x27;en-US&#x27;: &#x27;Grey&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/interiorcolors/grey.svg&#x27;,
},
visual: true,
price: 1000,
},
greenInterior: {
name: { &#x27;en-US&#x27;: &#x27;Green&#x27; },
attrs: {
imageUrl: &#x27;/public/images/configurationOptions/interiorcolors/green.svg&#x27;,
},
visual: true,
price: 2000,
},

offroadPackage: {
name: { &#x27;en-US&#x27;: &#x27;Off-Road&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;Lorem ipsum dolor sit amet.&#x27; },
imageUrl: &#x27;/public/images/configurationOptions/packages/offroad.png&#x27;,
},
visual: true,
price: 5000,
},
matchingSpareTire: {
name: { &#x27;en-US&#x27;: &#x27;Matching Spare Tire&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;Full sized tire&#x27; },
imageUrl: &#x27;/public/images/configurationOptions/packages/spare.png&#x27;,
},
price: 500,
},

wirelessCharger: {
name: { &#x27;en-US&#x27;: &#x27;Wireless charger&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;Lorem ipsum dolor sit amet.&#x27; },
imageUrl: &#x27;/public/images/configurationOptions/packages/wireless.png&#x27;,
},
price: 100,
},
selfDrivingPackage: {
name: { &#x27;en-US&#x27;: &#x27;Autonomy&#x27; },
attrs: {
description: { &#x27;en-US&#x27;: &#x27;Lorem ipsum dolor sit amet.&#x27; },
imageUrl: &#x27;/public/images/configurationOptions/packages/autonomy.png&#x27;,
},
price: 7000,
},

acceleration: {
name: { &#x27;en-US&#x27;: &#x27;0 - 60 mph&#x27; },
attrs: {
units: { &#x27;en-US&#x27;: &#x27;sec&#x27; },
decimals: 1,
},
value: 3.4,
},
power: {
name: { &#x27;en-US&#x27;: &#x27;Horsepower&#x27; },
attrs: {
units: { &#x27;en-US&#x27;: &#x27;hp&#x27; },
},
value: 750,
},
towing: {
name: { &#x27;en-US&#x27;: &#x27;Towing&#x27; },
attrs: {
units: { &#x27;en-US&#x27;: &#x27;lbs&#x27; },
},
value: 10000,
},
range: {
name: { &#x27;en-US&#x27;: &#x27;Range&#x27; },
attrs: {
units: { &#x27;en-US&#x27;: &#x27;mi&#x27; },
},
value: 400,
},
}
};
</code></pre></li></ul><h2>Public datasets</h2><ul><li>Instagram: 16539 images from 972 Instagram influencers (<a href="https://github.com/gvsi/instagram-like-predictor">link</a>)</li><li>TechCrunchPosts: (<a href="https://www.kaggle.com/thibalbo/techcrunch-posts-compilation">link</a>)</li><li>Tweets: (<a href="https://data.world/data-society/twitter-user-data">link</a>)</li></ul><p>Primary (available for academic use only, need university affiliation for access)</p><ul><li><a href="https://arxiv.org/abs/2006.08335">A Dataset and Benchmarks for Multimedia Social Analysis</a></li></ul><p>Secondary (low quality data, not sure if can be used at all)</p><ul><li><a href="https://www.kaggle.com/hacker-news/hacker-news-posts">Hacker News Posts</a></li><li><a href="https://www.kaggle.com/thibalbo/techcrunch-posts-compilation">TechCrunch Posts Compilation</a></li><li>Instagram image data <a href="https://towardsdatascience.com/predict-the-number-of-likes-on-instagram-a7ec5c020203">HowTo</a></li><li>Flikr Large with likes and comments</li><li><a href="http://chenlab.ece.cornell.edu/people/Andy/ImagesOfGroups.html">The Images of Groups Dataset</a></li><li><a href="http://www.multimediaeval.org/datasets/">http://www.multimediaeval.org/datasets/</a></li><li><a href="https://gombru.github.io/2018/08/01/InstaCities1M/">The InstaCities1M Dataset</a></li><li><a href="https://www.insight-centre.org/sites/default/files/publications/memes_classification_lrec_1.pdf">Multimodal Meme Classification: Identifying Offensive Content in Image and Text</a></li><li><a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/NRPHLC">Understanding Police Social Media Usage Through Posts and Tweets</a></li><li>Topic clusters text<ul><li>Model X<ul><li>I like model X</li><li>I want to buy model X</li><li>Model X is my favorite car</li><li>Tesla Modelx is my dream</li><li>modelx tesla love</li></ul></li><li>Cyber Truck<ul><li>I like Cyber Truck</li><li>I want to buy Cyber Truck</li><li>Cyber Truck is my favorite car</li><li>Tesla Cyber Truck is my dream</li><li>CyberTruck tesla love</li></ul></li><li>Adventure<ul><li>I like adventure</li><li>sports i play</li><li>i went on trip</li><li>I travels a lot</li><li>car adventure</li></ul></li><li>Exterior Color White<ul><li>I like white color</li><li>White is my fav</li><li>white car love</li><li>I like white exterior</li></ul></li><li>Exterior Color Black<ul><li>I like Black color</li><li>Black is my fav</li><li>Black car love</li><li>I like Black exterior</li></ul></li><li>Exterior Color Blue<ul><li>I like Blue color</li><li>Blue is my fav</li><li>Blue car love</li><li>I like Blue exterior</li></ul></li><li>Exterior Color Green<ul><li>I like Green color</li><li>Green is my fav</li><li>Green car love</li><li>I like Green exterior</li></ul></li><li>Exterior Color Red<ul><li>I like Red color</li><li>Red is my fav</li><li>Red car love</li><li>I like Red exterior</li></ul></li><li>Exterior Color Grey<ul><li>I like Grey color</li><li>Grey is my fav</li><li>Grey car love</li><li>I like Grey exterior</li></ul></li><li>Exterior Color Silver<ul><li>I like Silver color</li><li>Silver is my fav</li><li>Silver car love</li><li>I like Silver exterior</li></ul></li><li>Self driving<ul><li>I like self driving technology</li><li>selfDrivingPackage</li><li>selfDrivingtech love</li><li>self drive is my fav</li><li>self driving car is amazing</li></ul></li></ul></li><li>Celebs<!-- -->  <img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-1.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-1.png"/></li></ul><h2>Logical Reasoning</h2><ul><li>If I implicitly rate pictures of blue car, that means I might prefer a blue car.</li><li>If I like posts of self-driving, that means I might prefer a self-driving option.</li></ul><h1>Scope</h1><h3>Scope 1</h3><p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-2.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-2.png"/></p><h3>Scope 2</h3><p>media content categories: text and images</p><p>platforms: facebook, twitter and instagram</p><p>implicit rating categories: like, comment, share</p><p>columns: userid, timestamp, platform, type, content, rating</p><h1>Model Framework</h1><h3>Model framework 1</h3><ol><li>Convert user&#x27;s natural language query into vector using Universal Sentence Embedding model</li><li>Create a product specs binary matrix based on different categories</li><li>Find TopK similar query vectors using cosine distance</li><li>For each TopK vector, Find TopM product specs using interaction table weights</li><li>For each TopM specification, find TopN similar specs using binary matrix</li><li>Show all the qualified product specifications</li></ol><h3>Model framework 2</h3><ol><li>Seed data: 10 users with ground-truth persona, media content and implicit ratings</li><li>Inflated data: 10 users with media content and implicit ratings</li><li>media content ‚Üí Implicit rating (A)</li><li>media content ‚Üí feature vector (B) + (A) ‚Üí weighted pooling ‚Üí similar users (C)</li><li>media content ‚Üí QA model ‚Üí slot filling ‚Üí global pooling ‚Üí item associations (D)</li><li>(C) ‚Üí content-based filtering ‚Üí item recommendations ‚Üí (D) ‚Üí top-k recommendations</li></ol><p><strong>User selection</strong></p><ul><li>People who are connected to social media community of electric vehicles</li><li>Seed users are those who already have an electric vehicle</li><li>Inflated users are those who doesn&#x27;t own an EV but inclined to purchase</li><li>Users having presense on all three sites or at least 2</li><li>List of common users<!-- -->  <a href="https://www.facebook.com/gossman">https://www.facebook.com/gossman</a>  <a href="https://www.facebook.com/ryanm06">https://www.facebook.com/ryanm06</a>  <a href="https://www.facebook.com/chad.turner.7146">https://www.facebook.com/chad.turner.7146</a>  <a href="https://www.facebook.com/cjacobs05">https://www.facebook.com/cjacobs05</a>  <a href="https://www.facebook.com/MafiaAllen">https://www.facebook.com/MafiaAllen</a>  <a href="https://www.facebook.com/rahul.mii.33">https://www.facebook.com/rahul.mii.33</a>  <a href="https://www.facebook.com/francisco.chavira.547">https://www.facebook.com/francisco.chavira.547</a>  <a href="https://www.facebook.com/JayTheillest74">https://www.facebook.com/JayTheillest74</a>  <a href="https://www.facebook.com/michael.creighton20">https://www.facebook.com/michael.creighton20</a>  <a href="https://www.facebook.com/darryl.grigggardening">https://www.facebook.com/darryl.grigggardening</a>  <a href="https://www.facebook.com/4X4Aus/">https://www.facebook.com/4X4Aus/</a>  <a href="https://www.instagram.com/minnyrc/">https://www.instagram.com/minnyrc/</a>  <a href="https://www.instagram.com/warnerbu7lt/">https://www.instagram.com/warnerbu7lt/</a></li><li>List of celebs<ol><li><p><a href="https://en.wikipedia.org/wiki/List_of_most-followed_Instagram_accounts">https://en.wikipedia.org/wiki/List_of_most-followed_Instagram_accounts</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts">https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/List_of_most-followed_Facebook_pages">https://en.wikipedia.org/wiki/List_of_most-followed_Facebook_pages</a></p><p>[&#x27;Jennifer Lopez&#x27;, &#x27;Virat Kohli&#x27;, &#x27;Ariana Grande&#x27;, &#x27;Dwayne Johnson&#x27;, &#x27;Kylie Jenner&#x27;, &#x27;Lionel Messi&#x27;, &#x27;LeBron James&#x27;, &#x27;Beyonc√©&#x27;, &#x27;Justin Bieber&#x27;, &#x27;Akshay Kumar&#x27;, &#x27;Demi Lovato&#x27;, &#x27;Kendall Jenner&#x27;, &#x27;Nicki Minaj&#x27;, &#x27;Khlo√© Kardashian&#x27;, &#x27;Kim Kardashian&#x27;, &#x27;Gigi Hadid&#x27;, &#x27;Ellen DeGeneres&#x27;, &#x27;Deepika Padukone&#x27;, &#x27;Rihanna&#x27;, &#x27;Shakira&#x27;, &#x27;Cardi B&#x27;, &#x27;Eminem&#x27;, &#x27;Drake&#x27;, &#x27;Chris Brown&#x27;, &#x27;Maluma&#x27;, &#x27;Vin Diesel&#x27;, &#x27;Ronaldinho&#x27;, &#x27;Kevin Hart&#x27;, &#x27;Emma Watson&#x27;, &#x27;Shawn Mendes&#x27;, &#x27;Neymar&#x27;, &#x27;Justin Timberlake&#x27;, &#x27;Katy Perry&#x27;, &#x27;Donald Trump&#x27;, &#x27;Lady Gaga&#x27;, &#x27;Amitabh Bachchan&#x27;, &#x27;Selena Gomez&#x27;, &#x27;Lil Wayne&#x27;, &#x27;Elon Musk&#x27;, &#x27;Britney Spears&#x27;, &#x27;Jimmy Fallon&#x27;, &#x27;Bill Gates&#x27;, &#x27;Ariana Grande&#x27;, &#x27;Miley Cyrus&#x27;, &#x27;Oprah Winfrey&#x27;, &#x27;Cristiano Ronaldo&#x27;, &#x27;Salman Khan&#x27;, &#x27;Shah Rukh Khan&#x27;, &#x27;Niall Horan&#x27;]</p></li></ol></li></ul><h3>Model framework 3</h3><p>User-User Similarity (clustering)</p><ul><li>User ‚Üí Media content ‚Üí Embedding ‚Üí Average pooling</li><li>Cosine Similarity of user&#x27;s social vector with other user&#x27;s social vector</li></ul><p>User-Item Similarity (reranking)</p><ul><li><strong>User ‚Üí Implicit Rating on media content M ‚Üí M&#x27;s correlation with item features</strong></li><li>Item features: familySize</li><li>Cosine Similarity of user&#x27;s social vector with item&#x27;s feature vector</li></ul><p>User-User Similarity (clustering)</p><ul><li>User ‚Üí Media content ‚Üí Embedding ‚Üí Average pooling</li><li>Cosine Similarity of user&#x27;s social vector with other user&#x27;s social vector</li></ul><p>User-Item Similarity (reranking)</p><ul><li><strong>User ‚Üí Implicit Rating on media content M ‚Üí M&#x27;s correlation with item features</strong></li><li>Item features: familySize</li><li>Cosine Similarity of user&#x27;s social vector with item&#x27;s feature vector</li></ul><h3>Model framework 4</h3><p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-3.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-3.png"/></p><p>Text ‚Üí Prepare ‚Üí Vectorize ‚Üí Average ‚Üí Similar Users</p><p>Image ‚Üí Prepare ‚Üí Vectorize ‚Üí Average ‚Üí Similar Users</p><p>Text ‚Üí Prepare ‚Üí QA ‚Üí Slot filling</p><p>Image ‚Üí Prepare ‚Üí VQA ‚Üí Slot filling</p><p>Image ‚Üí Similar Image from users ‚Üí Detailed enquiry</p><h3>Model framework 5</h3><ol><li>Topic Clusters Text</li><li>Topic Clusters Image</li><li>Fetch raw text and images</li><li>Combine, Clean and Store text in text dataframe</li><li>Vectorize Texts</li><li>Cosine similarities of texts with topic clusters</li><li>Vectorize Images</li><li>Cosine similarities of images with topic clusters</li></ol><h1>Experimental Setup</h1><ul><li>Experiment 1<pre><code class="language-python">import numpy as np
import pandas as pd
import tensorflow_hub as hub
from itertools import product
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics.pairwise import cosine_similarity

vehicle = [&#x27;modelX&#x27;, &#x27;cyberTruck&#x27;]
trim = [&#x27;adventure&#x27;, &#x27;base&#x27;]
exteriorColor = [&#x27;whiteExterior&#x27;, &#x27;blueExterior&#x27;, &#x27;silverExterior&#x27;, &#x27;greyExterior&#x27;, &#x27;blackExterior&#x27;, &#x27;redExterior&#x27;, &#x27;greenExterior&#x27;]
wheels = [&#x27;20AllTerrain&#x27;, &#x27;21AllSeason&#x27;, &#x27;22Performance&#x27;]
tonneau = [&#x27;powerTonneau&#x27;, &#x27;manualTonneau&#x27;]
interiorColor = [&#x27;blackInterior&#x27;, &#x27;greyInterior&#x27;, &#x27;greenInterior&#x27;]
range = [&#x27;standardRange&#x27;, &#x27;mediumRange&#x27;, &#x27;extendedRange&#x27;]
packages = [&#x27;offroadPackage&#x27;, &#x27;matchingSpareTire&#x27;, &#x27;offroadPackage,matchingSpareTire&#x27;, &#x27;None&#x27;]
interiorAddons = [&#x27;wirelessCharger&#x27;, &#x27;None&#x27;]
software = [&#x27;selfDrivingPackage&#x27;, &#x27;None&#x27;]

specs_cols = [&#x27;vehicle&#x27;, &#x27;trim&#x27;, &#x27;exteriorColor&#x27;, &#x27;wheels&#x27;, &#x27;tonneau&#x27;, &#x27;interiorColor&#x27;, &#x27;range&#x27;, &#x27;packages&#x27;, &#x27;interiorAddons&#x27;, &#x27;software&#x27;]
specs = pd.DataFrame(list(product(vehicle, trim, exteriorColor, wheels, tonneau, interiorColor, range, packages, interiorAddons, software)),
                     columns=specs_cols)

enc = OneHotEncoder(handle_unknown=&#x27;error&#x27;, sparse=False)
specs = pd.DataFrame(enc.fit_transform(specs))

specs_ids = specs.index.tolist()

query_list = [&quot;I&#x27;m looking for a fast suv that I can go camping without worrying about recharging&quot;,
              &quot;cheap red car that is able to go long distances&quot;,
              &quot;i am looking for a daily driver that i can charge everyday, do not need any extras&quot;,
              &quot;i like to go offroading a lot on my jeep and i want to do the same with the truck&quot;,
              &quot;i want the most basic suv possible&quot;,
              &quot;I want all of the addons&quot;, 
              &quot;I have a big family and want to be able to take them around town and run errands without worrying about charging&quot;]

queries = pd.DataFrame(query_list, columns=[&#x27;query&#x27;])
query_ids = queries.index.tolist()

const_oneJSON = {
&#x27;vehicle&#x27;: &#x27;modelX&#x27;,
&#x27;trim&#x27; : &#x27;adventure&#x27;,
&#x27;exteriorColor&#x27;: &#x27;whiteExterior&#x27;,
&#x27;wheels&#x27;: &quot;22Performance&quot;,
&#x27;tonneau&#x27;: &quot;powerTonneau&quot;,
&#x27;packages&#x27;: &quot;None&quot;,
&#x27;interiorAddons&#x27;: &quot;None&quot;,
&#x27;interiorColor&#x27;: &quot;blackInterior&quot;,
&#x27;range&#x27;: &quot;extendedRange&quot;,
&#x27;software&#x27;: &quot;None&quot;,
}

const_twoJSON = {
&#x27;vehicle&#x27;: &#x27;cyberTruck&#x27;,
&#x27;trim&#x27; : &#x27;base&#x27;,
&#x27;exteriorColor&#x27;: &#x27;whiteExterior&#x27;,
&#x27;wheels&#x27;: &quot;21AllSeason&quot;,
&#x27;tonneau&#x27;: &quot;powerTonneau&quot;,
&#x27;packages&#x27;: &quot;None&quot;,
&#x27;interiorAddons&#x27;: &quot;None&quot;,
&#x27;interiorColor&#x27;: &quot;blackInterior&quot;,
&#x27;range&#x27;: &quot;extendedRange&quot;,
&#x27;software&#x27;: &quot;None&quot;,
}

const_threeJSON = {
&#x27;vehicle&#x27;: &#x27;cyberTruck&#x27;,
&#x27;trim&#x27; : &#x27;base&#x27;,
&#x27;exteriorColor&#x27;: &#x27;whiteExterior&#x27;,
&#x27;wheels&#x27;: &quot;21AllSeason&quot;,
&#x27;tonneau&#x27;: &quot;powerTonneau&quot;,
&#x27;packages&#x27;: &quot;None&quot;,
&#x27;interiorAddons&#x27;: &quot;None&quot;,
&#x27;interiorColor&#x27;: &quot;blackInterior&quot;,
&#x27;range&#x27;: &quot;standardRange&quot;,
&#x27;software&#x27;: &quot;None&quot;,
}

const_fourJSON = {
&#x27;vehicle&#x27;: &#x27;cyberTruck&#x27;,
&#x27;trim&#x27; : &#x27;adventure&#x27;,
&#x27;exteriorColor&#x27;: &#x27;whiteExterior&#x27;,
&#x27;wheels&#x27;: &quot;20AllTerrain&quot;,
&#x27;tonneau&#x27;: &quot;powerTonneau&quot;,
&#x27;packages&#x27;: &quot;offroadPackage,matchingSpareTire&quot;,
&#x27;interiorAddons&#x27;: &quot;None&quot;,
&#x27;interiorColor&#x27;: &quot;blackInterior&quot;,
&#x27;range&#x27;: &quot;extendedRange&quot;,
&#x27;software&#x27;: &quot;None&quot;,
}

const_fiveJSON = {
&#x27;vehicle&#x27;: &#x27;modelX&#x27;,
&#x27;trim&#x27; : &#x27;base&#x27;,
&#x27;exteriorColor&#x27;: &#x27;whiteExterior&#x27;,
&#x27;wheels&#x27;: &quot;20AllTerrain&quot;,
&#x27;tonneau&#x27;: &quot;manualTonneau&quot;,
&#x27;packages&#x27;: &quot;None&quot;,
&#x27;interiorAddons&#x27;: &quot;None&quot;,
&#x27;interiorColor&#x27;: &quot;blackInterior&quot;,
&#x27;range&#x27;: &quot;standardRange&quot;,
&#x27;software&#x27;: &quot;None&quot;,
}

const_sixJSON = {
&#x27;vehicle&#x27;: &#x27;cyberTruck&#x27;,
&#x27;trim&#x27; : &#x27;adventure&#x27;,
&#x27;exteriorColor&#x27;: &#x27;whiteExterior&#x27;,
&#x27;wheels&#x27;: &quot;20AllTerrain&quot;,
&#x27;tonneau&#x27;: &quot;powerTonneau&quot;,
&#x27;packages&#x27;: &quot;offroadPackage,matchingSpareTire&quot;,
&#x27;interiorAddons&#x27;: &quot;wirelessCharger&quot;,
&#x27;interiorColor&#x27;: &quot;blackInterior&quot;,
&#x27;range&#x27;: &quot;extendedRange&quot;,
&#x27;software&#x27;: &quot;selfDrivingPackage&quot;,
}

const_sevenJSON = {
&#x27;vehicle&#x27;: &#x27;modelX&#x27;,
&#x27;trim&#x27; : &#x27;base&#x27;,
&#x27;exteriorColor&#x27;: &#x27;whiteExterior&#x27;,
&#x27;wheels&#x27;: &quot;21AllSeason&quot;,
&#x27;tonneau&#x27;: &quot;powerTonneau&quot;,
&#x27;packages&#x27;: &quot;None&quot;,
&#x27;interiorAddons&#x27;: &quot;None&quot;,
&#x27;interiorColor&#x27;: &quot;blackInterior&quot;,
&#x27;range&#x27;: &quot;mediumRange&quot;,
&#x27;software&#x27;: &quot;None&quot;,
}

historical_data = pd.DataFrame([const_oneJSON, const_twoJSON, const_threeJSON, const_fourJSON, const_fiveJSON, const_sixJSON, const_sevenJSON])

input_vec = enc.transform([specs_frame.append(historical_data.iloc[0], sort=False).iloc[-1]])
idx = np.argsort(-cosine_similarity(input_vec, specs.values))[0,:][:1]
rslt = enc.inverse_transform([specs.iloc[idx]])

interactions = pd.DataFrame(columns=[&#x27;query_id&#x27;,&#x27;specs_id&#x27;])
interactions[&#x27;query_id&#x27;] = queries.index.tolist()
input_vecs = enc.transform(specs_frame.append(historical_data, sort=False).iloc[-len(historical_data):])
interactions[&#x27;specs_id&#x27;] = np.argsort(-cosine_similarity(input_vecs, specs.values))[:,0]

module_url = &quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot; 
embed_model = hub.load(module_url)
def embed(input):
  return embed_model(input)
query_vecs = embed(queries[&#x27;query&#x27;].tolist()).numpy()

_query = input(&#x27;Please enter query: &#x27;) or &#x27;i want the most basic suv possible&#x27;
_query_vec = embed([_query]).numpy()
_match_qid = np.argsort(-cosine_similarity(_query_vec, query_vecs))[0,:][:1]
_match_sid = interactions.loc[interactions[&#x27;query_id&#x27;]==_match_qid[0], &#x27;specs_id&#x27;].values[0]
input_vec = enc.transform([specs_frame.append(historical_data.iloc[0], sort=False).iloc[-1]])
idx = np.argsort(-cosine_similarity([specs.iloc[_match_sid].values], specs.values))[0,:][:5]
results = []
for x in idx:
  results.append(enc.inverse_transform([specs.iloc[x]]))
_temp = np.array(results).reshape(5,-1)
_temp = pd.DataFrame(_temp, columns=specs_frame.columns)
print(_temp)
</code></pre></li></ul><h2>Experiment 2</h2><p>Celeb Scraping</p><h3>Facebook Scraping</h3><p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-4.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-4.png"/></p><h3>Twitter Scraping</h3><p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-5.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-5.png"/></p><h3>Dataframe</h3><p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-6.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-6.png"/></p><h3>Insta Image Grid</h3><p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-7.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-7.png"/></p><h3>User Text NER</h3><p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-8.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-8.png"/></p><h2>Experiment 3</h2><p>Topic model</p><h3>Topic scores</h3><p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-9.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-9.png"/></p><h3>JSON rules</h3><p><img src="/img/content-blog-raw-blog-vehicle-suggestions-untitled-10.png" alt="/img/content-blog-raw-blog-vehicle-suggestions-untitled-10.png"/></p><h1>Results and Discussion</h1><ul><li>API with 3 input fields - Facebook username, Twitter handle &amp; Instagram username</li><li>The system will automatically scrap the user&#x27;s publicly available text and images from these 3 social media platforms and provide a list of recommendations from most to least preferred product</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Web Scraping using Scrapy, BS4, and Selenium]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/web-scraping-using-scrapy-bs4-and-selenium</link>
            <guid>/2021/10/01/web-scraping-using-scrapy-bs4-and-selenium</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[1. Handling single request & response by extracting a city‚Äôs weather from a weather site using Scrapy]]></description>
            <content:encoded><![CDATA[<ol><li>Handling single request &amp; response by extracting a city‚Äôs weather from a weather site using Scrapy</li><li>Handling multiple request &amp; response by extracting book details from a dummy online book store using Scrapy</li><li>Scrape the cover images of all the books from the website <a href="http://books.toscrape.com/">books.toscrape.com</a> using Scrapy</li><li>Logging into Facebook using Selenium</li><li>Extract PM2.5 data from <a href="http://openaq.org">openaq.org</a> using Selenium</li><li>Extract PM2.5 data from <a href="http://openaq.org">openaq.org</a> using Selenium Scrapy</li></ol><p>:::note Scrapy vs. Selenium</p><p>Selenium is an automation tool for testing web applications. It uses a webdriver as an interface to control webpages through programming languages. So, this gives Selenium the capability to handle dynamic webpages effectively. Selenium is capable of extracting data on its own. It is true, but it has its caveats. Selenium cannot handle large data, but Scrapy can handle large data with ease. Also, Selenium is much slower when compared to Scrapy. So, the smart choice would be to use Selenium with Scrapy to scrape dynamic webpages containing large data, consuming less time. Combining Selenium with Scrapy is a simpler process. All that needs to be done is let Selenium render the webpage and once it is done, pass the webpage‚Äôs source to create a Scrapy Selector object. And from here on, Scrapy can crawl the page with ease and effectively extract a large amount of data.</p><p>:::</p><pre><code class="language-python"># SKELETON FOR COMBINING SELENIUM WITH SCRAPY
from scrapy import Selector
# Other Selenium and Scrapy imports
...
driver = webdriver.Chrome()
# Selenium tasks and actions to render the webpage with required content
selenium_response_text = driver.page_source
new_selector = Selector(text=selenium_response_text)
# Scrapy tasks to extract data from Selector
</code></pre><h2>Project tree</h2><pre><code class="language-html">.
‚îú‚îÄ‚îÄ airQuality
‚îÇ   ‚îú‚îÄ‚îÄ countries_list.json
‚îÇ   ‚îú‚îÄ‚îÄ get_countries.py
‚îÇ   ‚îú‚îÄ‚îÄ get_pm_data.py
‚îÇ   ‚îú‚îÄ‚îÄ get_urls.py
‚îÇ   ‚îú‚îÄ‚îÄ openaq_data.json
‚îÇ   ‚îú‚îÄ‚îÄ openaq_scraper.py
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ urls.json
‚îú‚îÄ‚îÄ airQualityScrapy
‚îÇ   ‚îú‚îÄ‚îÄ LICENSE
‚îÇ   ‚îú‚îÄ‚îÄ openaq
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ countries_list.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openaq
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ items.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spiders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ output.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scrapy.cfg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ urls.json
‚îÇ   ‚îú‚îÄ‚îÄ performance_comparison
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ performance_comparison
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ items.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spiders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scrapy.cfg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scrapy_output.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ selenium_scraper
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ bts_scraper.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ selenium_output.json
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ urls.json
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ books
‚îÇ   ‚îú‚îÄ‚îÄ books
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ items.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spiders
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ book_spider.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ crawl_spider.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ crawl_spider_output.json
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ scrapy.cfg
‚îú‚îÄ‚îÄ booksCoverImage
‚îÇ   ‚îú‚îÄ‚îÄ booksCoverImage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ items.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spiders
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ image_crawl_spider.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ output.json
‚îÇ   ‚îú‚îÄ‚îÄ path
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ to
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ store
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ scrapy.cfg
‚îú‚îÄ‚îÄ etc
‚îÇ   ‚îî‚îÄ‚îÄ Selenium
‚îÇ       ‚îú‚îÄ‚îÄ chromedriver.exe
‚îÇ       ‚îú‚îÄ‚îÄ chromedriver_v87.exe
‚îÇ       ‚îî‚îÄ‚îÄ install.sh
‚îú‚îÄ‚îÄ facebook
‚îÇ   ‚îî‚îÄ‚îÄ login.py
‚îú‚îÄ‚îÄ gazpacho1
‚îÇ   ‚îú‚îÄ‚îÄ data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ media.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ocr.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ static
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stheno.mp4
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ table.html
‚îÇ   ‚îú‚îÄ‚îÄ media
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ euryale.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ medusa.mp3
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ medusa.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stheno.mp4
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test.png
‚îÇ   ‚îú‚îÄ‚îÄ scrap_login.py
‚îÇ   ‚îú‚îÄ‚îÄ scrap_media.py
‚îÇ   ‚îú‚îÄ‚îÄ scrap_ocr.py
‚îÇ   ‚îú‚îÄ‚îÄ scrap_page.py
‚îÇ   ‚îî‚îÄ‚îÄ scrap_table.py
‚îú‚îÄ‚îÄ houzzdotcom
‚îÇ   ‚îú‚îÄ‚îÄ houzzdotcom
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ items.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spiders
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ crawl_spider.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ scrapy.cfg
‚îú‚îÄ‚îÄ media
‚îÇ   ‚îî‚îÄ‚îÄ test.png
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ scrapyPractice
‚îÇ   ‚îú‚îÄ‚îÄ scrapy.cfg
‚îÇ   ‚îî‚îÄ‚îÄ scrapyPractice
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ items.py
‚îÇ       ‚îú‚îÄ‚îÄ middlewares.py
‚îÇ       ‚îú‚îÄ‚îÄ pipelines.py
‚îÇ       ‚îú‚îÄ‚îÄ settings.py
‚îÇ       ‚îî‚îÄ‚îÄ spiders
‚îÇ           ‚îî‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ weather
    ‚îú‚îÄ‚îÄ output.json
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ scrapy.cfg
    ‚îî‚îÄ‚îÄ weather
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ items.py
        ‚îú‚îÄ‚îÄ middlewares.py
        ‚îú‚îÄ‚îÄ pipelines.py
        ‚îú‚îÄ‚îÄ settings.py
        ‚îî‚îÄ‚îÄ spiders
            ‚îú‚îÄ‚îÄ __init__.py
            ‚îî‚îÄ‚îÄ weather_spider.py

35 directories, 98 files
</code></pre><p><img src="/img/content-blog-raw-blog-web-scraping-using-scrapy-bs4-and-selenium-untitled.png" alt="For code, drop me a message on mail or LinkedIn."/></p><p>For code, drop me a message on mail or LinkedIn.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Web Scraping with Gazpacho]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/web-scraping-with-gazpacho</link>
            <guid>/2021/10/01/web-scraping-with-gazpacho</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Using gazpacho to Download and Parse the Contents of a Website. Scrape the names of the three "Gorgons".]]></description>
            <content:encoded><![CDATA[<h3>Using gazpacho to Download and Parse the Contents of a Website. Scrape the names of the three &quot;Gorgons&quot;.</h3><p><img src="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled.png" alt="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled.png"/></p><h3>Using gazpacho and pandas to Retrieve the Contents of an HTML Table. Scrape the creature and habitat columns.</h3><p><img src="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-1.png" alt="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-1.png"/></p><h3>Using gazpacho and Selenium to Retrieve the Contents of a Password-Protected Web Page. Scrape the quote text behind the login form.</h3><p><img src="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-2.png" alt="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-2.png"/></p><h3>Using gazpacho and pytesseract to Parse the Contents of ‚ÄúNon-Text‚Äù Text Data. Extract the embedded text.</h3><p><img src="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-3.png" alt="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-3.png"/></p><h3>Using gazpacho and urllib to Retrieve and Download Images, Videos, and Audio Clippings. To download the Image, Audio and Video data.</h3><p><img src="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-4.png" alt="/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-4.png"/></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Wellness tracker chatbot]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/wellness-tracker-chatbot</link>
            <guid>/2021/10/01/wellness-tracker-chatbot</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled.png" alt="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled.png"/></p><h2>Problem Statement</h2><p>A bot that logs daily wellness data to a spreadsheet (using the Airtable API), to help the user keep track of their health goals. Connect the assistant to a messaging channel‚ÄîTwilio‚Äîso users can talk to the assistant via text message and Whatsapp.</p><hr/><h2>Proposed Solution</h2><ul><li>RASA chatbot with Forms and Custom actions</li><li>Connect with Airtable API to log records in table database</li><li>Connect with Whatsapp for user interaction</li></ul><hr/><h2>Modeling</h2><p><img src="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-1.png" alt="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-1.png"/></p><p><img src="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-2.png" alt="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-2.png"/></p><p><img src="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-3.png" alt="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-3.png"/></p><p><img src="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-4.png" alt="/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-4.png"/></p><hr/><h2>Delivery</h2><p><a href="https://github.com/sparsh-ai/chatbots/tree/master/wellnessTracker">https://github.com/sparsh-ai/chatbots/tree/master/wellnessTracker</a></p><hr/><h2>Reference</h2><p><a href="https://www.udemy.com/course/rasa-for-beginners/learn/lecture/20746878#overview">https://www.udemy.com/course/rasa-for-beginners/learn/lecture/20746878#overview</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is Livestream Ecommerce]]></title>
            <link>https://docs.recohut.com/blog/2021/10/01/what-is-livestream-ecommerce</link>
            <guid>/2021/10/01/what-is-livestream-ecommerce</guid>
            <pubDate>Fri, 01 Oct 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled.png]]></description>
            <content:encoded><![CDATA[<p><img src="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled.png" alt="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled.png"/></p><p>Recent years witness the prosperity of online live streaming. With the development of mobile phones, cameras, and high-speed internet, more and more users are able to broadcast their experiences in live streams on various social platforms, such as Facebook Live and YouTube Live. There are a variety of live streaming applications, including knowledge share, video-gaming, and outdoor traveling.</p><p>One of the most important scenarios is live streaming commerce, a new form of online shopping becomes more and more popular, which combines live streaming with E-Commerce activity. The streamers introduce products and interact with their audiences, and hence greatly improve the performance of selling products.</p><p><img src="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-1.png" alt="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-1.png"/></p><blockquote><p>Livestream ecommerce is a business model in which retailers, influencers, or celebrities sell products and services via online video streaming where the presenter demonstrates and discusses the offering and answers audience questions in real-time.</p></blockquote><p><img src="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-2.png" alt="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-2.png"/></p><h3>Examples</h3><p><a href="https://media.nngroup.com/media/editor/2021/02/16/tiktok_livestream_compressed.mp4">https://media.nngroup.com/media/editor/2021/02/16/tiktok_livestream_compressed.mp4</a></p><p><em>During a livestream event hosted by Walmart on TikTok, users watched an influencer presenting various products such as a pair of jeans. Those interested in the jeans could tap the product listing shown at the bottom of the screen. They could also browse the list of products promoted during the livestream and purchase them without leaving the TikTok app. Viewers‚Äô real-time comments appeared along the left-hand side of the livestream feed.</em></p><h3>Advantages</h3><ul><li>Livestreams allow users to see products in detail and get their questions answered in real time</li><li>During livestream sessions, the hosts can show product details in close-up (left), give instructions of use for products like essential oils and cosmetic face masks (middle), or even show how a particular product, like the tea they‚Äôre selling, is made (right)<!-- -->  <img src="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-3.png" alt="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-3.png"/></li><li>Greatly shorten the decision-making time of consumers and provoke the sales volume</li><li>The expert streamers introduce and promote the products in a live streaming manner, which makes the shopping process more interesting and convincing</li><li>Rich and real-time interactions between streamers and their audiences, which makes live streaming a new medium and a powerful marketing tool for E-Commerce</li><li>Viewers not only can watch the showing for product‚Äôs looks and functions, but also can ask the streamers to show different or individual perspectives of the products in real-time</li></ul><h3>Market</h3><p>Livestream ecommerce has been surging dramatically in China. According to Forbes, this industry is estimated to earn $60 billion annually. In 2019, about 37 percent of the online shoppers in China (265 million people) made livestream purchases. On Taobao‚Äôs 2020 annual Single-Day Global Shopping Festival (November 11th), livestreams accounted for $6 billion in sales (twice the amount from the prior year).</p><p>Amazon has also launched its live platform, where influencers promote items and chat with potential customers. And Facebook and Instagram are exploring the integration between ecommerce and social media. For instance, the new Shop feature on Instagram allows users to browse products and place orders directly within Instagram ‚Äî a form of social commerce.</p><p>The total GMV driven by live streaming achieved $6 Billion USD. Some quantitative research results show that adopting live streaming in sales can achieve a 21.8% increase in online sales volume.</p><p><img src="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-4.png" alt="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-4.png"/></p><h3>The Anatomy of a Livestream Session</h3><p><img src="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-5.png" alt="/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-5.png"/></p><p>A typical livestream session has the following basic components:</p><ol><li><strong>The video stream,</strong>¬†where the host shows the products, talks about them, and answers questions from the audience. In the Amazon Live case, the stream occupies the most of the screen space.</li><li><strong>The list of products being promoted</strong>, with the product currently being shown highlighted. This list appears at the bottom of the Amazon video stream.</li><li><strong>A chat area,</strong>¬†where viewers can type questions and comments to interact with the host and other viewers. The chat area is at the right of the live stream on Amazon Live.</li><li><strong>A reaction button, that users</strong>¬†can use to send reactions, displayed as animated emojis. The reaction button shows up as a little star icon at the bottom right of the video stream on Amazon.</li></ol><h3>References</h3><ol><li><a href="https://www.nngroup.com/articles/livestream-ecommerce-china/">Features of Livestream ecommerce: What We Can Learn from China</a></li><li><a href="https://tracxn.com/d/trending-themes/Startups-in-Live-Streaming-E-Commerce">Top Live Streaming E-Commerce Startups</a></li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Object detection with YOLO3]]></title>
            <link>https://docs.recohut.com/blog/2021/01/23/object-detection-with-yolo3</link>
            <guid>/2021/01/23/object-detection-with-yolo3</guid>
            <pubDate>Sat, 23 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Live app]]></description>
            <content:encoded><![CDATA[<h2>Live app</h2><p>This app can detect COCO 80-classes using three different models - Caffe MobileNet SSD, Yolo3-tiny, and Yolo3. It can also detect faces using two different models - SSD Res10 and OpenCV face detector.  Yolo3-tiny can also detect fires.</p><p><img src="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png" alt="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png"/></p><p><img src="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png" alt="/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png"/></p><h2>Code</h2><pre><code class="language-python">import streamlit as st
import cv2
from PIL import Image
import numpy as np
import os

from tempfile import NamedTemporaryFile
from tensorflow.keras.preprocessing.image import img_to_array, load_img

temp_file = NamedTemporaryFile(delete=False)

DEFAULT_CONFIDENCE_THRESHOLD = 0.5
DEMO_IMAGE = &quot;test_images/demo.jpg&quot;
MODEL = &quot;model/MobileNetSSD_deploy.caffemodel&quot;
PROTOTXT = &quot;model/MobileNetSSD_deploy.prototxt.txt&quot;

CLASSES = [
    &quot;background&quot;,
    &quot;aeroplane&quot;,
    &quot;bicycle&quot;,
    &quot;bird&quot;,
    &quot;boat&quot;,
    &quot;bottle&quot;,
    &quot;bus&quot;,
    &quot;car&quot;,
    &quot;cat&quot;,
    &quot;chair&quot;,
    &quot;cow&quot;,
    &quot;diningtable&quot;,
    &quot;dog&quot;,
    &quot;horse&quot;,
    &quot;motorbike&quot;,
    &quot;person&quot;,
    &quot;pottedplant&quot;,
    &quot;sheep&quot;,
    &quot;sofa&quot;,
    &quot;train&quot;,
    &quot;tvmonitor&quot;,
]
COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))

@st.cache
def process_image(image):
    blob = cv2.dnn.blobFromImage(
        cv2.resize(image, (300, 300)), 0.007843, (300, 300), 127.5
    )
    net = cv2.dnn.readNetFromCaffe(PROTOTXT, MODEL)
    net.setInput(blob)
    detections = net.forward()
    return detections

@st.cache
def annotate_image(
    image, detections, confidence_threshold=DEFAULT_CONFIDENCE_THRESHOLD
):
    # loop over the detections
    (h, w) = image.shape[:2]
    labels = []
    for i in np.arange(0, detections.shape[2]):
        confidence = detections[0, 0, i, 2]

        if confidence &gt; confidence_threshold:
            # extract the index of the class label from the `detections`,
            # then compute the (x, y)-coordinates of the bounding box for
            # the object
            idx = int(detections[0, 0, i, 1])
            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            (startX, startY, endX, endY) = box.astype(&quot;int&quot;)

            # display the prediction
            label = f&quot;{CLASSES[idx]}: {round(confidence * 100, 2)}%&quot;
            labels.append(label)
            cv2.rectangle(image, (startX, startY), (endX, endY), COLORS[idx], 2)
            y = startY - 15 if startY - 15 &gt; 15 else startY + 15
            cv2.putText(
                image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2
            )
    return image, labels

def main():
  selected_box = st.sidebar.selectbox(
    &#x27;Choose one of the following&#x27;,
    (&#x27;Welcome&#x27;, &#x27;Object Detection&#x27;)
    )
    
  if selected_box == &#x27;Welcome&#x27;:
      welcome()
  if selected_box == &#x27;Object Detection&#x27;:
      object_detection() 

def welcome():
  st.title(&#x27;Object Detection using Streamlit&#x27;)
  st.subheader(&#x27;A simple app for object detection&#x27;)
  st.image(&#x27;test_images/demo.jpg&#x27;,use_column_width=True)

def object_detection():
  
  st.title(&quot;Object detection with MobileNet SSD&quot;)

  confidence_threshold = st.sidebar.slider(
    &quot;Confidence threshold&quot;, 0.0, 1.0, DEFAULT_CONFIDENCE_THRESHOLD, 0.05)

  st.sidebar.multiselect(&quot;Select object classes to include&quot;,
  options=CLASSES,
  default=CLASSES
  )

  img_file_buffer = st.file_uploader(&quot;Upload an image&quot;, type=[&quot;png&quot;, &quot;jpg&quot;, &quot;jpeg&quot;])

  if img_file_buffer is not None:
      temp_file.write(img_file_buffer.getvalue())
      image = load_img(temp_file.name)
      image = img_to_array(image)
      image = image/255.0

  else:
      demo_image = DEMO_IMAGE
      image = np.array(Image.open(demo_image))

  detections = process_image(image)
  image, labels = annotate_image(image, detections, confidence_threshold)

  st.image(
      image, caption=f&quot;Processed image&quot;, use_column_width=True,
  )

  st.write(labels)

main()
</code></pre><p><em>You can play with the live app</em> <a href="https://share.streamlit.io/sparsh-ai/streamlit-489fbbb7/app.py">*here</a>. Source code is available <a href="https://github.com/sparsh-ai/streamlit-5a407279/tree/master">here</a> on Github.*</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[MobileNet SSD Caffe Pre-trained model]]></title>
            <link>https://docs.recohut.com/blog/2020/01/19/mobilenet-ssd-caffe-pre-trained-model</link>
            <guid>/2020/01/19/mobilenet-ssd-caffe-pre-trained-model</guid>
            <pubDate>Sun, 19 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[You can play with the live app here. Souce code is available here on Github.]]></description>
            <content:encoded><![CDATA[<p><em>You can play with the live app <a href="https://share.streamlit.io/sparsh-ai/streamlit-5a407279/app.py">here</a>. Souce code is available</em> <a href="https://github.com/sparsh-ai/streamlit-489fbbb7">here</a> <em>on Github.</em></p><h2>Live app</h2><p><img src="/img/content-blog-raw-mobilenet-ssd-caffe-pre-trained-model-untitled.png" alt="/img/content-blog-raw-mobilenet-ssd-caffe-pre-trained-model-untitled.png"/></p><h2>Code</h2><pre><code class="language-python">#------------------------------------------------------#
# Import libraries
#------------------------------------------------------#

import datetime
import urllib
import time
import cv2 as cv
import streamlit as st

from plugins import Motion_Detection
from utils import GUI, AppManager, DataManager

#------------------------------------------------------#
#------------------------------------------------------#

def imageWebApp(guiParam):
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    # Load the image according to the selected option
    conf = DataManager(guiParam)
    image = conf.load_image_or_video()
    
    # GUI
    switchProcessing = st.button(&#x27;* Start Processing *&#x27;)

    # Apply the selected plugin on the image
    bboxed_frame, output = AppManager(guiParam).process(image, True)

    # Display results
    st.image(bboxed_frame, channels=&quot;BGR&quot;,  use_column_width=True)

def main():
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    # Get the parameter entered by the user from the GUI
    guiParam = GUI().getGuiParameters()

    # Check if the application if it is Empty
    if guiParam[&#x27;appType&#x27;] == &#x27;Image Applications&#x27;:
        if guiParam[&quot;selectedApp&quot;] is not &#x27;Empty&#x27;:
            imageWebApp(guiParam)

    else:
        raise st.ScriptRunner.StopException

#------------------------------------------------------#
#------------------------------------------------------#

if __name__ == &quot;__main__&quot;:
    main()
</code></pre>]]></content:encoded>
        </item>
    </channel>
</rss>