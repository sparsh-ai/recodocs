<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-react-helmet="true">Bias &amp; Fairness | Recohut</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://docs.recohut.com/docs/concept-extras/bias-&amp;-fairness"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Bias &amp; Fairness | Recohut"><meta data-react-helmet="true" name="description" content="It canâ€™t be denied that there is bias all around us. A bias is a prejudice against a person or group of people, including, but not limited to their gender, race, and beliefs. Many of these biases arise from emergent behavior in social interactions, events in history, and cultural and political views around the world. These biases affect the data that we collect. Because AI algorithms work with this data, it is an inherent problem that the machine will â€œlearnâ€ these biases. From a technical perspective, we can engineer the system perfectly, but at the end of the day, humans interact with these systems, and itâ€™s our responsibility to minimize bias and prejudice as much as possible. The algorithms we use are only as good as the data provided to them. Understanding the data and the context in which it is being used is the first step in battling bias, and this understanding will help you build better solutionsâ€”because you will be well versed in the problem space. Providing balanced data with as little bias as possible should result in better solutions."><meta data-react-helmet="true" property="og:description" content="It canâ€™t be denied that there is bias all around us. A bias is a prejudice against a person or group of people, including, but not limited to their gender, race, and beliefs. Many of these biases arise from emergent behavior in social interactions, events in history, and cultural and political views around the world. These biases affect the data that we collect. Because AI algorithms work with this data, it is an inherent problem that the machine will â€œlearnâ€ these biases. From a technical perspective, we can engineer the system perfectly, but at the end of the day, humans interact with these systems, and itâ€™s our responsibility to minimize bias and prejudice as much as possible. The algorithms we use are only as good as the data provided to them. Understanding the data and the context in which it is being used is the first step in battling bias, and this understanding will help you build better solutionsâ€”because you will be well versed in the problem space. Providing balanced data with as little bias as possible should result in better solutions."><link data-react-helmet="true" rel="icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://docs.recohut.com/docs/concept-extras/bias-&amp;-fairness"><link data-react-helmet="true" rel="alternate" href="https://docs.recohut.com/docs/concept-extras/bias-&amp;-fairness" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://docs.recohut.com/docs/concept-extras/bias-&amp;-fairness" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.87c99533.css">
<link rel="preload" href="/assets/js/runtime~main.b5b3c7ff.js" as="script">
<link rel="preload" href="/assets/js/main.c68b4ffd.js" as="script">
</head>
<body data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Recohut Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Recohut Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">Recohut</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/recohut/docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/concept-basics/">Concept - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Concept - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--active hasHref_TwRn" href="/docs/concept-extras/">Concept - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Concept - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/amazon-personalize">Amazon Personalize</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/apps">Apps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/concept-extras/bias-&amp;-fairness">Bias &amp; Fairness</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/causal-inference">Causal Inference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/cold-start">Cold Start</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/cross-domain">Cross-domain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/data-science">Data Science</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/diversity">Diversity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/emerging-concepts-in-recommender-systems">Emerging Concepts in Recommender Systems</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/graph-embeddings">Graph Embeddings</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/graph-networks">Graph Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/incremental-learning">Incremental Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/jensen-shannon-divergence">Jensenâ€“Shannon divergence</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/meta-learning">Meta Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/mlops">MLOps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/model-deployment">Model Deployment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/model-retraining">Model Retraining</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/multi-objective-optimization">Multi-Objective Optimization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/multi-task-learning">Multi-Task Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/multitask-learning">Multi-task Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/offline-learning">Off-Policy Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/concept-extras/scalarization">Scalarization</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" tabindex="0" href="/docs/concept-extras/nlp/chatbot">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" tabindex="0" href="/docs/concept-extras/success-stories/1mg-prod2vec">Success Stories</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_TwRn" tabindex="0" href="/docs/concept-extras/vision/facial-analytics">Computer Vision</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/models/">Models</a><button aria-label="Toggle the collapsible sidebar category &#x27;Models&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/tutorials/">Tutorials</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorials&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_TwRn" href="/docs/tools/">Tools</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tools&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/datasets">Datasets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/projects">Projects</a></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Bias &amp; Fairness</h1></header><p>It canâ€™t be denied that there is bias all around us. A bias is a prejudice against a person or group of people, including, but not limited to their gender, race, and beliefs. Many of these biases arise from emergent behavior in social interactions, events in history, and cultural and political views around the world. These biases affect the data that we collect. Because AI algorithms work with this data, it is an inherent problem that the machine will â€œlearnâ€ these biases. From a technical perspective, we can engineer the system perfectly, but at the end of the day, humans interact with these systems, and itâ€™s our responsibility to minimize bias and prejudice as much as possible. The algorithms we use are only as good as the data provided to them. Understanding the data and the context in which it is being used is the first step in battling bias, and this understanding will help you build better solutionsâ€”because you will be well versed in the problem space. Providing balanced data with as little bias as possible should result in better solutions.</p><p>Recommender systems are important for connecting users to the right items. But are items recommended fairly? For example, in a recruiting recommender that recommends job candidates (the items here), are candidates of different genders treated equally? In a news recommender, are news stories with different political ideologies recommended fairly? And even for product recommenders, are products from big companies favored over products from new entrants? The danger of unfair recommendations for items has been recognized in the literature, with potential negative impacts on item providers, user satisfaction, the recommendation platform itself, and ultimately social good.</p><p>In practice, the data is observational rather than experimental, and is often affected by many factors, including but not limited to self-selection of the user (selection bias), exposure mechanism of the system (exposure bias), public opinions (conformity bias) and the display position (position bias). De-biasing the data is an important and critical pre-processing step. Biases cause training data distribution deviate from the ideal unbiased one.</p><p><img alt="Untitled" src="/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-64a535748137172862ffe40cdbfeede7.png"></p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="types-of-biases">Types of biases<a class="hash-link" href="#types-of-biases" title="Direct link to heading">â€‹</a></h2><p><img alt="Untitled" src="/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-1-c0494b16fb2ef50b14cce3f94a2991c1.png"></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="selection-bias">Selection bias<a class="hash-link" href="#selection-bias" title="Direct link to heading">â€‹</a></h3><p>Selection bias originates from usersâ€™ numerical ratings on items (i.e., explicit feedback), which is defined as - <em>&quot;Selection Bias happens as users are free to choose which items to rate, so that the observed ratings are not a representative sample of all ratings. In other words, the rating data is often missing not at random (MNAR).&quot;</em></p><p><img alt="Distribution of rating values for randomly selected items and user-selected items, as demonstrated in [this](https://www.notion.so/04c70cf18dbe401980fe9b00bb1a2077) paper." src="/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-2-783d9b3c06b9edb7a88c0c717f6e8d17.png"></p><p>Distribution of rating values for randomly selected items and user-selected items, as demonstrated in <a href="https://www.notion.so/04c70cf18dbe401980fe9b00bb1a2077" target="_blank" rel="noopener noreferrer">this</a> paper.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="conformity-bias">Conformity bias<a class="hash-link" href="#conformity-bias" title="Direct link to heading">â€‹</a></h3><p>Another bias inherent in the explicit feedback data is conformity bias, which is defined as: <em>&quot;Conformity bias happens as users tend to rate similarly to the others in a group, even if doing so goes against their own judgment, making the rating values do not always signify user true preference&quot;</em>.</p><p>For example, influenced by high ratings of public comments on an item, one user is highly likely to change her low rate, avoiding being too harsh. Such phenomenon of conformity is common and cause biases in user ratings. As shown in <a href="https://www.notion.so/A-methodology-for-learning-analyzing-and-mitigating-social-influence-bias-in-recommender-systems-e304120c16f1415583e396f786cac335" target="_blank" rel="noopener noreferrer">Krishnan et al.</a>, user ratings follow different distributions when users rate items before or after being exposed to the public opinions. Moreover, conformity bias might be caused by social influence, where users tend to behave similarly with their friends. Hence, the observed ratings are skewed and might not reflect usersâ€™ real preference on items.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="debiasing-methods">Debiasing methods<a class="hash-link" href="#debiasing-methods" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="ips">IPS<a class="hash-link" href="#ips" title="Direct link to heading">â€‹</a></h3><p>IPS eliminates popularity bias by re-weighting each instance according to item popularity. Specifically, weight for an instance is set as the inverse of corresponding item popularity value, hence popular items are imposed lower weights, while the importance for long-tail items are boosted.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="ips-c">IPS-C<a class="hash-link" href="#ips-c" title="Direct link to heading">â€‹</a></h3><p>This method adds max-capping on IPS value to reduce the variance of IPS.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="ips-cn">IPS-CN<a class="hash-link" href="#ips-cn" title="Direct link to heading">â€‹</a></h3><p>This method further adds normalization which also achieved lower variance than plain IPS, at the expense of introducing a small amount of bias.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="ips-cnsr">IPS-CNSR<a class="hash-link" href="#ips-cnsr" title="Direct link to heading">â€‹</a></h3><p>Smoothing and re-normalization are added to attain more stable output of IPS.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="cause">CausE<a class="hash-link" href="#cause" title="Direct link to heading">â€‹</a></h3><p>This method requires a large biased dataset and a small unbiased dataset. Each user or item has two embeddings to perform matrix factorization (MF) on the two datasets respectively, and L1 or L2 regularization is exploited.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="random">Random<a class="hash-link" href="#random" title="Direct link to heading">â€‹</a></h2><p><img alt="Aim for ethical and legal applications of technology" src="/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-3-b27bd256c9ce69e4b2b68cb103f314bc.png"></p><p>Aim for ethical and legal applications of technology</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="concepts">Concepts<a class="hash-link" href="#concepts" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="equal-opportunity"><a href="https://arxiv.org/abs/1610.02413" target="_blank" rel="noopener noreferrer">Equal opportunity</a><a class="hash-link" href="#equal-opportunity" title="Direct link to heading">â€‹</a></h3><p>In a classification task, equal opportunity requires a model to produce the same true positive rate (TPR) for all individuals or groups. The goal is to ensure that items from different groups can be equally recommended to matched users during testing (the same true positive rate): for example, candidates of different genders are equally recommended to job openings that they are qualified for. In contrast, demographic parity fairness only focuses on the difference in the amount of exposure to users without considering the ground-truth of user-item matching. However, because only the exposure to matched users (as considered by equal opportunity fairness) can influence the feedback or economic gain of items, in recommendation tasks, equal opportunity is better aligned than demographic parity fairness.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="rawlsian-max-min-fairness-principle-of-distributive-justice">Rawlsian Max-Min fairness principle of distributive justice<a class="hash-link" href="#rawlsian-max-min-fairness-principle-of-distributive-justice" title="Direct link to heading">â€‹</a></h3><p>Rawlsian Max-Min fairness requires a model to maximize the minimum utility of individuals or groups so that no subject is underserved by the model. Unlike equality (or parity) based notions of fairness aiming to eliminate difference among individuals or groups but neglecting a decrease of utility for betterserved subjects, Rawlsian Max-Min fairness accepts inequalities and thus does not requires decreasing utility of better-served subjects. So, Rawlsian Max-Min fairness is preferred in applications where perfect equality is not necessary, such as recommendation tasks, and it can also better preserve the overall model utility.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="autodebias">AutoDebias<a class="hash-link" href="#autodebias" title="Direct link to heading">â€‹</a></h2><p>AutoDebias is an automatic debiasing method for recommendation system based on meta learning, exploiting a small amount of uniform data to learn de-biasing parameters and using these parameters to guide the learning of the recommendation model.</p><p><img alt="The working flow of AutoDebias, consists of three steps: (1) tentatively updating ğœƒ to ğœƒ â€² on the training data ğ·ğ‘‡ with current ğœ™ (black arrows); (2) updating ğœ™ based on ğœƒ â€² on the uniform data (blue arrows); (3) actually updating ğœƒ with the updated ğœ™ (black arrows)." src="/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-4-1de344d9d5f6cc9b44f273e17b564c6a.png"></p><p>The working flow of AutoDebias, consists of three steps: (1) tentatively updating ğœƒ to ğœƒ â€² on the training data ğ·ğ‘‡ with current ğœ™ (black arrows); (2) updating ğœ™ based on ğœƒ â€² on the uniform data (blue arrows); (3) actually updating ğœƒ with the updated ğœ™ (black arrows).</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/recohut/docs/docs/docs/concept-extras/bias-&amp;-fairness.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_mt2f"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/concept-extras/apps"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Apps</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/concept-extras/causal-inference"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Causal Inference</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#types-of-biases" class="table-of-contents__link toc-highlight">Types of biases</a><ul><li><a href="#selection-bias" class="table-of-contents__link toc-highlight">Selection bias</a></li><li><a href="#conformity-bias" class="table-of-contents__link toc-highlight">Conformity bias</a></li></ul></li><li><a href="#debiasing-methods" class="table-of-contents__link toc-highlight">Debiasing methods</a><ul><li><a href="#ips" class="table-of-contents__link toc-highlight">IPS</a></li><li><a href="#ips-c" class="table-of-contents__link toc-highlight">IPS-C</a></li><li><a href="#ips-cn" class="table-of-contents__link toc-highlight">IPS-CN</a></li><li><a href="#ips-cnsr" class="table-of-contents__link toc-highlight">IPS-CNSR</a></li><li><a href="#cause" class="table-of-contents__link toc-highlight">CausE</a></li></ul></li><li><a href="#random" class="table-of-contents__link toc-highlight">Random</a></li><li><a href="#concepts" class="table-of-contents__link toc-highlight">Concepts</a><ul><li><a href="#equal-opportunity" class="table-of-contents__link toc-highlight">Equal opportunity</a></li><li><a href="#rawlsian-max-min-fairness-principle-of-distributive-justice" class="table-of-contents__link toc-highlight">Rawlsian Max-Min fairness principle of distributive justice</a></li></ul></li><li><a href="#autodebias" class="table-of-contents__link toc-highlight">AutoDebias</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Learn</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/concept-basics">Concepts</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/tutorials">Tutorials</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/projects">Projects</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/recohut/docs" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://nb.recohut.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Jupyter Notebooks<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://step.recohut.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Interactive Stories<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 Recohut Docs, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.b5b3c7ff.js"></script>
<script src="/assets/js/main.c68b4ffd.js"></script>
</body>
</html>