{
  "pluginId": "default",
  "version": "current",
  "label": "Next",
  "banner": null,
  "badge": false,
  "className": "docs-version-current",
  "isLast": true,
  "docsSidebars": {
    "tutorialSidebar": [
      {
        "type": "link",
        "label": "Recohut Intro",
        "href": "/docs/intro",
        "docId": "intro"
      },
      {
        "type": "category",
        "label": "Models",
        "collapsible": true,
        "collapsed": true,
        "items": [
          {
            "type": "link",
            "label": "A3C",
            "href": "/docs/models/a3c",
            "docId": "models/a3c"
          },
          {
            "type": "link",
            "label": "AFM",
            "href": "/docs/models/afm",
            "docId": "models/afm"
          },
          {
            "type": "link",
            "label": "AFN",
            "href": "/docs/models/afn",
            "docId": "models/afn"
          },
          {
            "type": "link",
            "label": "AR",
            "href": "/docs/models/ar",
            "docId": "models/ar"
          },
          {
            "type": "link",
            "label": "ASMG",
            "href": "/docs/models/asmg",
            "docId": "models/asmg"
          },
          {
            "type": "link",
            "label": "AttRec",
            "href": "/docs/models/attrec",
            "docId": "models/attrec"
          },
          {
            "type": "link",
            "label": "AutoInt",
            "href": "/docs/models/autoint",
            "docId": "models/autoint"
          },
          {
            "type": "link",
            "label": "BCQ",
            "href": "/docs/models/bcq",
            "docId": "models/bcq"
          },
          {
            "type": "link",
            "label": "BiasOnly",
            "href": "/docs/models/biasonly",
            "docId": "models/biasonly"
          },
          {
            "type": "link",
            "label": "BPR",
            "href": "/docs/models/bpr",
            "docId": "models/bpr"
          },
          {
            "type": "link",
            "label": "CASER",
            "href": "/docs/models/caser",
            "docId": "models/caser"
          },
          {
            "type": "link",
            "label": "DCN",
            "href": "/docs/models/dcn",
            "docId": "models/dcn"
          },
          {
            "type": "link",
            "label": "DDPG",
            "href": "/docs/models/ddpg",
            "docId": "models/ddpg"
          },
          {
            "type": "link",
            "label": "DeepCross",
            "href": "/docs/models/deepcross",
            "docId": "models/deepcross"
          },
          {
            "type": "link",
            "label": "DeepFM",
            "href": "/docs/models/deepfm",
            "docId": "models/deepfm"
          },
          {
            "type": "link",
            "label": "DeepWalk",
            "href": "/docs/models/deepwalk",
            "docId": "models/deepwalk"
          },
          {
            "type": "link",
            "label": "DGTN",
            "href": "/docs/models/dgtn",
            "docId": "models/dgtn"
          },
          {
            "type": "link",
            "label": "DQN",
            "href": "/docs/models/dqn",
            "docId": "models/dqn"
          },
          {
            "type": "link",
            "label": "DRQN",
            "href": "/docs/models/drqn",
            "docId": "models/drqn"
          },
          {
            "type": "link",
            "label": "DRR",
            "href": "/docs/models/drr",
            "docId": "models/drr"
          },
          {
            "type": "link",
            "label": "Dueling DQN",
            "href": "/docs/models/dueling-dqn",
            "docId": "models/dueling-dqn"
          },
          {
            "type": "link",
            "label": "FFM",
            "href": "/docs/models/ffm",
            "docId": "models/ffm"
          },
          {
            "type": "link",
            "label": "FGNN",
            "href": "/docs/models/fgnn",
            "docId": "models/fgnn"
          },
          {
            "type": "link",
            "label": "FM",
            "href": "/docs/models/fm",
            "docId": "models/fm"
          },
          {
            "type": "link",
            "label": "GAT",
            "href": "/docs/models/gat",
            "docId": "models/gat"
          },
          {
            "type": "link",
            "label": "GC-SAN",
            "href": "/docs/models/gc-san",
            "docId": "models/gc-san"
          },
          {
            "type": "link",
            "label": "GCE-GNN",
            "href": "/docs/models/gce-gnn",
            "docId": "models/gce-gnn"
          },
          {
            "type": "link",
            "label": "GRU4Rec",
            "href": "/docs/models/gru4rec",
            "docId": "models/gru4rec"
          },
          {
            "type": "link",
            "label": "HMLET",
            "href": "/docs/models/hmlet",
            "docId": "models/hmlet"
          },
          {
            "type": "link",
            "label": "ItemPop",
            "href": "/docs/models/itempop",
            "docId": "models/itempop"
          },
          {
            "type": "link",
            "label": "LESSR",
            "href": "/docs/models/lessr",
            "docId": "models/lessr"
          },
          {
            "type": "link",
            "label": "LightFM WARP",
            "href": "/docs/models/lightfm-warp",
            "docId": "models/lightfm-warp"
          },
          {
            "type": "link",
            "label": "LightGCN",
            "href": "/docs/models/lightgcn",
            "docId": "models/lightgcn"
          },
          {
            "type": "link",
            "label": "LIRD",
            "href": "/docs/models/lird",
            "docId": "models/lird"
          },
          {
            "type": "link",
            "label": "Markov Chains",
            "href": "/docs/models/markov-chains",
            "docId": "models/markov-chains"
          },
          {
            "type": "link",
            "label": "MB-GMN",
            "href": "/docs/models/mb-gmn",
            "docId": "models/mb-gmn"
          },
          {
            "type": "link",
            "label": "MF",
            "href": "/docs/models/mf",
            "docId": "models/mf"
          },
          {
            "type": "link",
            "label": "MIAN",
            "href": "/docs/models/mian",
            "docId": "models/mian"
          },
          {
            "type": "link",
            "label": "NeuMF",
            "href": "/docs/models/neumf",
            "docId": "models/neumf"
          },
          {
            "type": "link",
            "label": "NFM",
            "href": "/docs/models/nfm",
            "docId": "models/nfm"
          },
          {
            "type": "link",
            "label": "NGCF",
            "href": "/docs/models/ngcf",
            "docId": "models/ngcf"
          },
          {
            "type": "link",
            "label": "PNN",
            "href": "/docs/models/pnn",
            "docId": "models/pnn"
          },
          {
            "type": "link",
            "label": "PPO",
            "href": "/docs/models/ppo",
            "docId": "models/ppo"
          },
          {
            "type": "link",
            "label": "Q-learning",
            "href": "/docs/models/q-learning",
            "docId": "models/q-learning"
          },
          {
            "type": "link",
            "label": "SAC",
            "href": "/docs/models/sac",
            "docId": "models/sac"
          },
          {
            "type": "link",
            "label": "SARSA",
            "href": "/docs/models/sarsa",
            "docId": "models/sarsa"
          },
          {
            "type": "link",
            "label": "SASRec",
            "href": "/docs/models/sasrec",
            "docId": "models/sasrec"
          },
          {
            "type": "link",
            "label": "SGL",
            "href": "/docs/models/sgl",
            "docId": "models/sgl"
          },
          {
            "type": "link",
            "label": "SiReN",
            "href": "/docs/models/siren",
            "docId": "models/siren"
          },
          {
            "type": "link",
            "label": "SLIST",
            "href": "/docs/models/slist",
            "docId": "models/slist"
          },
          {
            "type": "link",
            "label": "SPop",
            "href": "/docs/models/spop",
            "docId": "models/spop"
          },
          {
            "type": "link",
            "label": "SR-GNN",
            "href": "/docs/models/sr-gnn",
            "docId": "models/sr-gnn"
          },
          {
            "type": "link",
            "label": "SR-SAN",
            "href": "/docs/models/sr-san",
            "docId": "models/sr-san"
          },
          {
            "type": "link",
            "label": "SR",
            "href": "/docs/models/sr",
            "docId": "models/sr"
          },
          {
            "type": "link",
            "label": "STAMP",
            "href": "/docs/models/stamp",
            "docId": "models/stamp"
          },
          {
            "type": "link",
            "label": "SVAE",
            "href": "/docs/models/svae",
            "docId": "models/svae"
          },
          {
            "type": "link",
            "label": "TAGNN-PP",
            "href": "/docs/models/tagnn-pp",
            "docId": "models/tagnn-pp"
          },
          {
            "type": "link",
            "label": "TAGNN",
            "href": "/docs/models/tagnn",
            "docId": "models/tagnn"
          },
          {
            "type": "link",
            "label": "VNCF",
            "href": "/docs/models/vncf",
            "docId": "models/vncf"
          },
          {
            "type": "link",
            "label": "VSKNN",
            "href": "/docs/models/vsknn",
            "docId": "models/vsknn"
          },
          {
            "type": "link",
            "label": "Wide and Deep",
            "href": "/docs/models/wide-and-deep",
            "docId": "models/wide-and-deep"
          },
          {
            "type": "link",
            "label": "Word2vec",
            "href": "/docs/models/word2vec",
            "docId": "models/word2vec"
          },
          {
            "type": "link",
            "label": "xDeepFM",
            "href": "/docs/models/xdeepfm",
            "docId": "models/xdeepfm"
          }
        ]
      }
    ]
  },
  "docs": {
    "intro": {
      "id": "intro",
      "title": "Recohut Intro",
      "description": "",
      "sidebar": "tutorialSidebar"
    },
    "models/a3c": {
      "id": "models/a3c",
      "title": "A3C",
      "description": "A3C stands for Asynchronous Advantage Actor-Critic. The A3C algorithm builds upon the Actor-Critic class of algorithms by using a neural network to approximate the actor (and critic). The actor learns the policy function using a deep neural network, while the critic estimates the value function. The asynchronous nature of the algorithm allows the agent to learn from different parts of the state space, allowing parallel learning and faster convergence. Unlike DQN agents, which use an experience replay memory, the A3C agent uses multiple workers to gather more samples for learning.",
      "sidebar": "tutorialSidebar"
    },
    "models/afm": {
      "id": "models/afm",
      "title": "AFM",
      "description": "AFM stands for Attentional Factorization Machines. It Improves FM by discriminating the importance of different feature interactions, and learns the importance of each feature interaction from data via a neural attention network. Empirically, it is shown on regression task that AFM performs betters than FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters.",
      "sidebar": "tutorialSidebar"
    },
    "models/afn": {
      "id": "models/afn",
      "title": "AFN",
      "description": "AFN stands for Adaptive Factorization Network.",
      "sidebar": "tutorialSidebar"
    },
    "models/ar": {
      "id": "models/ar",
      "title": "AR",
      "description": "Simple Association Rules (AR) are a simplified version of the association rule mining technique [Agrawal et al. 1993] with a maximum rule size of two. The method is designed to capture the frequency of two co-occurring events, e.g., ‚ÄúCustomers who bought . . . also bought‚Äù.",
      "sidebar": "tutorialSidebar"
    },
    "models/asmg": {
      "id": "models/asmg",
      "title": "ASMG",
      "description": "ASMG stands for Adaptive Sequential Model Generation.",
      "sidebar": "tutorialSidebar"
    },
    "models/attrec": {
      "id": "models/attrec",
      "title": "AttRec",
      "description": "AttRec stands for Self-Attentive Sequential Recommendation.",
      "sidebar": "tutorialSidebar"
    },
    "models/autoint": {
      "id": "models/autoint",
      "title": "AutoInt",
      "description": "Song et. al., ‚ÄúAutomatic Feature Interaction Learning via Self-Attentive Neural Networks‚Äù. CIKM, 2018.",
      "sidebar": "tutorialSidebar"
    },
    "models/bcq": {
      "id": "models/bcq",
      "title": "BCQ",
      "description": "Current off-policy deep reinforcement learning algorithms fail to address extrapolation error by selecting actions with respect to a learned value estimate, without consideration of the accuracy of the estimate. As a result, certain outof-distribution actions can be erroneously extrapolated to higher values. However, the value of an off-policy agent can be accurately evaluated in regions where data is available.",
      "sidebar": "tutorialSidebar"
    },
    "models/biasonly": {
      "id": "models/biasonly",
      "title": "BiasOnly",
      "description": "BiasOnly is a simple baseline that assumes no interactions between users and items. Formally, it learns: (1) a global bias ùõº; (2) scalar biases $\\betau$ for each user ùë¢ ‚àà U; and (3) scalar biases $\\betai$ for each item ùëñ ‚àà I. Ultimately, the rating/relevance for user ùë¢ and item ùëñ is modeled as $\\hati^u = \\alpha + \\betau + \\beta_i$.",
      "sidebar": "tutorialSidebar"
    },
    "models/bpr": {
      "id": "models/bpr",
      "title": "BPR",
      "description": "BPR stands for Bayesian Personalized Ranking. In matrix factorization (MF), to compute the prediction we have to multiply the user factors to the item factors:",
      "sidebar": "tutorialSidebar"
    },
    "models/caser": {
      "id": "models/caser",
      "title": "CASER",
      "description": "CASER stands for Convolutional Sequence Embedding Recommendation. Top-N sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-N ranked items that a user will likely interact in a 'near future'. The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. Convolutional Sequence Embedding Recommendation Model (Caser) address this requirement by embedding a sequence of recent items into an image' in the time and latent spaces and learn sequential patterns as local features of the image using convolutional filters. This approach provides a unified and flexible network structure for capturing both general preferences and sequential patterns. In other words, Caser adopts convolutional neural networks capture the dynamic pattern influences of users‚Äô recent activities.",
      "sidebar": "tutorialSidebar"
    },
    "models/dcn": {
      "id": "models/dcn",
      "title": "DCN",
      "description": "DCN stands for Deep and Cross Network. Manual explicit feature crossing process is very laborious and inefficient. On the other hand, automatic implicit feature crossing methods like MLPs cannot efficiently approximate even 2nd or 3rd-order feature crosses. Deep-cross networks provides a solution to this problem. DCN was designed to learn explicit and bounded-degree cross features more effectively. It starts with an input layer (typically an embedding layer), followed by a cross network containing multiple cross layers that models explicit feature interactions, and then combines with a deep network that models implicit feature interactions.",
      "sidebar": "tutorialSidebar"
    },
    "models/ddpg": {
      "id": "models/ddpg",
      "title": "DDPG",
      "description": "Deterministic Policy Gradient (DPG)¬†is a type¬†of Actor-Critic RL algorithm that uses two neural networks: one for estimating the action value function, and the other for estimating the optimal target policy. The¬†Deep Deterministic Policy Gradient¬†(DDPG) agent¬†builds upon the idea of DPG and is quite efficient compared to vanilla Actor-Critic agents due¬†to the use¬†of deterministic action policies.",
      "sidebar": "tutorialSidebar"
    },
    "models/deepcross": {
      "id": "models/deepcross",
      "title": "DeepCross",
      "description": "Shan, Y., Hoens, T., Jiao, J., Wang, H., Yu, D. and Mao, J., 2016.¬†Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features. [online] Kdd.org.",
      "sidebar": "tutorialSidebar"
    },
    "models/deepfm": {
      "id": "models/deepfm",
      "title": "DeepFM",
      "description": "DeepFM stands for Deep Factorization Machines. It consists of an FM component and a deep component which are integrated in a parallel structure. The FM component is the same as the 2-way factorization machines which is used to model the low-order feature interactions. The deep component is a multi-layered perceptron that is used to capture high-order feature interactions and nonlinearities. These two components share the same inputs/embeddings and their outputs are summed up as the final prediction.",
      "sidebar": "tutorialSidebar"
    },
    "models/deepwalk": {
      "id": "models/deepwalk",
      "title": "DeepWalk",
      "description": "DeepWalk learns representations of online social networks graphs. By performing random walks to generate sequences, the paper demonstrated that it was able to learn vector representations of nodes (e.g., profiles, content) in the graph.",
      "sidebar": "tutorialSidebar"
    },
    "models/dgtn": {
      "id": "models/dgtn",
      "title": "DGTN",
      "description": "DGTN stands for Dual-channel Graph Transition Network.",
      "sidebar": "tutorialSidebar"
    },
    "models/dqn": {
      "id": "models/dqn",
      "title": "DQN",
      "description": "The¬†Q-learning component of DQN was invented in 1989 by Christopher Watkins in his PhD thesis titled ‚ÄúLearning from Delayed Rewards‚Äù. Experience replay quickly followed, invented by Long-Ji Lin in 1992. This played a major role in improving the efficiency of¬†Q-learning. In the years that followed, however, there were no major success stories involving deep¬†Q-learning. This is perhaps not surprising given the combination of limited computational power in the 1990s and early 2000s, data-hungry deep learning architectures, and the sparse, noisy, and delayed feedback signals experienced in RL. Progress had to wait for the emergence of general-purpose GPU programming, for example with the launch of CUDA in 2006, and the reignition of interest in deep learning within the machine learning community that began in the mid-2000s and rapidly accelerated after 2012.",
      "sidebar": "tutorialSidebar"
    },
    "models/drqn": {
      "id": "models/drqn",
      "title": "DRQN",
      "description": "DRQN stands for Deep Recurrent Q-Learning. It is a combination of a recurrent neural network (RNN) and a deep Q-network (DQN). The idea being that the RNN will be able to retain information from states further back in time and incorporate that into predicting better Q values and thus performing better on games that require long term planning.",
      "sidebar": "tutorialSidebar"
    },
    "models/drr": {
      "id": "models/drr",
      "title": "DRR",
      "description": "DRR Framework",
      "sidebar": "tutorialSidebar"
    },
    "models/dueling-dqn": {
      "id": "models/dueling-dqn",
      "title": "Dueling DQN",
      "description": "A¬†Dueling DQN agent explicitly estimates two quantities through a modified network architecture:",
      "sidebar": "tutorialSidebar"
    },
    "models/ffm": {
      "id": "models/ffm",
      "title": "FFM",
      "description": "FFM stands for Field-aware Factorization Machines. In the official FFM paper, it is empirically proven that for large, sparse datasets with many categorical features, FFM performs better. Conversely, for small and dense datasets or numerical datasets, FFM may not be as effective as FM. FFM is also prone to overfitting on the training dataset, hence one should use a standalone validation set and use early stopping when the loss increases.",
      "sidebar": "tutorialSidebar"
    },
    "models/fgnn": {
      "id": "models/fgnn",
      "title": "FGNN",
      "description": "Ruihong Qiu, Jingjing Li, Zi Huang and Hongzhi Yin, ‚ÄúRethinking the Item Order in Session-based Recommendation with Graph Neural Networks‚Äù. CIKM, 2019.",
      "sidebar": "tutorialSidebar"
    },
    "models/fm": {
      "id": "models/fm",
      "title": "FM",
      "description": "Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Factorization Machine type algorithms are a combination of linear regression and matrix factorization, the cool idea behind this type of algorithm is it aims model interactions between features (a.k.a attributes, explanatory variables) using factorized parameters. By doing so it has the ability to estimate all interactions between features even with extremely sparse data.",
      "sidebar": "tutorialSidebar"
    },
    "models/gat": {
      "id": "models/gat",
      "title": "GAT",
      "description": "GAT stands for Graph Attention Networks. This is a special GNN model that addresses several key challenges of spectral models, such as poor ability of generalization from a specific graph structure to another and sophisticated computation of matrix inverse. GAT utilizes attention mechanisms to aggregate neighborhood features (embeddings) by specifying different weights to different nodes.",
      "sidebar": "tutorialSidebar"
    },
    "models/gc-san": {
      "id": "models/gc-san",
      "title": "GC-SAN",
      "description": "GC-SAN stands for Graph contextualized self-attention.",
      "sidebar": "tutorialSidebar"
    },
    "models/gce-gnn": {
      "id": "models/gce-gnn",
      "title": "GCE-GNN",
      "description": "GCE-GNN stands for Global Context Enhanced Graph Neural Networks. It exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session.",
      "sidebar": "tutorialSidebar"
    },
    "models/gru4rec": {
      "id": "models/gru4rec",
      "title": "GRU4Rec",
      "description": "It uses session-parallel mini-batch approach where we first create an order for the sessions and then, we use the first event of the first X sessions to form the input of the first mini-batch (the desired output is the second events of our active sessions). The second mini-batch is formed from the second events and so on. If any of the sessions end, the next available session is put in its place. Sessions are assumed to be independent, thus we reset the appropriate hidden state when this switch occurs.",
      "sidebar": "tutorialSidebar"
    },
    "models/hmlet": {
      "id": "models/hmlet",
      "title": "HMLET",
      "description": "HMLET stands for Hybrid Method of Linear and nonlinEar collaborative filTering (HMLET, pronounced as Hamlet). It is a GCN-based CF method.",
      "sidebar": "tutorialSidebar"
    },
    "models/itempop": {
      "id": "models/itempop",
      "title": "ItemPop",
      "description": "Itempop is a na√Øve baseline that simply ranks items according to overall train-set popularity. Note that this method is unaffected by the user for which items are being recommended, and has the same global ranking of all items",
      "sidebar": "tutorialSidebar"
    },
    "models/lessr": {
      "id": "models/lessr",
      "title": "LESSR",
      "description": "Tianwen Chen and Raymond Chi-Wing Wong, ‚ÄúLESSR: Handling Information Loss of Graph Neural Networks for Session-based Recommendation‚Äù. KDD, 2020.",
      "sidebar": "tutorialSidebar"
    },
    "models/lightfm-warp": {
      "id": "models/lightfm-warp",
      "title": "LightFM WARP",
      "description": "LightFM is probably the only recommender package implementing the WARP (Weighted Approximate-Rank Pairwise) loss for implicit feedback learning-to-rank. Generally, it performs better than the more popular BPR (Bayesian Personalised Ranking) loss --- often by a large margin.",
      "sidebar": "tutorialSidebar"
    },
    "models/lightgcn": {
      "id": "models/lightgcn",
      "title": "LightGCN",
      "description": "GCN is a representative model of graph neural networks that applies message passing to aggregate neighborhood information. The message passing layer with self-loops is defined as follows:",
      "sidebar": "tutorialSidebar"
    },
    "models/lird": {
      "id": "models/lird",
      "title": "LIRD",
      "description": "Existing reinforcement learning recommender methods also could recommend a list of items. E.g. DQN can calculate Q-values of all recalled items separately, and recommend a list of items with highest Q-values. But these recommendations are similar in Euclidean space and we want to find similarity in associative space. For instance, for a bread üçû, I want egg ü•ö, milk ü•õ in my recommendation list instead of white bread üçû, brown bread ü•™, bun ü´ì etc.",
      "sidebar": "tutorialSidebar"
    },
    "models/markov-chains": {
      "id": "models/markov-chains",
      "title": "Markov Chains",
      "description": "Markov chains, named after Andrey Markov, are mathematical systems that hop from one \"state\" (a situation or set of values) to another. For example, if you made a Markov chain model of a baby's behavior, you might include \"playing,\" \"eating\", \"sleeping,\" and \"crying\" as states, which together with other behaviors could form a 'state space': a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probability of hopping, or \"transitioning,\" from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first.",
      "sidebar": "tutorialSidebar"
    },
    "models/mb-gmn": {
      "id": "models/mb-gmn",
      "title": "MB-GMN",
      "description": "MB-GMN stands for Multi-behavior pattern modeling with meta-knowledge learner.",
      "sidebar": "tutorialSidebar"
    },
    "models/mf": {
      "id": "models/mf",
      "title": "MF",
      "description": "Matrix Factorization is an iterative approach of SVD called Regularized SVD. It uses the gradient-descent method to estimate the resulting matrices. The obtained model will not be a true SVD of the rating-matrix, as the component matrices are no longer orthogonal, but tends to be more accurate at predicting unseen preferences than the standard SVD [Ekstrand et al. 2011].",
      "sidebar": "tutorialSidebar"
    },
    "models/mian": {
      "id": "models/mian",
      "title": "MIAN",
      "description": "MIAN stands for Multi-Interactive Attention Network. It aggregate multiple information, and gain latent representations through interactions between candidate items and other fine-grained features.",
      "sidebar": "tutorialSidebar"
    },
    "models/neumf": {
      "id": "models/neumf",
      "title": "NeuMF",
      "description": "NMF Leverages the representation power of deep neural-networks to capture nonlinear correlations between user and item embeddings. Formally, the rating/relevance for user ùë¢ and item ùëñ is modeled as $\\hati^u = \\alpha + \\betau + \\betai + f(\\gammau || \\gammai || \\gammau \\cdot \\gammai)$ where $\\gammau , \\gamma_i \\in \\mathbb{R}^d$, ‚Äò||‚Äô represents the concatenation operation, and $f: \\mathbb{R}^{3d} \\rightarrow \\mathbb{R}$ represents an arbitrarily complex neural network.",
      "sidebar": "tutorialSidebar"
    },
    "models/nfm": {
      "id": "models/nfm",
      "title": "NFM",
      "description": "NFM stands for Neural Factorization Machine.",
      "sidebar": "tutorialSidebar"
    },
    "models/ngcf": {
      "id": "models/ngcf",
      "title": "NGCF",
      "description": "NGCF stands for Neural Graph Collaborative Filtering. This GNN-based approach follows basic operations inherited from the standard GCN to explore the high-order connectivity information. More specifically, NGCF stacks embedding layers and concatenates embeddings obtained in all layers to constitute the final embeddings.",
      "sidebar": "tutorialSidebar"
    },
    "models/pnn": {
      "id": "models/pnn",
      "title": "PNN",
      "description": "PNN stands for Product-based Neural Network.",
      "sidebar": "tutorialSidebar"
    },
    "models/ppo": {
      "id": "models/ppo",
      "title": "PPO",
      "description": "The PPO (Proximal Policy Optimization) algorithm was¬†introduced by the OpenAI team in 2017¬†and quickly became one of the most popular Reinforcement Learning method that pushed all other RL methods at that moment aside. PPO involves collecting a small batch of experiences interacting with the environment and using that batch to update its decision-making policy. Once the policy is updated with that batch, the experiences are thrown away and a newer batch is collected with the newly updated policy. This is the reason why it is an ‚Äúon-policy learning‚Äù approach where the experience samples collected are only useful for updating the current policy.",
      "sidebar": "tutorialSidebar"
    },
    "models/q-learning": {
      "id": "models/q-learning",
      "title": "Q-learning",
      "description": "Q-learning can be applied to model-free RL problems. It supports off-policy learning and therefore provides a practical solution to problems where available experiences were/are collected using some other policy or by some other agent (even humans).",
      "sidebar": "tutorialSidebar"
    },
    "models/sac": {
      "id": "models/sac",
      "title": "SAC",
      "description": "SAS stands for Soft Actor-Critic. It not only boasts of being more sample efficient than traditional RL algorithms but also promises to be robust to brittleness in convergence.",
      "sidebar": "tutorialSidebar"
    },
    "models/sarsa": {
      "id": "models/sarsa",
      "title": "SARSA",
      "description": "The SARSA algorithm can be applied to model-free control problems and allows us to optimize the value function of an unknown MDP. SARSA is an on-policy¬†temporal difference learning-based control algorithm. The SARSA algorithm can be summarized as follows:",
      "sidebar": "tutorialSidebar"
    },
    "models/sasrec": {
      "id": "models/sasrec",
      "title": "SASRec",
      "description": "SASRec stands for Self-Attentive Sequential Recommendation. It relies on the sequence modeling capabilities of self-attentive neural networks to predict the occurence of the next item in a user‚Äôs consumption sequence. To be precise, given a user ùë¢ and their time-ordered consumption history $S^ùë¢ = (S1^u, S2^u, \\dots, S_{|S^u|}^ùë¢),$ SASRec first applies self-attention on $S^ùë¢$ followed by a series of non-linear feed-forward layers to finally obtain the next item likelihood.",
      "sidebar": "tutorialSidebar"
    },
    "models/sgl": {
      "id": "models/sgl",
      "title": "SGL",
      "description": "SGL is the latest baseline for top-k recommendations. It introduces self-supervised learning into the recommendation system based on the contrastive learning framework. It is implemented on LightGCN and uses a multitask approach that unites the contrastive loss and the BPR loss function. SGL mainly benefits from graph contrastive learning to reinforce user and item representations.",
      "sidebar": "tutorialSidebar"
    },
    "models/siren": {
      "id": "models/siren",
      "title": "SiReN",
      "description": "Existing literature often ignores the negative feedback e.g. dislikes on YouTube videos, and only capture the homophily (or assortativity) patterns by positive feedback. This is a missed opportunity situation. Performance of GNN-based Recommender Systems can be improved by including negative feedbacks. Disassortivity patterns can be learned by negative feedback. LightGCN can capture the assortativity patterns. and the MLP network can capture the disassortivity patterns.",
      "sidebar": "tutorialSidebar"
    },
    "models/slist": {
      "id": "models/slist",
      "title": "SLIST",
      "description": "SLIST stands for Session-aware Linear Similarity/Transition. It is built by unifying two linear models - SLIS and SLIT.",
      "sidebar": "tutorialSidebar"
    },
    "models/spop": {
      "id": "models/spop",
      "title": "SPop",
      "description": "SPop stands for Session-based Popularity. It is a session popularity predictor that gives higher scores to items with higher number of occurrences in the session. Ties are broken up by adding the popularity score of the item.",
      "sidebar": "tutorialSidebar"
    },
    "models/sr": {
      "id": "models/sr",
      "title": "SR",
      "description": "SR stands for Sequential Rules. The SR method is a variation of MC and AR. It also takes the order of actions into account, but in a less restrictive manner. In contrast to the MC method, we create a rule when an item q appeared after an item p in a session even when other events happened between p and q. When assigning weights to the rules, we consider the number of elements appearing between p and q in the session.",
      "sidebar": "tutorialSidebar"
    },
    "models/sr-gnn": {
      "id": "models/sr-gnn",
      "title": "SR-GNN",
      "description": "SR-GNN stands for Session-based Recommendation with Graph Neural Networks.",
      "sidebar": "tutorialSidebar"
    },
    "models/sr-san": {
      "id": "models/sr-san",
      "title": "SR-SAN",
      "description": "SR-SAN stands for Session-based Recommendation with Self-Attention Networks.",
      "sidebar": "tutorialSidebar"
    },
    "models/stamp": {
      "id": "models/stamp",
      "title": "STAMP",
      "description": "/img/content-models-raw-mp2-stamp-untitled.png",
      "sidebar": "tutorialSidebar"
    },
    "models/svae": {
      "id": "models/svae",
      "title": "SVAE",
      "description": "SVAE stands for Sequential Variational Autoencoder.",
      "sidebar": "tutorialSidebar"
    },
    "models/tagnn": {
      "id": "models/tagnn",
      "title": "TAGNN",
      "description": "TAGNN stands for Target Attentive Graph Neural Network. Session-based recommendations are challenging due to limited user-item interactions. Typical sequential models are not able to capture complex patterns from all previous interactions. SessionGraph (a graph representation of sessions) can capture the complex patterns from all previous interactions.",
      "sidebar": "tutorialSidebar"
    },
    "models/tagnn-pp": {
      "id": "models/tagnn-pp",
      "title": "TAGNN-PP",
      "description": "TAGNN-PP models item interactions with GNN, and both local and global user interactions with  a Transformer.",
      "sidebar": "tutorialSidebar"
    },
    "models/vncf": {
      "id": "models/vncf",
      "title": "VNCF",
      "description": "VNCF stands for Variational Neural Collaborative Filtering.",
      "sidebar": "tutorialSidebar"
    },
    "models/vsknn": {
      "id": "models/vsknn",
      "title": "VSKNN",
      "description": "VSKNN stands for Vector Multiplication Session-Based kNN. The idea of this variant is to put more emphasis on the more recent events of a session when computing the similarities. Instead of encoding a session as a binary vector, we use real-valued vectors to encode the current session. Only the very last element of the session obtains a value of ‚Äú1‚Äù; the weights of the other elements are determined using a linear decay function that depends on the position of the element within the session, where elements appearing earlier in the session obtain a lower weight. As a result, when using the dot product as a similarity function between the current weight-encoded session and a binary-encoded past session, more emphasis is given to elements that appear later in the sessions.",
      "sidebar": "tutorialSidebar"
    },
    "models/wide-and-deep": {
      "id": "models/wide-and-deep",
      "title": "Wide and Deep",
      "description": "Wide and Deep Learning Model, proposed by Google, 2016, is a DNN-Linear mixed model, which combines the strength of memorization and generalization. It's useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). It has been used for Google App Store for their app recommendation.",
      "sidebar": "tutorialSidebar"
    },
    "models/word2vec": {
      "id": "models/word2vec",
      "title": "Word2vec",
      "description": "/img/content-models-raw-mp2-word2vec-untitled.png",
      "sidebar": "tutorialSidebar"
    },
    "models/xdeepfm": {
      "id": "models/xdeepfm",
      "title": "xDeepFM",
      "description": "xDeepFM stands for Extreme Deep Factorization Machines.",
      "sidebar": "tutorialSidebar"
    }
  }
}